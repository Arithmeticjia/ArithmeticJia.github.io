<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>请叫我算术嘉的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://arithmeticjia.github.io/"/>
  <updated>2020-04-20T07:46:10.345Z</updated>
  <id>http://arithmeticjia.github.io/</id>
  
  <author>
    <name>请叫我算术嘉</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>VAE-变分自动编码器</title>
    <link href="http://arithmeticjia.github.io/2020/04/20/VAE-%E5%8F%98%E5%88%86%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8/"/>
    <id>http://arithmeticjia.github.io/2020/04/20/VAE-%E5%8F%98%E5%88%86%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8/</id>
    <published>2020-04-20T07:20:14.000Z</published>
    <updated>2020-04-20T07:46:10.345Z</updated>
    
    <content type="html"><![CDATA[<p>Variational Auto-Encoder<br><a id="more"></a></p><h3 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h3><h4 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h4><script type="math/tex; mode=display">I(x) = -logp(x)</script><p>p(x) 为事件x发生的概率，当log底数为e时，信息量的单位为nat（奈特），当log底数为2时，信息量的单位为bit（比特）。</p><h4 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h4><p>表示随机变量x在离散和连续情况下的信息熵H:</p><script type="math/tex; mode=display">H = \sum -\log p(x) * p(x)</script><script type="math/tex; mode=display">H = \int -\log p(x) * p(x)d(x)</script><h3 id="K-L散度（Kullback-Leibler-divergence）"><a href="#K-L散度（Kullback-Leibler-divergence）" class="headerlink" title="K-L散度（Kullback-Leibler divergence）"></a>K-L散度（Kullback-Leibler divergence）</h3><p>K-L散度又被称为相对熵（relative entropy），是对两个概率分布间差异的非对称性度量。</p><h3 id="贝叶斯公式（Bayes-Rule）"><a href="#贝叶斯公式（Bayes-Rule）" class="headerlink" title="贝叶斯公式（Bayes Rule）"></a>贝叶斯公式（Bayes Rule）</h3><script type="math/tex; mode=display">p(z|x) = \frac{p(z,x)}{p(x)} = \frac{p(x|z)p(z)}{p(x)}</script><h3 id="AE-自编码器"><a href="#AE-自编码器" class="headerlink" title="AE-自编码器"></a>AE-自编码器</h3><p>encoder-decoder结构</p><p><img src="https://pic3.zhimg.com/80/v2-8ae0e598375aeeed45488edd064e1cfa_1440w.jpg" alt=""></p><h3 id="VAE-差分自编码器"><a href="#VAE-差分自编码器" class="headerlink" title="VAE-差分自编码器"></a>VAE-差分自编码器</h3><p>假设给定样本x，希望在给定x的条件下推出z的分布，即p(z|x)</p><p>根据贝叶斯公式:</p><script type="math/tex; mode=display">p(z|x) = \frac{p(z,x)}{p(x)} = \frac{p(x|z)p(z)}{p(x)}</script><p>因为无法得知p(x),使用q(z|x)去近似p(z|x),满足minKL(q(z|x)||p(z|x))</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Variational Auto-Encoder&lt;br&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="http://arithmeticjia.github.io/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-04-17周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/04/17/2020-04-17%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/04/17/2020-04-17%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-04-17T10:50:20.000Z</published>
    <updated>2020-04-17T10:51:48.719Z</updated>
    
    <content type="html"><![CDATA[<p>不知道说啥<br><a id="more"></a></p><p>1、堆叠LSTM</p><p><img src="https://upload-images.jianshu.io/upload_images/7311123-f4f81fb3930f84a0.png?imageMogr2/auto-orient/strip|imageView2/2/w/640/format/webp.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不知道说啥&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-04-03周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/04/02/2020-04-03%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/04/02/2020-04-03%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-04-02T09:08:46.000Z</published>
    <updated>2020-04-03T11:50:56.026Z</updated>
    
    <content type="html"><![CDATA[<p>A Memory-Network Based Solution for Multivariate Time-Series Forecasting<br><a id="more"></a></p><h3 id="By-the-way"><a href="#By-the-way" class="headerlink" title="By the way"></a>By the way</h3><p>Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks(SIGIR2018)</p><ul><li>LSTNet</li><li>利用数据中的周期模式，GRU中计算$t$时刻的隐向量时，不是以$t-1$时刻的隐向量为输入，而是以$t-p$时刻的隐向量为输入($p$为周期)</li></ul><p><img src="https://pic2.zhimg.com/80/v2-2b3ff23e31f2a6fbc85058f5754b2c09_1440w.jpg" alt=""></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li>一种基于深度学习的时间序列预测模型MTNet</li><li>MTNet由一个大的内存组件、三个独立的编码器和一个联合训练的自回归组件组成</li><li>可解释性</li></ul><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>DA-RNN不考虑外源数据不同成分间的空间相关性。更重要的是，在第二阶段DA-RNN进行的点式注意可能不合适捕捉连续的周期模式</p></li><li><p>认为LSTNet网络为了考虑周期信息，需要引入超参$p$，在周期长度会发生变化的环境中，这个超参p是未知的</p></li></ul><h3 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h3><h4 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h4><script type="math/tex; mode=display">Y = {y_{1},y_{2},...,y_{T}},y_{t} \in R^{D}</script><p>D表示变量的维度</p><h4 id="Memory-Time-series-Network"><a href="#Memory-Time-series-Network" class="headerlink" title="Memory Time-series Network"></a>Memory Time-series Network</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/1585821126570.jpg" alt=""></p><p>long-term time series historical data</p><p>{$X_{i}$} = <script type="math/tex">X_{1},...,X_{n}</script></p><p>that are to be store in the memory</p><p>$Q$<br>a short-term historical time series data</p><p><strong>这里的{$X_{i}$}和Q不重叠</strong></p><p>Encoder Architecture</p><p>使用非池化的卷积层来提取时间维度上的短期模式和变量之间的局部依赖关系</p><script type="math/tex; mode=display">X \in T × D</script><p>卷积层由多个内核组成，尺寸都是w × D，所以最后的卷积结果就是(T-w+1) × 1</p><p>一共有$d_{c}$个过滤器（卷积核），得到了</p><script type="math/tex; mode=display">d_{c} × T_{c}</script><p>矩阵</p><p>加上attention层，再使用RNN(GRU)</p><p>首先，一条时序数据被分为长期的历史数据{$X_{i}$}和最近的历史数据$Q$ ，{$X_{i}$}和$Q$没有重合部分。<br>每一个X_{i}通过$Encoder_{m}$网络得到表示$m_{i}$ ；$Q$通过$Encoder_{in}$网络得到表示$u$。<br>其中$Encoder_{m}$和$Encoder_{in}$的结构都为上节所述的 Encoder 网络。</p><p>将$u$和每一个$m_{i}$做内积，在通过Softmax函数归一化，得到一系列的权重$p_{i}$。</p><script type="math/tex; mode=display">p_{i} = Softmax(u^{T}m_{i})</script><p><strong>本文的亮点是attention的设计，即权重$p_{i}$的得到。权重$p_{i}$越大表示Q和X_{i}位置之前的一段序列的相似程度大，则该段序列对于当前的预测更重要</strong></p><p>每一个$X_{i}$再通过$Encoder_{c}$网络得到表示$c_{i}$。每一个$c_{i}$再和$p_{i}$相乘得到$o_{i}$。</p><script type="math/tex; mode=display">o_{i} = p_{i} * c_{i}</script><p>将所有的$o_{i}$和$u$拼接后通过W矩阵转换，得到最终的预测输出：</p><script type="math/tex; mode=display">y_{t}^{D} = W^{D}[u;o_{1};o_{2};...;o_{T}] + b</script><p>Autoregressive Component（自回归模型）</p><script type="math/tex; mode=display">y_{t,i}^{L} = \sum_{k=0}^{s^{ar}-1}w_{k}^{ar}q_{t-k,i}+b^{ar}</script><p>The final prediction of MTNet</p><script type="math/tex; mode=display">y_{t} = y_{t}^{D} + y_{t}^{L}</script><p>将非线性（MTNet 中的神经网络部分）和线性（MTNet 中的自回归部分）模型进行ensemble。得到最终的预测输出。</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h4 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h4><p>2 univariate datasets:  Beijing PM2.51, GEFCom(2014) Electricity Price (Hong et al. 2016),</p><p>4 multivariate datasets: Traffic, Solar-Energy, Electricity,Exchange-Rate. </p><h4 id="Methods-for-comparison"><a href="#Methods-for-comparison" class="headerlink" title="Methods for comparison"></a>Methods for comparison</h4><h4 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h4><p>univariate: Root Mean Squared Error (RMSE)/Mean Absolute Error (MAE)</p><p>multivariate: Root Relative Squared Error (RRSE)/Empirical Correlation Coefficient (CORR)</p><h4 id="Interpretability-of-MTNet"><a href="#Interpretability-of-MTNet" class="headerlink" title="Interpretability of MTNet"></a>Interpretability of MTNet</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/1585836468820.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A Memory-Network Based Solution for Multivariate Time-Series Forecasting&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-3-27周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/03/26/2020-3-27%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/03/26/2020-3-27%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-03-26T09:05:17.000Z</published>
    <updated>2020-03-27T12:17:38.751Z</updated>
    
    <content type="html"><![CDATA[<p>PTMs-Pre-trained Models,PTMs(预训练模型)<br><a id="more"></a></p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>在图像领域，预训练过程是一个比较常规的做法，对于图像来说一般是CNN的多层叠加网络结构。一般步骤如下：</p><ul><li>先现在一个大数据集A上预训练模型，模型参数保存下来，设为Model_A</li><li>对于target任务B，在Model_A的基础上进行再训练Model_B，这时候一般策略是使用Model_A的底层网络参数初始化Model_B，上层参数随机初始化并训练</li></ul><p>对于底层参数，一般有两种策略：</p><ul><li>一种是Frozen，即训练Model_B过程中底层网络参数设为不可训练的，直接使用Model_A训练好的参数</li><li>另一种是Fine-tuning，即训练Model_B过程中底层网络参数设为可训练的，在Model_A训练好的参数基础上微调</li></ul><p>在NLP领域，使用word embedding</p><p>Word2vec—-&gt;ELMO(Embedding from Language Models)—-&gt;GPT(Generative Pre-Training)—-&gt;BERT(Bidirectional Encoder Representations from Transformers)</p><h3 id="word-embedding（词嵌入）"><a href="#word-embedding（词嵌入）" class="headerlink" title="word embedding（词嵌入）"></a>word embedding（词嵌入）</h3><p>高维词向量嵌入到一个低维空间</p><p>Embedding是数学领域的有名词，是指某个对象 X 被嵌入到另外一个对象 Y 中，映射 f : X → Y </p><p>Word Embedding 是NLP中一组语言模型和特征学习技术的总称，把词汇表中的单词或者短语映射成由实数构成的向量上(映射)</p><h4 id="One-Hot-Representation"><a href="#One-Hot-Representation" class="headerlink" title="One-Hot-Representation"></a>One-Hot-Representation</h4><p><img src="https://pic2.zhimg.com/80/v2-09e1bda72c4b903e25db203ab4aa6dc6_1440w.jpg" alt=""></p><p>在one hot representation编码的每个单词都是一个维度，彼此independent</p><p>语料库<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">John likes to watch movies.Marry likes too.</span><br><span class="line">John also likes to watch football games.</span><br></pre></td></tr></table></figure><br>词典<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "John": 1，</span><br><span class="line">    "likes": 2,</span><br><span class="line">    "to": 3,</span><br><span class="line">    "watch": 4,</span><br><span class="line">    "movies": 5,</span><br><span class="line">    "also": 6,</span><br><span class="line">    "football": 7, </span><br><span class="line">    "games": 8,</span><br><span class="line">    "Marry": 9,</span><br><span class="line">    "too": 10</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>one-hot表示<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">John:[1,0,0,0,0,0,0,0,0,0]</span><br><span class="line">likes:[0,1,0,0,0,0,0,0,0,0]</span><br><span class="line">...</span><br><span class="line">too:[0,0,0,0,0,0,0,0,0,1]</span><br></pre></td></tr></table></figure><br>再举个例子</p><p><img src="https://img-blog.csdnimg.cn/20190807181106401.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI2ODExMzc3,size_16,color_FFFFFF,t_70" alt=""><br>一共有四种状态，1，2，3，4<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1 -&gt; 0001</span><br><span class="line">2 -&gt; 0010</span><br><span class="line">3 -&gt; 0100</span><br><span class="line">4 -&gt; 1000</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/20190808002943747.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI2ODExMzc3,size_16,color_FFFFFF,t_70" alt=""></p><ul><li>无法捕捉两个word之间的关系，也就是没有办法捕捉语义信息</li><li>词向量可能非常长</li></ul><h4 id="Distributed-Representation"><a href="#Distributed-Representation" class="headerlink" title="Distributed-Representation"></a>Distributed-Representation</h4><h4 id="N-Gram"><a href="#N-Gram" class="headerlink" title="N-Gram"></a>N-Gram</h4><p>N-Gram是基于一个假设：第n个词出现与前n-1个词相关，而与其他任何词不相关</p><script type="math/tex; mode=display">S = (w_{1},w_{2},w_{3},...,w_{n})</script><p>假设每一个单词$w_{i}$都要依赖于第一个单词到$w_{1}$到他之前的一个单词$w_{i-1}$的影响</p><script type="math/tex; mode=display">p(S) = p(w_{1},w_{2},w_{3},...,w_{n})=p(w_{1})p(w_{2}|w_{1})...p(w_{n}|w_{n-1}w_{n-2}...w_{1})</script><p>不妨利用马尔科夫假设<br>即当前这个词仅仅跟前面几个有限的词相关，因此也就不必追溯到最开始的那个词，这样便可以大幅缩减上述算式的长度</p><script type="math/tex; mode=display">p(S) = p(w_{1},w_{2},w_{3},...,w_{n})=\prod p(w_{i}|w_{i-1}...w_{1})≈\prod p(w_{i}|w_{i-1}...w_{i-N+1})</script><p>当N=2时，称为Bi-Gram<br>当N=3时，称为Tri-Gram<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">I am John</span><br><span class="line">John I am</span><br><span class="line">I like learning</span><br></pre></td></tr></table></figure><br>此时Tri-Gram下</p><script type="math/tex; mode=display">p(am|I) = 2/3</script><p>两个重要应用场景</p><ul><li>评估句子之间差异性</li><li>评估一个句子是否合理<br>N-Gram距离<script type="math/tex; mode=display">s = "ABCD"</script><script type="math/tex; mode=display">t = "ABC"</script>当N=2时，第一个字符串可以拆成<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(AB,BC,CD)</span><br></pre></td></tr></table></figure>第二个字符串可以拆成<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(AB,BC)</span><br></pre></td></tr></table></figure>距离公式<script type="math/tex; mode=display">|G_{N}(s)| + |G_{N}(t)| - 2 \times |G_{N}(s) \cap G_{N}(t)|</script>d = 4 + 3 - 2 * 1 = 1</li></ul><h3 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h3><p>预备知识回顾</p><h4 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h4><script type="math/tex; mode=display">\sigma (x) = \frac{1}{1+e^{-x}}</script><p><img src="https://www.guanacossj.com/media/articlebodypics/sigmoid.jpg" alt=""><br>求导</p><script type="math/tex; mode=display">\sigma^{'} (x) = \sigma (x)[1-\sigma (x)]</script><p>易得</p><script type="math/tex; mode=display">[log\sigma (x)]^{'} = 1 - \sigma (x)</script><script type="math/tex; mode=display">[log(1-\sigma (x))]^{'} = - \sigma (x)</script><h4 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h4><script type="math/tex; mode=display">P(A|B) = \frac {P(A,B)}{P(B)}</script><script type="math/tex; mode=display">P(B|A) = \frac {P(A,B)}{P(A)}</script><script type="math/tex; mode=display">P(A|B) = P(A) \frac {P(B|A)}{P(A)}</script><h4 id="逻辑回归二分类器"><a href="#逻辑回归二分类器" class="headerlink" title="逻辑回归二分类器"></a>逻辑回归二分类器</h4><p>设</p><script type="math/tex; mode=display">\{\{x_{i},y_{i}\}\}_{i=1}^{m}</script><p>二分类函数长这样</p><script type="math/tex; mode=display">h_{\theta }(x) = \sigma (\theta _{0}+\theta _{1}x_{1}+\theta _{1}x_{1}+...+\theta _{n}x_{n})</script><p>令</p><script type="math/tex; mode=display">\theta =(\theta_{0},\theta_{1},\theta_{2},...,\theta_{n})^{T}</script><p>其中θ为待定参数</p><p>简化二分类函数</p><script type="math/tex; mode=display">h_{\theta }(x) = \sigma (\theta ^{T}x)=\frac{1}{1+e^{-\theta ^{T}x}}</script><p>取阈值T=0.5</p><h4 id="Huffman树-编码"><a href="#Huffman树-编码" class="headerlink" title="Huffman树-编码"></a>Huffman树-编码</h4><p>最优二叉树—-带权路径长度最短的二叉树</p><p>“我”，”喜欢”，”观看”，”巴西”，”足球”，”世界杯”<br> 15     8      6      5      3       1</p><p>选根节点最小的树合并</p><p><img src="https://www.guanacossj.com/media/articlebodypics/1585241496316.jpg" alt=""></p><p>词频越大的词离根节点越近<br>显然词频越小，权重越小</p><p><img src="https://www.guanacossj.com/media/articlebodypics/1585241408762.jpg" alt=""></p><h4 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h4><p>使用一个词预测上下文</p><h4 id="CBOW-Continues-Bag-of-Words-Model"><a href="#CBOW-Continues-Bag-of-Words-Model" class="headerlink" title="CBOW-Continues Bag-of-Words Model"></a>CBOW-Continues Bag-of-Words Model</h4><p>使用一个词语的上下文作为输入，来预测这个词语本身</p><p><img src="https://www.guanacossj.com/media/articlebodypics/1585297454343.jpg" alt=""></p><p><strong>输入层到隐藏层</strong></p><p>输入层是四个词的one-hot向量表示，分别是$x_{t-2}$,$x_{t-1}$,$x_{t+1}$,$x_{t+2}$，维度都是V×1，V是模型的训练本文中所有词的个数</p><p>输入层到隐藏层的权重矩阵为W，维度为V×d，d是认为给定的词向量维度，隐藏层的向量为h，维度是d×1</p><script type="math/tex; mode=display">h = \frac{W*x_{t-2}+W*x_{t-1}+W*x_{t+1}+W*x_{t+2}}{4}</script><p><strong>隐藏层到输出层</strong></p><p>记隐藏层到输出层的权重矩阵为U，维度为d×V，输出向量为y，维度为V×1，那么</p><script type="math/tex; mode=display">y = softmax(U^{T}*h)</script><p>此时输出层的向量y和输入层的向量x，虽然维度一样，但是y并不是one-hot向量，假设训练样本是</p><p>“I like to eat apple”，此时用”I”,”like”,”eat”,”apple”预测”to”，输出的y向量大概是这样</p><p><img src="https://pic4.zhimg.com/80/v2-918b97c077fe15b4a67e0afddb62bfa3_1440w.jpg" alt=""></p><p>目的是构造最大化函数L</p><script type="math/tex; mode=display">L = \prod_{t=1}^{V}p(w_{t}|w_{t-k},w_{t-k+1},...,w_{t-1},w_{t+1},...,w_{t+k-1},w_{t+k})</script><h4 id="层次softmax和负采样"><a href="#层次softmax和负采样" class="headerlink" title="层次softmax和负采样"></a>层次softmax和负采样</h4><p>层次softmax是一棵huffman树，树的叶子节点是训练文本中所有的词，非叶子节点都是一个逻辑回归二分类器，每个逻辑回归分类器的参数都不同，分别用$θ_{*}$表示。</p><p><img src="https://www.guanacossj.com/media/articlebodypics/1585301521653.jpg" alt=""></p><p>分类器的输入是向量h(隐藏层向量)</p><p>采样到 I 的概率$p(I|context) = (1-\sigma(\theta_{1}h)) * (1-\sigma(\theta_{3}h))$</p><p>采样到 eat 的概率$p(eat|context) = (1-\sigma(\theta_{1}h)) * \sigma(\theta_{3}h)$</p><p>采样到 to 的概率$p(to|context) = \sigma(\theta_{1}h) * (1-\sigma(\theta_{2}h))$</p><p>正样本</p><script type="math/tex; mode=display">p(w|context(w)) = \sigma(\theta^{w}h^{context(w})</script><p>负样本</p><script type="math/tex; mode=display">p(w|NEG(w)) = \sigma(\theta^{w}h^{NEG(w})</script><script type="math/tex; mode=display">L_{CBOW} = \prod_{t=1}^{V} p(w^{t}|context(w^{t}))p(w^{t}|NEG(w^{t}))</script><p>最大化—-&gt;梯度上升法</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PTMs-Pre-trained Models,PTMs(预训练模型)&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-3-20周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/03/20/2020-3-20%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/03/20/2020-3-20%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-03-20T09:08:59.000Z</published>
    <updated>2020-03-20T09:40:19.918Z</updated>
    
    <content type="html"><![CDATA[<p>searching and mining trillions of time series subsquences under dynamic time warping<br><a id="more"></a></p><h3 id="Assumptions"><a href="#Assumptions" class="headerlink" title="Assumptions"></a>Assumptions</h3><ul><li>Time Series Subsequences must be Normalized （时间序列子序列必须经过归一化处理）</li><li>Dynamic Time Warping is the Best Measure</li><li>Arbitrary Query Lengths cannot be Indexed</li><li>There Exists Data Mining Problems that we are Willing to Wait Some Hours to Answer</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;searching and mining trillions of time series subsquences under dynamic time warping&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-3-13周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/03/10/2020-3-13%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/03/10/2020-3-13%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-03-10T13:13:01.000Z</published>
    <updated>2020-03-13T05:36:21.778Z</updated>
    
    <content type="html"><![CDATA[<p>DTW(Dynamic Time Warping)<br>动态时间规整<br><a id="more"></a></p><h3 id="DTW-Dynamic-Time-Warping"><a href="#DTW-Dynamic-Time-Warping" class="headerlink" title="DTW(Dynamic Time Warping)"></a>DTW(Dynamic Time Warping)</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/dtw01.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/dtw02.jpg" alt=""></p><p>假设有两个序列：</p><p>a = [2, 0, 1, 1, 2, 4, 2, 1, 2, 0]</p><p>b = [1, 1, 2, 4, 2, 1, 2, 0]</p><p>用欧式距离计算出每序列的每两点之间的距离</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[1. 1. 0. 2. 0. 1. 0. 2.]</span><br><span class="line"> [1. 1. 2. 4. 2. 1. 2. 0.]</span><br><span class="line"> [0. 0. 1. 3. 1. 0. 1. 1.]</span><br><span class="line"> [0. 0. 1. 3. 1. 0. 1. 1.]</span><br><span class="line"> [1. 1. 0. 2. 0. 1. 0. 2.]</span><br><span class="line"> [3. 3. 2. 0. 2. 3. 2. 4.]</span><br><span class="line"> [1. 1. 0. 2. 0. 1. 0. 2.]</span><br><span class="line"> [0. 0. 1. 3. 1. 0. 1. 1.]</span><br><span class="line"> [1. 1. 0. 2. 0. 1. 0. 2.]</span><br><span class="line"> [1. 1. 2. 4. 2. 1. 2. 0.]]</span><br></pre></td></tr></table></figure><p>waring path W</p><p>$W = w_{1},w_{2},w_{3},…,w_{k}$ </p><p>$max(m,n) &lt;= k &lt; m+n-1$</p><p>$w_{k} = (i,j)_{k}$</p><script type="math/tex; mode=display">DTW(Q,C) = \min (\frac{\sum_{k=1}^{K}w_{k}}{K})</script><p>分母中的K主要是用来对不同的长度的规整路径做补偿</p><p>采用动态规划算法。假设我们要求到位置(𝑖,𝑗)的最小累计距离𝐷(𝑖,𝑗)，那么它只能由𝐷(𝑖−1,𝑗)，𝐷(𝑖,𝑗−1)和𝐷(𝑖−1,𝑗−1)这三个位置的最小累计距离中寻找，也就是</p><script type="math/tex; mode=display">𝐷(i,j)=d_{i,j}+𝑚𝑖𝑛[𝐷(i−1,j),𝐷(i,j−1),𝐷(i−1,j−1)]</script><p><img src="https://www.guanacossj.com/media/articlebodypics/dtw.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>DTW方法是欧氏距离方法的改进，只改进了其不能处理local time shifting的问题。没有引入任何阈值参数，因此对时间上的偏移（噪声和离群点）的抑制并不好，且对时间上的偏移的适应性也不好。</p><p>优点：使用动态规划的思想，实现了对某些点的重复使用，确保重复使用的点达成的路径最优的，从而较为高效地解决了数据不对齐的问题。</p><p>缺点：还是无法处理离群点、异常点，对于噪声的抑制没有进行处理。虽然能够处理local time shifting，但是对时间上的偏移做的也不好。算法也不是metric类型的。</p><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DTW(Dynamic Time Warping)&lt;br&gt;动态时间规整&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-3.6周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/03/06/2020-3-6%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/03/06/2020-3-6%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-03-06T04:34:50.000Z</published>
    <updated>2020-03-11T08:49:17.306Z</updated>
    
    <content type="html"><![CDATA[<p>Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models<br><a id="more"></a></p><h3 id="Shape-and-Time-Distortion-Loss-for-Training-DeepTime-Series-Forecasting-Models"><a href="#Shape-and-Time-Distortion-Loss-for-Training-DeepTime-Series-Forecasting-Models" class="headerlink" title="Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models"></a>Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models</h3><p>NeurIPS 2019</p><p>训练深度时间序列预测模型的形状和时间失真损失</p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>This paper addresses the problem of time series forecasting for non-stationary signals and multiple future steps prediction. </p><p>DILATE (DIstortion Loss including shApe and TimE) 形状和时间失真损失</p><p>DILATE aims at accurately predicting sudden changes, and explicitly incorporates two terms supporting precise shape and temporal change detection.</p><p><img src="https://img-blog.csdn.net/20180824212209631?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMxODIxNjc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><p><img src="https://img-blog.csdn.net/20180824212233242?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMxODIxNjc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><p><img src="https://img-blog.csdn.net/2018082421225311?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMxODIxNjc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>Time series forecasting [6] consists in analyzing the dynamics and correlations between historical data for predicting future behavior</p><p>In one-step prediction problems [39, 30], future prediction reduces to a single scalar value. This is in sharp contrast with multi-step time series prediction [49, 2, 48], which consists in predicting a complete trajectory[trəˈdʒektəri] of future data at a rather long temporal extent. Multi-step forecasting thus requires to accurately describe time series evolution.</p><p><img src="https://pic1.zhimg.com/v2-b872cd50b4a341901005bf4246493fa0_r.jpg" alt=""></p><p>(a) Non informative prediction 非信息性预测<br>(b) Correct shape, time delay<br>(c) Correct time, inaccurate shape</p><p>In contrast, the DILATE loss proposed in this work, which disentangles shape and temporal decay terms,<br>supports predictions (b) and (c) over prediction (a) that does not capture the sharp change of regime.</p><h4 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h4><p>Time series forecasting Traditional methods for time series forecasting include linear autoregressive models, such as the ARIMA model [6], and Exponential[ˌekspəˈnenʃl] Smoothing [27], which both fall into the broad category of linear State Space Models (SSMs) [17].</p><h4 id="Training-Deep-Neural-Networks-with-DILATE-DIstortion-Loss-including-shApe-and-TimE"><a href="#Training-Deep-Neural-Networks-with-DILATE-DIstortion-Loss-including-shApe-and-TimE" class="headerlink" title="Training Deep Neural Networks with DILATE((DIstortion Loss including shApe and TimE))"></a>Training Deep Neural Networks with DILATE((DIstortion Loss including shApe and TimE))</h4><p><img src="https://pic3.zhimg.com/v2-fc51e16266d817369cbd3bcbd6624552_b.jpg" alt=""></p><p>a set of N input time series:</p><script type="math/tex; mode=display">A=\{ X_{i} \}_{i\in \{1:N\}}</script><p>对于</p><script type="math/tex; mode=display">x_{i} = (x_{i}^{1},...,x_{i}^{n})</script><p>predicts the future <strong>k-step</strong> ahead trajectory </p><script type="math/tex; mode=display">\hat{y}_{i} = (\hat{y}_{i}^{1},...,\hat{y}_{i}^{k})</script><p>actual ground truth future trajectory</p><script type="math/tex; mode=display">\dot{y}_{i} = (\dot{y}_{i}^{1},...,\dot{y}_{i}^{k})</script><p><img src="https://www.guanacossj.com/media/articlebodypics/1583471554346.jpg" alt=""></p><p><script type="math/tex">\alpha \in [0,1]</script>  hyper parameter [ˈhaɪpə(r) pəˈræmɪtə(r)] </p><p>Notations and definitions(符号和定义):</p><ul><li>A: a warping path as a binary matrix(二值矩阵)</li></ul><h4 id="Shape-and-temporal-terms"><a href="#Shape-and-temporal-terms" class="headerlink" title="Shape and temporal terms"></a>Shape and temporal terms</h4><h5 id="Shape-term"><a href="#Shape-term" class="headerlink" title="Shape term"></a>Shape term</h5><p>Shape term Our shape loss function is based on the Dynamic Time Warping (DTW)</p><p>The DTW loss focuses on the structural shape dissimilarity between signals</p><p>Temporal term Our second term Ltemporal in Eq (1) aims at penalizing temporal distortions between $\hat{y}_{i}$ $\dot{y}_{i}$</p><h4 id="DILATE-Efficient-Forward-and-Backward-Implementation"><a href="#DILATE-Efficient-Forward-and-Backward-Implementation" class="headerlink" title="DILATE Efficient Forward and Backward Implementation"></a>DILATE Efficient Forward and Backward Implementation</h4><h4 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h4><p>To illustrate the relevance of DILATE, we carry out experiments on 3 non-stationary time series datasets from different domains </p><p><img src="https://pic3.zhimg.com/v2-b22b26e263592f889b52486b8c1f85ce_b.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-2-28周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/02/28/2020-2-28%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/02/28/2020-2-28%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-02-28T04:22:34.000Z</published>
    <updated>2020-02-28T06:32:20.573Z</updated>
    
    <content type="html"><![CDATA[<p>Memory In Memory（学习高阶非平稳特征信息）<br><a id="more"></a><br>Memory In Memory: A Predictive Neural Network for Learning Higher-Order Non-Stationarity from Spatiotemporal Dynamics<br>一种用于高阶学习的预测神经网络时空动力学中的非平稳性</p><p>cvpr2019</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>any non-stationary process can be decomposed into deterministic, time-variant polynomials[ˌpɒlɪˈnəʊmiəlz] , plus a zero-mean stochastic term. </p><p>任何一个非平稳过程都可以分解为：确定项+时间变量多项式+零均值随机项</p><p>By applying differencing operations appropriately, we may turn time-variant polynomials into a constant, making the deterministic[dɪˌtɜːmɪˈnɪstɪk] component predictable.</p><p>通过差分的操作，我们可以把时间变量多项式转换成一个常量，使确定性的组成部分可预测</p><p>We propose the Memory In Memory (MIM) networks and corresponding recurrent blocks for this purpose. The MIM blocks exploit the differential signals between adjacent recurrent states to model the non-stationary and approximately stationary properties in spatiotemporal dynamics with two cascaded, self-renewed memory modules.</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>We attempt to resolve this problem by proposing a generic RNNs architecture that is more effective in non-stationarity modeling. </p><p>In particular, the forget gates in the recent PredRNN model [32] does not work appropriately on precipitation forecasting: about 80% of them are saturated over all timestamps, implying almost timeinvariant memory state transitions. </p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h4 id="ARIMA-Autoregressive-Integrated-Moving-Average-Model"><a href="#ARIMA-Autoregressive-Integrated-Moving-Average-Model" class="headerlink" title="ARIMA(Autoregressive Integrated Moving Average Model)"></a>ARIMA(Autoregressive Integrated Moving Average Model)</h4><p>A time-series random variable whose power spectrum remains constant over time can be viewed as a combination of signal and noise. </p><p>功率谱是功率谱密度函数（PSD）的简称，它定义为单位频带内的信号功率</p><p><img src="https://www.guanacossj.com/media/articlebodypics/w.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/w_f.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/p.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/f.jpg" alt=""></p><script type="math/tex; mode=display">|S(f)|^2</script><script type="math/tex; mode=display">\lim_{T->\propto }\frac{1}{T}|S(f)|^2</script><h4 id="Deterministic-Spatiotemporal-Prediction"><a href="#Deterministic-Spatiotemporal-Prediction" class="headerlink" title="Deterministic Spatiotemporal Prediction"></a>Deterministic Spatiotemporal Prediction</h4><h4 id="Stochastic-Spatiotemporal-Prediction"><a href="#Stochastic-Spatiotemporal-Prediction" class="headerlink" title="Stochastic Spatiotemporal Prediction"></a>Stochastic Spatiotemporal Prediction</h4><h3 id="Memory-In-Memory"><a href="#Memory-In-Memory" class="headerlink" title="Memory In Memory"></a>Memory In Memory</h3><p><img src="https://pic1.zhimg.com/v2-dc4a2024ce0201315661daf3b43c6ab8_r.jpg" alt=""></p><p>左边是ST-LSTM结构，右边是更改的</p><p>ST-LSTM中的忘记门基本是饱和的，所以它基本上只获取了平稳的信息，而整个直接联系就是C状态值，再加上下面的输入为差分，而差分的转换其实就是非平稳的信息</p><p><img src="https://pic1.zhimg.com/v2-69b9793882f1a8739d6b1d37336cc808_r.jpg" alt=""></p><p><img src="https://pic3.zhimg.com/v2-f1eac55dddaa00ab46da1b4ea116072e_r.jpg" alt=""></p><p><img src="https://pic3.zhimg.com/80/v2-6ec88fa03dd40e82a037c13ff6b64bd2_1440w.jpg" alt=""></p><h3 id="Memory-In-Memory-Networks"><a href="#Memory-In-Memory-Networks" class="headerlink" title="Memory In Memory Networks"></a>Memory In Memory Networks</h3><p><img src="https://pic3.zhimg.com/v2-06741d37a4c01fce37ef43901f5b311e_r.jpg" alt=""></p><p>红色箭头：用于微分建模的H的对角状态转移路径</p><p>蓝色箭头：存储单元C，N和S的水平转换路径</p><p>黑色箭头：之字形状态</p><h3 id="experience"><a href="#experience" class="headerlink" title="experience"></a>experience</h3><p>模型参数：一共四层，第一层是ST-LSTM，其余三层为MIM，MIM的feature channel为64，利用l2损失，ADAM optimizer，lr为0.001，利用了两个trick，为layer nomalization和scheduled sampling</p><p><img src="https://pic3.zhimg.com/v2-fec039cb2962195deb890b6680c5395a_r.jpg" alt=""></p><h3 id="GluonTS-AWS"><a href="#GluonTS-AWS" class="headerlink" title="GluonTS(AWS)"></a>GluonTS(AWS)</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/gluonts_all.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/gluonts67.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Memory In Memory（学习高阶非平稳特征信息）&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-2-21周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/02/20/2020-2-21%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/02/20/2020-2-21%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-02-20T12:53:19.000Z</published>
    <updated>2020-02-21T07:20:17.207Z</updated>
    
    <content type="html"><![CDATA[<p>PredRNN++…<br><a id="more"></a></p><h3 id="PredRNN"><a href="#PredRNN" class="headerlink" title="PredRNN++:"></a>PredRNN++:</h3><p>Towards A Resolution of the Deep-in-Time Dilemma in Spatiotemporal Predictive Learning</p><p>旨在解决时空预测的深层次时间困境</p><p>ICML2018 Tsinghua</p><h4 id="PredRNN-1"><a href="#PredRNN-1" class="headerlink" title="PredRNN"></a>PredRNN</h4><p>nips2017 Tsinghua</p><p>PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs</p><p>用ST-LSTM的预测学习循环神经网络(spatial + temporal)</p><p>PredRNN利用了一种双重记忆机制，通过简单的门控级联，将水平更新的时间记忆C与垂直转换的空间记忆M结合起来</p><p>先来回忆一下LSTM</p><p><img src="https://pic2.zhimg.com/v2-810f2d553fa6e6f43854efdc881be8a1_r.jpg" alt=""></p><ol><li>$h_{t-1}$与$X_{t}$做concat操作，之后经过sigmoid形成[0, 1]的忘记门，输入门，输出门</li><li>Ct-1通过忘记门 </li><li>ht-1与Xt做concat操作通过tanh激活函数，通过输入门（这里在通过输入门之前相当于生成了此时的输入生成状态） </li><li>以上通过遗忘门和输入门的两个向量相加就是最后的Ct，也就是此时的cell state</li><li>最后，这个cell state通过再一次的非线性变化tanh 最终通过输出门输出得到最后的ht</li></ol><p>Spatiotemporal memory flow</p><p><img src="https://pic4.zhimg.com/80/v2-7c898aed50f1e1d9aee647e4c273ad33_hd.jpg" alt=""></p><p><img src="https://pic1.zhimg.com/80/v2-f5c836f08237baea9393aefce80d0fd8_hd.jpg" alt=""></p><p>缺点:</p><ol><li>去掉水平方向的时间流，会牺牲时间上的一致性，因为在同一层的不同时间没有时间流了。 </li><li>记忆需要在遥远的状态之间流动更长的路径，更容易造成梯度消失。 所以引入了一个新的building blocks为ST-LSTM。</li></ol><p>Spatiotemporal LSTM</p><p><img src="https://pic4.zhimg.com/v2-f3cd76086384ede22b29e6c8a5f7f45b_r.jpg" alt=""></p><p><img src="https://pic2.zhimg.com/80/v2-fdb1465c439cd9f3eea4ee52bf2b4125_hd.jpg" alt=""></p><p>震惊！！！</p><ul><li>上半部分就是LSTM(Standard Temporal Memory)</li><li>下半部分相当于把c和h一起更改为M，M即时空记忆状态(Spatiotemporal Memory)</li></ul><p><img src="https://pic2.zhimg.com/80/v2-bbe7560a50ff9d511746fb94562ebd39_hd.jpg" alt=""></p><h4 id="PredRNN-2"><a href="#PredRNN-2" class="headerlink" title="PredRNN++"></a>PredRNN++</h4><ul><li>Stacked ConvLSTMs(nips2015)</li><li>Deep Transition ConvLSTMs</li><li>Pred RNN 红线表示空间记忆的深度过渡路径，水平的黑色箭头表示时间记忆的更新方向<br><img src="https://img-blog.csdnimg.cn/20191222210421460.png?#pic_center" alt=""></li></ul><p>Causal LSTM(因果长短期记忆)<br>通过这种方式，将获得更强大的建模能力，以实现更强的空间相关性和短期动态</p><p><img src="https://img-blog.csdnimg.cn/20191222205916540.png?#pic_center" alt=""></p><ul><li>每个门不是由X和H决定，而是由X和H以及C决定，通过输入门之前的状态也是由三者决定的</li><li>两个memory结构，即C和M，C为temporal state，M为spatial state，因为输入C为上一个时刻的C，M是上一层的M，所以这里C与时间维度有关，M与空间维度有关</li><li>M作为第二部分的state输入，并且通过忘记门之前做了一个非线性操作tanh</li></ul><p>对比ST-LSTM来说，Causal LSTM对于M和H定义更加清晰，并且不是简单的concat，而是采用了一个递归深度更深的一个级联结构最终输出H</p><p>Gradient Highway(高速梯度)</p><p>通过Recurrent Highway Networks的思想能够证明高速网络能够有效的在非常深的网络中传递梯度，继而防止长时导致的梯度消失</p><p><img src="https://pic3.zhimg.com/80/v2-3b541bda171d5f4c9299c77326e13702_hd.jpg" alt=""></p><p><img src="https://pic1.zhimg.com/80/v2-cf06c70d336a89700435195a0574b39c_hd.jpg" alt=""></p><p>总体架构</p><p><img src="https://pic3.zhimg.com/80/v2-61d4c59ff38010cb9613574bf0290c9a_hd.jpg" alt=""></p><p>GHU连接了当前时刻以及前一个时刻的输入，引导的结果就是梯度不再是一股线传播了，而是可以直接在第一层与第二层之间有个高速的传播，换句话讲就是传播的距离缩短了，也就变得没有之前的那么’深‘了，可以有效的解决梯度消失的问题</p><h3 id="Block-Hankel-Tensor-ARIMA-for-Multiple-Short-Time-Series-Forecasting"><a href="#Block-Hankel-Tensor-ARIMA-for-Multiple-Short-Time-Series-Forecasting" class="headerlink" title="Block Hankel Tensor ARIMA for Multiple Short Time Series Forecasting"></a>Block Hankel Tensor ARIMA for Multiple Short Time Series Forecasting</h3><h4 id="Hankel-汉克尔矩阵（每一条逆对角线上的元素都相等的矩阵）"><a href="#Hankel-汉克尔矩阵（每一条逆对角线上的元素都相等的矩阵）" class="headerlink" title="Hankel:汉克尔矩阵（每一条逆对角线上的元素都相等的矩阵）"></a>Hankel:汉克尔矩阵（每一条逆对角线上的元素都相等的矩阵）</h4><script type="math/tex; mode=display">\begin{bmatrix} 1&  2&  3&  4&  5&  6& 7\\  2&  3&  4&  5&  6&  7& 8\\  3&  4&  5&  6&  7&  8& 9\end{bmatrix}</script><h4 id="Tucker分解"><a href="#Tucker分解" class="headerlink" title="Tucker分解"></a>Tucker分解</h4><p>这是一个三阶张量</p><p><img src="/Users/Arithmetic/Pictures/tensor.png" alt=""></p><p>秩一张量：如果一个K阶张量能够表示成K个向量的外积，那么该张量称为秩一张量<br>[[3 4],[6 8]]这个二阶张量可以表示为[1 2]○[3 4]的外积，那么这就是一个二阶秩一张量<br><img src="https://www.guanacossj.com/media/articlebodypics/1582207111495.jpg" alt=""></p><p>CP分解<br>CP分解其实就是多个rank-one tensors的和</p><p><img src="/Users/Arithmetic/Pictures/cp.png" alt=""></p><p>公式表示如下：</p><p><img src="/Users/Arithmetic/Pictures/cp_f.png" alt=""></p><p>tucker分解</p><p><img src="http://www.xiongfuli.com/assets/img/201606/tucker.png" alt=""></p><p><img src="/Users/Arithmetic/Pictures/tucker_f.png" alt=""></p><p>这里A$\in$R$^{I\times P}$,B$\in$R$^{J\times Q}$,C$\in$R$^{K\times R}$是因子矩阵（通常是正交的），可以当做是每一维上的主要成分。核张量表示每一维成分之间的联系<br>因此，对于一个三阶张量，可以通过tucker分解为三个二阶因子矩阵和一个三阶核向量</p><h4 id="Step1-Block-Hankel-Tensor-via-MDT"><a href="#Step1-Block-Hankel-Tensor-via-MDT" class="headerlink" title="Step1: Block Hankel Tensor via MDT"></a>Step1: Block Hankel Tensor via MDT</h4><p>MDT:multi-way delay embedding transform(多路延迟变换)<br>目的是利用MDT将多个TS转换成一个高阶的块Hankel张量<br>假设有1000条时间序列，每条序列的长度为40，即 I = 1000，T=40，设置参数t = 5<br>经MDT沿着时间维度变换后，得到一个1000✖️5*✖️（40-5+1）=1000✖️5✖️36的三维张量</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PredRNN++…&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-2.14周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/02/13/2020-2-14%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/02/13/2020-2-14%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-02-13T13:27:22.000Z</published>
    <updated>2020-02-14T07:33:34.201Z</updated>
    
    <content type="html"><![CDATA[<p>DA-RNN + Transformer + CW-RNN<br><a id="more"></a></p><h3 id="彩蛋"><a href="#彩蛋" class="headerlink" title="彩蛋"></a>彩蛋</h3><ul><li>写了一篇关于使用百度Echarts绘制新型冠状病毒全国分布图的博客</li><li>收到来自皖南医学院弋矶山医院教育处金来润的邮件希望合作</li><li>其实就是他们团队收集了一点疫情数据打算发一篇文章，希望我按照他们的要求给他们画个图</li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/QQ20200214-0.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/QQ20200214-1.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/echarts.png" alt=""></p><h3 id="DA-RNN"><a href="#DA-RNN" class="headerlink" title="DA-RNN"></a>DA-RNN</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-67-1.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-67-2.jpg" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.256</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.348</span><br></pre></td></tr></table></figure><h3 id="CW-RNN-Clock-Work-RNN"><a href="#CW-RNN-Clock-Work-RNN" class="headerlink" title="CW-RNN(Clock Work RNN)"></a>CW-RNN(Clock Work RNN)</h3><p>ICML2014</p><p>本文介绍了对标准RNN体系结构的一个简单而强大的改进，即时钟工作RNN（CW-RNN），它将隐藏层划分为不同的模块，每个处理以自己的时间粒度输入，仅以指定的时钟速率进行计算。<br>CW-RNN没有使标准RNN模型更加复杂，而是减少了RNN参数的数量，显著提高了测试任务的性能，加快了网络评估的速度</p><p>Input = ($x_{1}$,$x_{2}$,…,$x_{t}$,…)</p><p>Output = ($y_{1}$,$y_{2}$,…,$y_{t}$,…)</p><p><img src="https://www.guanacossj.com/media/articlebodypics/cwrnn.jpg" alt=""></p><ul><li>把隐含层节点分成了若干个模块（在图中分成了3个模块，是为了方便说明，实际中的模块个数可以自定义），而且每个模块都分配了一个时钟周期（Ti），便于独立管理</li><li>隐含层之间的连接，在一个模块内部是全连接，但是模块之间是有方向的。模块之间的连接是从高时钟频率的模块指向低时钟频率的模块</li><li>标准RNN<script type="math/tex; mode=display">y_{H}^{(t)} = f_{H}(W_{H}*y^{(t-1)}+W_{I}*x^{(t)})</script><script type="math/tex; mode=display">y_{O}^{(t)} = f_{O}(W_{O}*y_{H}^{(t)})</script></li><li>只有当$t$ MOD $T_{i}$ = 0时才会被执行</li><li>{$T_{1}$,…,$T_{g}$}的设置是任意的，在论文中使用$T_{i}=2^{i-1}$</li></ul><p>$T_{1}=1$,$T_{2}=2$,$T_{3}=4$,$T_{4}=8$,$T_{5}=16$,$T_{6}=32$,$T_{7}=64$</p><ul><li>分块<script type="math/tex; mode=display">W_{H} = \begin{pmatrix}\\ W_{H_{1}}\\ .\\ .\\ W_{H_{g}}\end{pmatrix}</script><script type="math/tex; mode=display">W_{I} = \begin{pmatrix}\\ W_{I_{1}}\\ .\\ .\\ W_{I_{g}}\end{pmatrix}</script></li><li>不参与运算的部分置零<script type="math/tex; mode=display">\left\{\begin{matrix}\\ W_{H_{i}},t mod T_{i} = 0\\ 0,otherwise\end{matrix}\right.</script></li><li>将$W_{h}$强制转成上三角<script type="math/tex; mode=display">W_{H_{i}}={0_{1},...,0_{i-1},W_{H_{i,i}},...,W_{H_{i,g}}}</script></li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/cwrnn-2.jpg" alt=""></p><p>我们要处理序列中第6（t=6）个元素的时候，通过t与每个模块的时钟周期进行MOD（求余数）计算后可以得到只有前两个模块会参与运算。所以$W_{h}$和$W_{x}$矩阵除了上面两行之外，其他元素的值都是0。经过计算之后，得到的$h_{t}$也只有前两个模块有值。因此，我们也可以把CW-RNN过程看成是通过一些人工的干预，选择不同的隐含层节点进行工作</p><p><img src="http://ir.dlut.edu.cn/Uploads/ue/image/20151201/6358457411082925904194442.jpg" alt=""></p><ul><li>低时钟速率模块处理、保留和输出从输入中获得的长期信息序列，</li><li>高时钟速率模块则侧重于本地的高频信息</li></ul><p>作者对比了传统RNN、LSTM、CW-RNN，在取局部图的时候可以观察到，LSTM的回归效果相对平滑，而CW-RNN并没有这种缺陷<br><img src="https://www.guanacossj.com/media/articlebodypics/comparecwrnn.jpg" alt=""></p><p>我复现了下股票数据集上的效果，实验还没完全完成<br><img src="https://www.guanacossj.com/media/articlebodypics/cwrnn-stock.jpg" alt=""></p><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/multi_head_net.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/transf-67-1.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/transf-67-2.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DA-RNN + Transformer + CW-RNN&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-1-17周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/01/17/2020-1-17%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/01/17/2020-1-17%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-01-17T08:38:22.000Z</published>
    <updated>2020-01-17T09:58:07.962Z</updated>
    
    <content type="html"><![CDATA[<p>Django+uwsgi+Nginx<br><a id="more"></a></p><h3 id="WSGI"><a href="#WSGI" class="headerlink" title="WSGI"></a>WSGI</h3><p>WSGI Web Server Gateway Interface</p><p><img src="https://pic1.zhimg.com/80/v2-6c4572c783816364f2569af961814430_hd.jpg" alt=""></p><p>WSGI是一种通信协议，WSGI 不是框架，也不是一个模块，而是介于 Web应用程序（Web框架）与 Web Server 之间交互的一种规范。</p><h3 id="uwsgi"><a href="#uwsgi" class="headerlink" title="uwsgi"></a>uwsgi</h3><ul><li>二进制协议，可以携带任何类型的数据。一个uwsgi分组的头4个字节描述了这个分组包含的数据类型。</li><li>uwsgi是一种线路协议而不是通信协议，在此常用于在uWSGI服务器与其他网络服务器的数据通信。</li></ul><h3 id="uWSGI"><a href="#uWSGI" class="headerlink" title="uWSGI"></a>uWSGI</h3><p>uWSGI是实现了uwsgi和WSGI两种协议的Web服务器，使用c语言开发。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> uwsgi</span><br></pre></td></tr></table></figure><ul><li>两级结构 在这种结构里，uWSGI作为服务器，它用到了HTTP协议以及wsgi协议，flask应用作为application，实现了wsgi协议。当有客户端发来请求，uWSGI接受请求，调用flask app得到相应，之后相应给客户端。 这里说一点，通常来说，Flask等web框架会自己附带一个wsgi服务器(这就是flask应用可以直接启动的原因)，但是这只是在开发阶段用到的，在生产环境是不够用的，所以用到了uwsgi这个性能高的wsgi服务器。</li><li>三级结构 在这种结构里，uWSGI作为中间件，它用到了uwsgi协议(与nginx通信)，wsgi协议(调用Flask app)。</li><li>提高web server性能(uWSGI处理静态资源不如nginx；nginx会在收到一个完整的http请求后再转发给wWSGI)。</li><li>nginx可以做负载均衡(前提是有多个服务器)，保护了实际的web服务器(客户端是和nginx交互而不是uWSGI)。</li></ul><h3 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a>Nginx</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install nginx</span><br></pre></td></tr></table></figure><p>Nginx是一款轻量级的Web服务器、反向代理服务器，由于它的内存占用少，启动极快，高并发能力强，在互联网项目中广泛应用。</p><p><img src="https://pic2.zhimg.com/80/v2-4787a512240b238ebf928cd0651e1d99_hd.jpg" alt=""></p><h3 id="Django-uWSGI-Nginx"><a href="#Django-uWSGI-Nginx" class="headerlink" title="Django + uWSGI + Nginx"></a>Django + uWSGI + Nginx</h3><p><img src="https://img-blog.csdnimg.cn/20181216174304355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d5bWFpc3ls,size_16,color_FFFFFF,t_70" alt=""></p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">user</span> www-data;</span><br><span class="line"><span class="attribute">worker_processes</span> auto;</span><br><span class="line"><span class="attribute">pid</span> /run/nginx.pid;</span><br><span class="line"><span class="attribute">include</span> /etc/nginx/modules-enabled/<span class="regexp">*.conf</span>;</span><br><span class="line"></span><br><span class="line"><span class="section">events</span> &#123;</span><br><span class="line"><span class="attribute">worker_connections</span> <span class="number">768</span>;</span><br><span class="line"><span class="comment"># multi_accept on;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">http</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Basic Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="section">server</span> &#123;   <span class="comment"># 这个server标识我要配置了</span></span><br><span class="line"><span class="attribute">listen</span> <span class="number">80</span>;  <span class="comment"># 我要监听那个端口</span></span><br><span class="line"><span class="attribute">server_name</span> <span class="number">118.25.79.249</span> ;  <span class="comment"># 你访问的路径前面的url名称</span></span><br><span class="line"><span class="attribute">charset</span>  utf-<span class="number">8</span>; <span class="comment"># Nginx编码</span></span><br><span class="line"><span class="attribute">gzip</span> <span class="literal">on</span>;  <span class="comment"># 启用压缩,这个的作用就是给用户一个网页,比如3M压缩后1M这样传输速度就会提高很多</span></span><br><span class="line"><span class="attribute">gzip_types</span> text/plain application/x-javascript text/css text/javascript application/x-httpd-php application/json text/json image/jpeg image/gif image/png application/octet-stream;  <span class="comment"># 支持压缩的类型</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">error_page</span>  <span class="number">404</span>           /<span class="number">404</span>.html;  <span class="comment"># 错误页面</span></span><br><span class="line"><span class="attribute">error_page</span>   <span class="number">500</span> <span class="number">502</span> <span class="number">503</span> <span class="number">504</span>  /50x.html;  <span class="comment"># 错误页面</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定项目路径uwsgi</span></span><br><span class="line"><span class="attribute">location</span> / &#123;        <span class="comment"># 这个location就和咱们Django的url(r'^admin/', admin.site.urls),</span></span><br><span class="line"><span class="attribute">include</span> uwsgi_params;  <span class="comment"># 导入一个Nginx模块他是用来和uWSGI进行通讯的</span></span><br><span class="line"><span class="attribute">uwsgi_connect_timeout</span> <span class="number">30</span>;  <span class="comment"># 设置连接uWSGI超时时间</span></span><br><span class="line"><span class="attribute">uwsgi_pass</span>  <span class="number">127.0.0.1:8000</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定静态文件路径</span></span><br><span class="line"><span class="attribute">location</span> /static/ &#123;</span><br><span class="line"><span class="attribute">alias</span>  /home/mysite/static/;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="attribute">sendfile</span> <span class="literal">on</span>;</span><br><span class="line"><span class="attribute">tcp_nopush</span> <span class="literal">on</span>;</span><br><span class="line"><span class="attribute">tcp_nodelay</span> <span class="literal">on</span>;</span><br><span class="line"><span class="attribute">keepalive_timeout</span> <span class="number">65</span>;</span><br><span class="line"><span class="attribute">types_hash_max_size</span> <span class="number">2048</span>;</span><br><span class="line"><span class="comment"># server_tokens off;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># server_names_hash_bucket_size 64;</span></span><br><span class="line"><span class="comment"># server_name_in_redirect off;</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">include</span> /etc/nginx/mime.types;</span><br><span class="line"><span class="attribute">default_type</span> application/octet-stream;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># SSL Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">ssl_protocols</span> TLSv1 TLSv1.<span class="number">1</span> TLSv1.<span class="number">2</span>; <span class="comment"># Dropping SSLv3, ref: POODLE</span></span><br><span class="line"><span class="attribute">ssl_prefer_server_ciphers</span> <span class="literal">on</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Logging Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">access_log</span> /var/log/nginx/access.log;</span><br><span class="line"><span class="attribute">error_log</span> /var/log/nginx/error.log;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Gzip Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">gzip</span> <span class="literal">on</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># gzip_vary on;</span></span><br><span class="line"><span class="comment"># gzip_proxied any;</span></span><br><span class="line"><span class="comment"># gzip_comp_level 6;</span></span><br><span class="line"><span class="comment"># gzip_buffers 16 8k;</span></span><br><span class="line"><span class="comment"># gzip_http_version 1.1;</span></span><br><span class="line"><span class="comment"># gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Virtual Host Configs</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">include</span> /etc/nginx/conf.d/<span class="regexp">*.conf</span>;</span><br><span class="line"><span class="attribute">include</span> /etc/nginx/sites-enabled/*;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#mail &#123;</span></span><br><span class="line"><span class="comment">## See sample authentication script at:</span></span><br><span class="line"><span class="comment">## http://wiki.nginx.org/ImapAuthenticateWithApachePhpScript</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">## auth_http localhost/auth.php;</span></span><br><span class="line"><span class="comment">## pop3_capabilities "TOP" "USER";</span></span><br><span class="line"><span class="comment">## imap_capabilities "IMAP4rev1" "UIDPLUS";</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#server &#123;</span></span><br><span class="line"><span class="comment">#listen     localhost:110;</span></span><br><span class="line"><span class="comment">#protocol   pop3;</span></span><br><span class="line"><span class="comment">#proxy      on;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#server &#123;</span></span><br><span class="line"><span class="comment">#listen     localhost:143;</span></span><br><span class="line"><span class="comment">#protocol   imap;</span></span><br><span class="line"><span class="comment">#proxy      on;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[uwsgi] </span><br><span class="line">chdir = /home/mysite</span><br><span class="line">module = mysite.wsgi:application</span><br><span class="line">socket = 127.0.0.1:8000</span><br><span class="line">master = true </span><br><span class="line">processes = 1</span><br><span class="line">threads = 2</span><br><span class="line">max-requests = 6000</span><br><span class="line">chmod-socket = 666</span><br><span class="line">buffer-size = 65535</span><br><span class="line">logto = /var/log/mysite.log</span><br><span class="line">async</span><br><span class="line">ugreen =''</span><br><span class="line">http-timeout = 300</span><br><span class="line"><span class="comment">#plugins=python</span></span><br></pre></td></tr></table></figure><h3 id="Activemq"><a href="#Activemq" class="headerlink" title="Activemq"></a>Activemq</h3><p>ActiveMQ 是 Apache 出品，最流行的，能力强劲的开源消息总线。ActiveMQ 是一个完全支持 JMS1.1 和 J2EE 1.4 规范的 JMS Provider 实现。</p><h4 id="queue"><a href="#queue" class="headerlink" title="queue"></a>queue</h4><p><img src="https://pic4.zhimg.com/80/v2-b7edcfa850af9627bed67ef9e89f8d3f_hd.jpg" alt=""></p><h4 id="topic"><a href="#topic" class="headerlink" title="topic"></a>topic</h4><p><img src="https://pic2.zhimg.com/80/v2-b1874d392a6119fb4e497425dcc58609_hd.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Django+uwsgi+Nginx&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-1-10周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/01/07/2020-1-10%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/01/07/2020-1-10%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-01-07T06:07:24.000Z</published>
    <updated>2020-01-10T10:47:51.445Z</updated>
    
    <content type="html"><![CDATA[<p>“All you need is attention”<br><a id="more"></a></p><h3 id="LSTM-Attention"><a href="#LSTM-Attention" class="headerlink" title="LSTM + Attention"></a>LSTM + Attention</h3><p>FEED-FORWARD NETWORKS WITH ATTENTION CAN SOLVE SOME LONG-TERM MEMORY PROBLEMS</p><h4 id="FEED-FORWARD-ATTENTION"><a href="#FEED-FORWARD-ATTENTION" class="headerlink" title="FEED-FORWARD ATTENTION"></a>FEED-FORWARD ATTENTION</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/FEED-FORWARD-ATTENTION.jpg" alt=""></p><script type="math/tex; mode=display">e_{t} = a(h_{t})</script><script type="math/tex; mode=display">\alpha_{t} = \frac{exp(e_{t})}{\sum_{k=1}^{T}exp(e_{k})}</script><script type="math/tex; mode=display">c = \sum_{t=1}^{T}\alpha_{t}h_{t}</script><h4 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/all-lstmattention.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/test-lstmattention.png" alt=""></p><h3 id="Seq2Seq-Attention"><a href="#Seq2Seq-Attention" class="headerlink" title="Seq2Seq + Attention"></a>Seq2Seq + Attention</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/all-seq2seqattention.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/test-seq2seqattention.png" alt=""></p><h3 id="DA-RNN"><a href="#DA-RNN" class="headerlink" title="DA-RNN"></a>DA-RNN</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-gru.jpg" alt=""></p><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h4><p>顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制</p><p>给出信息输入：用X = [x1, · · · , xN ]表示N 个输入信息；通过线性变换得到为查询向量序列，键向量序列和值向量序列，其中$W^{Q}$,$W^{K}$,$W^{V}$是我们模型训练过程学习到的合适的参数</p><script type="math/tex; mode=display">Q = W^{Q}X</script><script type="math/tex; mode=display">K = W^{K}X</script><script type="math/tex; mode=display">V = W^{V}X</script><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix}*[v^{T}_{1},v^{T}_{2},...,v^{T}_{n}])*\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix} = softmax(QK^{T})V</script><p><img src="https://pic2.zhimg.com/v2-07c4c02a9bdecb23d9664992f142eaa5_r.jpg" alt=""></p><p>Source中的构成元素想象成是由一系列的<Key,Value>数据对构成<br>Target中的某个元素Query<br>(在Seq2Se2中，Q是Decoder的隐藏态，K和V都是Encoder的隐藏态)</p><ul><li>1、根据Query和Key计算权重系数，常用的相似度函数有点积，拼接，感知机等</li><li>2、使用softmax函数对这些权重进行归一化</li><li>3、根据权重系数对Value进行加权求和得到attention</li></ul><h4 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h4><p>防止Q和K点乘积结果过大，会除以一个尺度标度 </p><script type="math/tex; mode=display">Attention(Q,K,V) = sofrmax(\frac{QK^{T}}{\sqrt{d_{k}}})V</script><h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><ul><li>$Q$，$K$，$V$首先进过一个线性变换，然后输入到放缩点积attention</li><li>每次$Q$，$K$，$V$进行线性变换的参数$W$是不一样的</li><li>通过$h$个不同的线性变换对$Q$，$K$，$V$进行投影，最后将不同的attention结果拼接起来</li></ul><script type="math/tex; mode=display">Multihead(Q,K,V) = Concat(head_{1},...,head_{h})W^{O}</script><script type="math/tex; mode=display">head_{i} = Attention(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})</script><h4 id="Experiment-1"><a href="#Experiment-1" class="headerlink" title="Experiment"></a>Experiment</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/all-transformer.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/test-transformer.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;“All you need is attention”&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>说说LSTM</title>
    <link href="http://arithmeticjia.github.io/2019/12/29/%E8%AF%B4%E8%AF%B4LSTM/"/>
    <id>http://arithmeticjia.github.io/2019/12/29/%E8%AF%B4%E8%AF%B4LSTM/</id>
    <published>2019-12-29T15:22:35.000Z</published>
    <updated>2019-12-29T15:34:02.931Z</updated>
    
    <content type="html"><![CDATA[<p>Long Short Term Memory<br><a id="more"></a></p><h4 id="从RNN开始"><a href="#从RNN开始" class="headerlink" title="从RNN开始"></a>从RNN开始</h4><p>RNN(Recurrent Neural Network)是一类用于处理序列数据的神经网络，擅长对序列数据进行建模处理。LSTM(Long Short-Term Memory) 在传统的 RNN 的基础上增加了状态$c$，称为记忆单元态 (cell state)，用以取代传统的隐含神经元节点。它负责把记忆信息从序列的初始位置，传递到序列的末端。</p><h4 id="LSTM的组成"><a href="#LSTM的组成" class="headerlink" title="LSTM的组成"></a>LSTM的组成</h4><p>在$t$时刻，当前神经元的输入有三个：当前时刻输入值$x_{t}$、前一时刻输出值$s_{t-1}$,和前一时刻的记忆单元状态$c_{t-1}$, 输出有两个，当前时刻LSTM的输出值$s_{t}$和当前时刻的记忆单元状态$c_{t}$。<br>LSTM通过三个门控开关传递记忆状态。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Long Short Term Memory&lt;br&gt;
    
    </summary>
    
    
      <category term="LSTM" scheme="http://arithmeticjia.github.io/categories/LSTM/"/>
    
    
      <category term="lstm" scheme="http://arithmeticjia.github.io/tags/lstm/"/>
    
      <category term="deeplearning" scheme="http://arithmeticjia.github.io/tags/deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode78Pascal-Triangle-2-Java</title>
    <link href="http://arithmeticjia.github.io/2019/12/29/Leetcode78Pascal-Triangle-2-Java/"/>
    <id>http://arithmeticjia.github.io/2019/12/29/Leetcode78Pascal-Triangle-2-Java/</id>
    <published>2019-12-29T03:54:18.000Z</published>
    <updated>2019-12-29T03:58:39.143Z</updated>
    
    <content type="html"><![CDATA[<p>Java 找规律法<br><a id="more"></a><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">getRow</span><span class="params">(<span class="keyword">int</span> rowIndex)</span> </span>&#123;</span><br><span class="line">        List&lt;Integer&gt; res = <span class="keyword">new</span> ArrayList&lt;Integer&gt;();</span><br><span class="line">        <span class="keyword">long</span> k = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(rowIndex &gt;= <span class="number">0</span>)</span><br><span class="line">            res.add(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= rowIndex + <span class="number">1</span>; i++) &#123;</span><br><span class="line">            k = k * (rowIndex + <span class="number">2</span> - i) / (i-<span class="number">1</span>);</span><br><span class="line">            res.add((<span class="keyword">int</span>)k);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>这里用到了杨辉三角的规律，第n行m个数等于</p><p>譬如第三行第二个数</p><script type="math/tex; mode=display">C_{3-1}^{2-1} = C_{2}^{1} = 2</script><p>譬如第四行第三个数</p><script type="math/tex; mode=display">C_{4-1}^{3-1} = C_{3}^{2} = 3</script><p>那这个对我们的算法有啥帮助呢？</p><p>举个栗子，看第四行</p><p>应该是1 3 3 1</p><p>在本题中是1 4 6 4 1</p><p>$C_{5-1}^{1-1} = C_{4}^{0} = 1$，$C_{5-1}^{2-1} = C_{4}^{1} = 4$，$C_{5-1}^{3-1} = C_{4}^{2} = 6$，$C_{5-1}^{4-1} = C_{4}^{3} = 4$，$C_{5-1}^{5-1} = C_{4}^{4} = 1$</p><p>找规律如下：</p><p>第一个数：<script type="math/tex">C_{5-1}^{1-1} = C_{4}^{0} = 1</script></p><p>第二个数：<script type="math/tex">C_{5-1}^{2-1} = C_{4}^{1} = C_{5-1}^{1-1} * \frac{(rowIndex-2+2)}{2-1}</script></p><p>第n行m个数：第m-1个数 × $ \frac{(rowIndex-m+2)}{m-1} $，第n行第一个数永远是1</p><p>晚安~~~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Java 找规律法&lt;br&gt;
    
    </summary>
    
    
      <category term="Leetcode" scheme="http://arithmeticjia.github.io/categories/Leetcode/"/>
    
    
      <category term="leetcode" scheme="http://arithmeticjia.github.io/tags/leetcode/"/>
    
      <category term="java" scheme="http://arithmeticjia.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode[78]Pascal&#39;s Triangle II</title>
    <link href="http://arithmeticjia.github.io/2019/12/28/Leetcode78Pascal-Triangle-2/"/>
    <id>http://arithmeticjia.github.io/2019/12/28/Leetcode78Pascal-Triangle-2/</id>
    <published>2019-12-28T14:05:49.000Z</published>
    <updated>2019-12-28T14:07:35.863Z</updated>
    
    <content type="html"><![CDATA[<p>python3 最优雅解法<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getRow</span><span class="params">(self, rowIndex)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type rowIndex: int</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        res = [<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, rowIndex + <span class="number">1</span>):</span><br><span class="line">            res.insert(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">            <span class="comment"># j循环每次算出r[0]...r[j-1]，再加上最后一个永远存在的1，正好是rowIndex+1个数</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">                res[j] = res[j] + res[j + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;python3 最优雅解法&lt;br&gt;
    
    </summary>
    
    
      <category term="Leetcode" scheme="http://arithmeticjia.github.io/categories/Leetcode/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="leetcode" scheme="http://arithmeticjia.github.io/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>2019-12-27周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2019/12/27/2019-12-27%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2019/12/27/2019-12-27%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2019-12-27T07:40:46.000Z</published>
    <updated>2020-01-10T09:53:38.616Z</updated>
    
    <content type="html"><![CDATA[<p>RNN -&gt; LSTM -&gt; GRU -&gt; Seq2Seq -&gt; Attention -&gt; Transformer<br><a id="more"></a></p><h3 id="Encoder-Decoder-Seq2Seq"><a href="#Encoder-Decoder-Seq2Seq" class="headerlink" title="Encoder-Decoder(Seq2Seq)"></a>Encoder-Decoder(Seq2Seq)</h3><p><img src="https://pic4.zhimg.com/80/v2-77e8a977fc3d43bec8b05633dc52ff9f_hd.jpg" alt=""></p><ul><li>Encoder-Decoder结构先将输入数据编码成一个上下文向量$c$</li><li>把Encoder的最后一个隐状态赋值给$c$,还可以对最后的隐状态做一个变换得到$c$，也可以对所有的隐状态做变换</li><li>拿到c之后，就用另一个RNN网络对其进行解码(Decoder),将c当做之前的初始状态$h_{0}$输入到Decoder中</li><li>还有一种做法是将$c$当做每一步的输入</li></ul><p><img src="https://pic4.zhimg.com/80/v2-e0fbb46d897400a384873fc100c442db_hd.jpg" alt=""></p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><ul><li>在Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征$c$再解码，因此，$c$中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈</li><li>Attention机制通过在每个时间输入不同的$c$来解决这个问题</li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/v2-8da16d429d33b0f2705e47af98e66579_hd_gaitubao_525x551_gaitubao_345x362.jpg" alt=""></p><ul><li>每一个$c$会自动去选取与当前所要输出的$y$最合适的上下文信息。具体来说，我们用$\alpha_{ij}$衡量Encoder中第$j$阶段的$h_{j}$和解码时第$i$阶段的相关性，最终Decoder中第$i$阶段的输入的上下文信息$c_{i}$就来自于所有$h_{j}$对$\alpha_{ij}$的加权和。</li><li>$\alpha_{ij}$和Decoder的第$i$阶段的隐藏状态、Encoder第$j$个阶段的隐藏状态有关</li><li>在Encoder的过程中保留每个RNN单元的隐藏状态(hidden state)得到($h_{1}$…$h_{N}$)，取$h_{j}$，表示Encoder层的隐层第$j$时刻的输出</li><li>在Decoder的过程中根据$x_{i}$和$h’_{i-1}$(这里和Encoder的$h_{i}$区分一下)得到$h’_{i}$，设为$s_{i}$</li><li>注：最开始的论文在Encoder-Decoder里面的当前Decoder的attention得分用的是$s_{i-1}$和$h_{j}$来算，但斯坦福教材上图上确实是画的$s_{i}$和$h_{j}$来算，而且后续论文大多是用的这种方式，即当前步的attention score用的当前步的隐藏状态$s_{i}$和前面的$h_{j}$去算的</li><li>通过Decoder的hidden states加上Encoder的hidden states来计算一个分数，用于计算权重<script type="math/tex; mode=display">e_{ij} = score(s_{i},h_{j})</script></li><li>注：这里有很多计算方式<script type="math/tex; mode=display">score(s_{i},h_{j}) = \left\{\begin{matrix}s^{T}_{i}h_{j}\\ s^{T}_{i}W_{a}h_{j}\\ v^{T}_{a}tanh(W_{a}[s^{T}_{i};h_{j}])\end{matrix}\right.</script></li><li>softmax权重归一化<script type="math/tex; mode=display">\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}</script></li><li>计算$c$<script type="math/tex; mode=display">c_{i} = \sum_{j=1}^{T_{x}}\alpha_{ij}h_{j}</script></li></ul><p><img src="https://pic4.zhimg.com/80/v2-8ddf993a95ee6e525fe2cd5ccd49bba7_hd.jpg" alt=""></p><p>(1)$h_{t} = RNN_{enc}(x_{t},h_{t-1})$, Encoder方面接受的是每一个单词word embedding，和上一个时间点的hidden state。输出的是这个时间点的hidden state。</p><p>(2)$s_{t} = RNN_{dnc}(y_{t},s_{t-1})$, Decoder方面接受的是目标句子里单词的word embedding，和上一个时间点的hidden state。</p><p>(3)$c_{i} = \sum_{j=1}^{T_{x}}\alpha _{ij}h_{j}$, context vector是一个对于encoder输出的hidden states的一个加权平均。</p><p>(4)$\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}$, 每一个encoder的hidden states对应的权重。</p><p>(5)$e_{ij} = score(s_{i},h_{j})$, 通过decoder的hidden states加上encoder的hidden states来计算一个分数，用于计算权重(4)</p><p>(6)$\hat{s}_{t}=tanh(W_{c}[c_{t};s_{t}])$, 将context vector 和 decoder的hidden states 串起来。</p><p>(7)$p(y_{t}|y_{&lt;t},x) = softmax(W_{s}\hat{s}_{t})$, 计算最后的输出概率。</p><h3 id="Transformer—-Attention-Is-All-You-Need"><a href="#Transformer—-Attention-Is-All-You-Need" class="headerlink" title="Transformer—-Attention Is All You Need"></a>Transformer—-Attention Is All You Need</h3><p><img src="https://pic1.zhimg.com/80/v2-4b53b731a961ee467928619d14a5fd44_hd.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/v2-4b53b731a961ee467928619d14a5fd44_r.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/4155986-208004e73fb93c97.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/4155986-e7fd5fcf3acc00a3.png" alt=""></p><ul><li>Transformer 的 Encoder 由 6 个编码器叠加组成，Decoder 也由 6 个解码器组成，在结构上都是相同的，但它们不共享权重。</li><li>Encoder的每一层有两个操作，分别是Self-Attention和Feed Forward；</li><li>Decoder的每一层有三个操作，分别是Self-Attention、Encoder-Decoder Attention以及Feed Forward操作。</li><li>这里的Self-Attention和Encoder-Decoder Attention都是用的是Multi-Head Attention机制</li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/v2-df2ca1b7a60d829245b7b7c37f80a3aa_r.jpg" alt=""></p><h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h4><ul><li>RNN的循环特性导致其不利于并行计算，模型训练时间较长</li><li>在传统的seq2seq中，我们通过RNN获取hidden state去做attention，那么当我们完全抛弃RNN的时候，怎么去做attention呢？</li><li>对每个input做embedding，代替hidden state，embedding通过三个不同的线性层生成$Q$，$K$，$V$。</li><li>Q: query;K: key; V: value</li><li>K = V = Q</li></ul><script type="math/tex; mode=display">Q = W_{Q}X</script><script type="math/tex; mode=display">K = W_{K}X</script><script type="math/tex; mode=display">V = W_{V}X</script><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix}*[v^{T}_{1},v^{T}_{2},...,v^{T}_{n}])*\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix} = softmax(QK^{T})V</script><p>举个栗子</p><p><img src="https://pic1.zhimg.com/80/v2-087b831f622f83e4529c1bbf646530f0_hd.jpg" alt=""></p><ul><li>假如我们要翻译一个词组Thinking Machines，其中Thinking的输入的embedding vector用$x_{1}$表示，Machines的embedding vector用$x_{2}$表示</li><li>$W^{Q}$，$W^{K}$，$W^{V}$是我们模型训练过程学习到的合适的参数</li><li>$x$与$W^{Q}$，$W^{K}$，$W^{V}$相乘获得$q$，$k$，$v$</li><li>如上图中所示我们分别得到了$q_{1}$与$k_{1}$，$k_{2}$的点乘积，然后我们进行尺度缩放与softmax归一化</li></ul><h4 id="Scaled-Dot-Product-Attention-缩放了的点乘注意力"><a href="#Scaled-Dot-Product-Attention-缩放了的点乘注意力" class="headerlink" title="Scaled Dot-Product Attention(缩放了的点乘注意力)"></a>Scaled Dot-Product Attention(缩放了的点乘注意力)</h4><script type="math/tex; mode=display">Attention(Q,K,V) = sofrmax(\frac{QK^{T}}{\sqrt{d_{k}}})V</script><ul><li>输入包含$d_{k}$维的query和key，以及$d_{v}$维的value。通过计算query和各个key的点积，除以$\sqrt{d_{k}}$归一化，然后经过softmax激活变成权重，最后再乘value。点积注意力机制的优点是速度快、占用空间小。</li></ul><h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><ul><li>$Q$，$K$，$V$首先进过一个线性变换，然后输入到放缩点积attention</li><li>每次$Q$，$K$，$V$进行线性变换的参数$W$是不一样的</li><li>通过$h$个不同的线性变换对$Q$，$K$，$V$进行投影，最后将不同的attention结果拼接起来</li></ul><script type="math/tex; mode=display">Multihead(Q,K,V) = Concat(head_{1},...,head_{h})W^{O}</script><script type="math/tex; mode=display">head_{i} = Attention(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})</script><h4 id="Position-wise-feed-forward-networks-位置全链接前馈网络"><a href="#Position-wise-feed-forward-networks-位置全链接前馈网络" class="headerlink" title="Position-wise feed-forward networks(位置全链接前馈网络)"></a>Position-wise feed-forward networks(位置全链接前馈网络)</h4><ul><li>由两个线性变换（Wx+b）和一个ReLU（relu的数学表达式就是f(x)=max(0,x)）<script type="math/tex; mode=display">FFN(x) = max(0,xW_{1} + b_{1})W_{2} + b_{2}</script></li></ul><h4 id="Positional-Encoding-位置编码"><a href="#Positional-Encoding-位置编码" class="headerlink" title="Positional Encoding(位置编码)"></a>Positional Encoding(位置编码)</h4><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1115<span class="string">-1120</span> after data smoothing</span><br><span class="line">T = 10</span><br><span class="line">features = 70</span><br><span class="line">train = all * 0.7</span><br><span class="line"><span class="keyword">test </span>= all * 0.3</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-1115-1120-all.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-1115-1120-test-m.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-1115-1120-test.jpg" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 3.955</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.289</span><br></pre></td></tr></table></figure><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nasdaq100_padding</span><br><span class="line">T = 10</span><br><span class="line">features = 81</span><br><span class="line">train = all * 0.7</span><br><span class="line"><span class="keyword">test </span>= all * 0.3</span><br></pre></td></tr></table></figure><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">Encoder:</span> LSTM</span><br><span class="line"><span class="symbol">Decoder:</span> LSTM</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-lstm.jpg" alt=""><br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.579</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.105</span><br></pre></td></tr></table></figure></p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">Encoder:</span> BiLSTM</span><br><span class="line"><span class="symbol">Decoder:</span> LSTM</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-bi-lstm.jpg" alt=""><br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.384</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.069</span><br></pre></td></tr></table></figure></p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">Encoder:</span> GRU</span><br><span class="line"><span class="symbol">Decoder:</span> LSTM</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-gru.jpg" alt=""><br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.252</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.046</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RNN -&amp;gt; LSTM -&amp;gt; GRU -&amp;gt; Seq2Seq -&amp;gt; Attention -&amp;gt; Transformer&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>Leetcode38Count-and-Say</title>
    <link href="http://arithmeticjia.github.io/2019/12/24/Leetcode38Count-and-Say/"/>
    <id>http://arithmeticjia.github.io/2019/12/24/Leetcode38Count-and-Say/</id>
    <published>2019-12-24T08:01:01.000Z</published>
    <updated>2019-12-24T08:02:18.323Z</updated>
    
    <content type="html"><![CDATA[<p>Nothing<br><a id="more"></a><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">countAndSay</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">1</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"1"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        String str = countAndSay(n-<span class="number">1</span>) + <span class="string">"*"</span>;<span class="comment">// 这样末尾的数才能被循环处理到</span></span><br><span class="line">        <span class="keyword">char</span>[] str_c = str.toCharArray();</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line">        StringBuilder temp = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (i &lt; str_c.length-<span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span>(str_c[i] == str_c[i+<span class="number">1</span>])&#123;</span><br><span class="line">                count++;  <span class="comment">//遇到相同的计数器加</span></span><br><span class="line">                i++;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                temp.append(Integer.toString(count)+ str_c[i]);</span><br><span class="line">                <span class="comment">// 遇到不同的，先append计数器的值，再append最后一个相同的值</span></span><br><span class="line">                <span class="comment">// temp.append("" + count + str_c[i]);</span></span><br><span class="line">                count = <span class="number">1</span>;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> temp.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Nothing&lt;br&gt;
    
    </summary>
    
    
      <category term="Leetcode" scheme="http://arithmeticjia.github.io/categories/Leetcode/"/>
    
    
      <category term="leetcode" scheme="http://arithmeticjia.github.io/tags/leetcode/"/>
    
      <category term="java" scheme="http://arithmeticjia.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Seq2seq模型及注意力机制模型</title>
    <link href="http://arithmeticjia.github.io/2019/12/22/Seq2seq%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A8%A1%E5%9E%8B/"/>
    <id>http://arithmeticjia.github.io/2019/12/22/Seq2seq%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A8%A1%E5%9E%8B/</id>
    <published>2019-12-22T07:21:43.000Z</published>
    <updated>2019-12-24T10:48:12.961Z</updated>
    
    <content type="html"><![CDATA[<p>对于处理输出序列为不定长情况的问题，例如机器翻译，例如英文到法语的句子翻译，输入和输出均为不定长。前人提出了seq2seq模型，basic idea是设计一个encoder与decoder，其中encoder将输入序列编码为一个包含输入序列所有信息的context vector $ c $，decoder通过对$ c $的解码获得输入序列的信息，从而得到输出序列。encoder及decoder都通常为RNN循环神经网络<br><a id="more"></a></p><h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><ul><li>input: 当前时刻输入值$x_{t}$,上一时刻LSTM的输出值$h_{t-1}$,上一时刻的单元状态$c_{t-1}$</li><li>output: 当前时刻LSTM的输出值$h_{t}$,当前时刻的单元状$c_{t}$</li><li>forget gate:</li></ul><script type="math/tex; mode=display">f_{t} = \sigma (W_{f}[h_{t-1};x_{t}]+b_{f})</script><p>$W_{f}$是遗忘门的权重矩阵，$[h_{t-1};x_{t}]$表示把两个向量连接成一个更长的向量，$b_{f}$是遗忘门的偏置项，$\sigma$是sigmoid函数<br>如果输入的维度是$d_{x}$，隐藏层的维度是$d_{h}$，单元状态的维度是$d_{c}$（通常$d_{c} = d_{h}$），则遗忘门的权重矩阵$W_{f}$的维度是$d_{c}×(d_{h}+d_{x})$</p><ul><li><p>input gate:</p><script type="math/tex; mode=display">i_{t} = \sigma (W_{i}[h_{t-1};x_{t}]+b_{i})</script></li><li><p>output gate:</p><script type="math/tex; mode=display">o_{t} = \sigma (W_{o}[h_{t-1};x_{t}]+b_{o})</script></li><li><p>final out:</p><script type="math/tex; mode=display">\tilde{c}_{t}= tanh(W_{c}[h_{t-1};x_{t}]+b_{c})</script><script type="math/tex; mode=display">c_{t} = f_{t} * c_{t-1} + i_{t} * \tilde{c}_{t}</script><script type="math/tex; mode=display">h_{t} = o_{t} * tanh(c_{t})</script></li><li><p>前向计算每个神经元的输出值，对于LSTM来说就是$f_{t}$,$i_{t}$,$c_{t}$,$o_{t}$,$h_{t}$ 5个向量的值</p></li><li>反向计算每个神经元的误差项$\delta$，包括两个方向，一是沿时间的反向传播，即从当前t时刻开始，计算每个时刻的误差项；另一个是将误差项向上一层传播</li><li>根据相应的误差项，计算每个权重的梯度</li><li>sigmoid</li></ul><script type="math/tex; mode=display">\delta (x) = \frac{1}{1+e^{-x}}</script><script type="math/tex; mode=display">\delta^{'} (x) = \frac{e^{-x}}{(1+e^{-x})^{2}}=\delta(x)(1-\delta(x))</script><ul><li>tanh</li></ul><script type="math/tex; mode=display">tanh(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}</script><script type="math/tex; mode=display">tanh^{'}(x) = 1 - tanh^{2}(x)</script><p>LSTM需要学习的参数共有8组，分别是：</p><ul><li>遗忘门的权重矩阵$W_{f}$和偏置项$b_{f}$</li><li>输入门的权重矩阵$W_{i}$和偏置项$b_{i}$</li><li>输出门的权重矩阵$W_{o}$和偏置项$b_{o}$</li><li>计算单元状态的权重矩阵$W_{c}$和偏置项$b_{c}$</li></ul><h4 id="seq2seq模型"><a href="#seq2seq模型" class="headerlink" title="seq2seq模型"></a>seq2seq模型</h4><h5 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h5><p>编码器的作用是把一个不定长的输入序列$ x_{1},x_{2},…,x_{T} $转化成一个定长的context vector $c$. 该context vector编码了输入序列$ x_{1},x_{2},…,x_{T} $的序列。回忆一下循环神经网络，假设该循环神经网络单元为$f$（可以为vanilla RNN, LSTM, GRU)，那么hidden state为</p><script type="math/tex; mode=display">h_{t} = f(x_{t},h_{t-1})</script><p>编码器的context vector是所有时刻hidden state的函数，即：</p><script type="math/tex; mode=display">c=q(h_{1},...,h_{T})</script><p>简单地，我们可以把最终时刻的hidden state[公式]作为context vecter。当然我们也可以取各个时刻hidden states的平均，以及其他方法。</p><h5 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h5><p>编码器最终输出一个context vector $c$，该context vector编码了输入序列$ x_{1},x_{2},…,x_{T} $的信息。</p><p>假设训练数据中的输出序列为$y_{1}y_{2},…,y_{T}^{‘}$,我们希望每个$t$时刻的输出即取决于之前的输出也取决于context vector，即估计$P(y_{t’}|y_{1},…,y_{t’-1},c)$，从而得到输出序列的联合概率分布：</p><script type="math/tex; mode=display">P(y_{1},...,y_{T'})=\prod_{t'-1}^{T'}P(y_{t'}|y_{1},...,y_{t'-1},c)</script><p>并定义该序列的损失函数loss function</p><script type="math/tex; mode=display">-\log P(y_{1},...,y_{T'})</script><p>通过最小化损失函数来训练seq2seq模型。</p><p>那么如何估计$ P(y_{t’}|y_{1},…,y_{t’-1},c) $？</p><p>我们使用另一个循环神经网络作为解码器。解码器使用函数$p$来表示$t’$时刻输出$y_{t’}$的概率</p><script type="math/tex; mode=display">P(y_{t'}|y_{1},...,y_{t'-1},c) = p(y_{t'-1},s_{t'},c)</script><p>为了区分编码器中的hidden state[公式]，其中[公式]为[公式]时刻解码器的hidden state。区别于编码器，解码器中的循环神经网络的输入除了前一个时刻的输出序列[公式]，和前一个时刻的hidden state[公式]以外，还包含了context vector[公式]。即：</p><script type="math/tex; mode=display">s_{t'} = g(y_{t'-1},s_{t'-1},c)</script><p>其中函数g为解码器的循环神经网络单元。</p><h4 id="DA-RNN"><a href="#DA-RNN" class="headerlink" title="DA-RNN"></a>DA-RNN</h4><h5 id="第一阶段，使用注意力机制自适应地提取每个时刻的相关feature"><a href="#第一阶段，使用注意力机制自适应地提取每个时刻的相关feature" class="headerlink" title="第一阶段，使用注意力机制自适应地提取每个时刻的相关feature"></a>第一阶段，使用注意力机制自适应地提取每个时刻的相关feature</h5><script type="math/tex; mode=display">e_{t}^{k}=v_{e}^{T}tanh(W_{e}[h_{t-1};c_{t-1}]+U_{e}x^{k})</script><ul><li>用softmax函数将其归一化<script type="math/tex; mode=display">\alpha _{t}^{k}=\frac{exp(e_{t}^{k})}{\sum_{i-1}^{n}exp(e_{t}^{i})}</script></li><li>得到更新后的x<script type="math/tex; mode=display">\tilde{x} = (\alpha _{t}^{1}x_{t}^{1}, \alpha _{t}^{2}x_{t}^{2},...,\alpha _{t}^{n}x_{t}^{n})</script></li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/lstm.jpg" alt=""></p><ul><li><p>选取LSTM作为编码器<script type="math/tex">f_{1}</script></p><script type="math/tex; mode=display">h_{t} = f_{1}(h_{t-1},  \tilde{x})</script></li><li><p>Encoder方面接受的是每一个输入，和上一个时间点的隐藏态。输出的是当前时间点的隐藏态</p></li></ul><h5 id="第二阶段，使用另一个注意力机制选取与之相关的encoder-hidden-states"><a href="#第二阶段，使用另一个注意力机制选取与之相关的encoder-hidden-states" class="headerlink" title="第二阶段，使用另一个注意力机制选取与之相关的encoder hidden states"></a>第二阶段，使用另一个注意力机制选取与之相关的encoder hidden states</h5><ul><li><p>Decoder方面接受的是目标输入，和上一个时间点的隐藏态</p></li><li><p>对所有时刻的$h_{t’}$取加权平均，即：</p></li></ul><script type="math/tex; mode=display">c_{t}^{'} = \sum_{t-1}^{T}\beta _{t^{'}}^{t}h_{t}</script><ul><li><script type="math/tex">\beta _{t^{'}}^{t}</script>的设计类似于Bahanau的工作，基于前一个时刻解码器的hidden state $ d_{t’-1} $和cell state$s_{t’-1}^{‘}$计算得到：</li></ul><script type="math/tex; mode=display">l_{t}^{t}=v_{d}^{T}tanh(W_{d}[d_{t-1};s_{t-1}^{'}]+U_{d}h_{t})</script><script type="math/tex; mode=display">\beta _{t}^{i}=\frac{exp(l_{t}^{i})}{\sum_{j=1}^{T}exp(l_{t}^{j})}</script><script type="math/tex; mode=display">c_{t}=\sum_{i=1}^{T}\beta _{t}^{i}h_{i}</script><ul><li>解码器的输入是上一个时刻的目标序列$y_{t’-1}$和hidden state$d_{t’-1}$以及context vector $c_{t’-1}$，即<script type="math/tex; mode=display">d_{t'}=f_{2}(y_{t'-1},c_{t'-1},d_{t'-1})</script></li><li>这里设计了$\tilde{y}_{t’-1}$来combie$y_{t’-1}$与$c_{t’-1}$的信息，即<script type="math/tex; mode=display">\tilde{y}_{t'-1} = \tilde{\omega }^{T}[y_{t'-1};c_{t'-1}]+\tilde{b}</script></li><li>然后<script type="math/tex; mode=display">d_{t}=f_{2}(d_{t-1},\tilde{y}_{t-1})</script></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于处理输出序列为不定长情况的问题，例如机器翻译，例如英文到法语的句子翻译，输入和输出均为不定长。前人提出了seq2seq模型，basic idea是设计一个encoder与decoder，其中encoder将输入序列编码为一个包含输入序列所有信息的context vector $ c $，decoder通过对$ c $的解码获得输入序列的信息，从而得到输出序列。encoder及decoder都通常为RNN循环神经网络&lt;br&gt;
    
    </summary>
    
    
    
      <category term="seq2seq" scheme="http://arithmeticjia.github.io/tags/seq2seq/"/>
    
      <category term="attention" scheme="http://arithmeticjia.github.io/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>da-rnn-bug-fix</title>
    <link href="http://arithmeticjia.github.io/2019/12/21/da-rnn-bug-fix/"/>
    <id>http://arithmeticjia.github.io/2019/12/21/da-rnn-bug-fix/</id>
    <published>2019-12-21T14:19:27.000Z</published>
    <updated>2019-12-23T08:00:10.477Z</updated>
    
    <content type="html"><![CDATA[<p>Bugs fix for<br><a href="https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py" target="_blank" rel="noopener" title="https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py">https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py</a><br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> concatenate</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">filename = <span class="string">'nasdaq100_padding.csv'</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(filename)</span><br><span class="line"><span class="comment"># print(dataset.values)</span></span><br><span class="line"></span><br><span class="line">features = dataset.values.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 82</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderAtt</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, T)</span>:</span></span><br><span class="line">        <span class="comment"># input size: number of underlying factors (81)</span></span><br><span class="line">        <span class="comment"># T: number of time steps (10)</span></span><br><span class="line">        <span class="comment"># hidden_size: dimension of the hidden state</span></span><br><span class="line">        super(EncoderAtt, self).__init__()</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.T = T</span><br><span class="line"></span><br><span class="line">        self.lstm_layer = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=<span class="number">1</span>)</span><br><span class="line">        self.attn_linear = nn.Linear(in_features=<span class="number">2</span> * hidden_size + T - <span class="number">1</span>, out_features=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_data)</span>:</span></span><br><span class="line">        <span class="comment"># input_data: batch_size * T - 1 * input_size</span></span><br><span class="line">        input_weighted = Variable(input_data.data.new(input_data.size(<span class="number">0</span>), self.T - <span class="number">1</span>, self.input_size).zero_())</span><br><span class="line">        input_encoded = Variable(input_data.data.new(input_data.size(<span class="number">0</span>), self.T - <span class="number">1</span>, self.hidden_size).zero_())</span><br><span class="line">        <span class="comment"># hidden, cell: initial states with dimention hidden_size</span></span><br><span class="line">        hidden = self.init_hidden(input_data) <span class="comment"># 1 * batch_size * hidden_size</span></span><br><span class="line">        cell = self.init_hidden(input_data)</span><br><span class="line">        <span class="comment"># hidden.requires_grad = False</span></span><br><span class="line">        <span class="comment"># cell.requires_grad = False</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(self.T - <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># Eqn. 8: concatenate the hidden states with each predictor</span></span><br><span class="line">            x = torch.cat((hidden.repeat(self.input_size, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>),</span><br><span class="line">                           cell.repeat(self.input_size, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>),</span><br><span class="line">                           input_data.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)), dim = <span class="number">2</span>) <span class="comment"># batch_size * input_size * (2*hidden_size + T - 1)</span></span><br><span class="line">            <span class="comment"># Eqn. 9: Get attention weights</span></span><br><span class="line">            x = self.attn_linear(x.view(<span class="number">-1</span>, self.hidden_size * <span class="number">2</span> + self.T - <span class="number">1</span>)) <span class="comment"># (batch_size * input_size) * 1</span></span><br><span class="line">            attn_weights = F.softmax(x.view(<span class="number">-1</span>, self.input_size)) <span class="comment"># batch_size * input_size, attn weights with values sum up to 1.</span></span><br><span class="line">            <span class="comment"># Eqn. 10: LSTM</span></span><br><span class="line">            weighted_input = torch.mul(attn_weights, input_data[:, t, :]) <span class="comment"># batch_size * input_size</span></span><br><span class="line">            <span class="comment"># Fix the warning about non-contiguous memory</span></span><br><span class="line">            <span class="comment"># see https://discuss.pytorch.org/t/dataparallel-issue-with-flatten-parameter/8282</span></span><br><span class="line">            self.lstm_layer.flatten_parameters()</span><br><span class="line">            _, lstm_states = self.lstm_layer(weighted_input.unsqueeze(<span class="number">0</span>), (hidden, cell))</span><br><span class="line">            hidden = lstm_states[<span class="number">0</span>]</span><br><span class="line">            cell = lstm_states[<span class="number">1</span>]</span><br><span class="line">            <span class="comment"># Save output</span></span><br><span class="line">            input_weighted[:, t, :] = weighted_input</span><br><span class="line">            input_encoded[:, t, :] = hidden</span><br><span class="line">        <span class="keyword">return</span> input_weighted, input_encoded</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># No matter whether CUDA is used, the returned variable will have the same type as x.</span></span><br><span class="line">        <span class="keyword">return</span> Variable(x.data.new(<span class="number">1</span>, x.size(<span class="number">0</span>), self.hidden_size).zero_()) <span class="comment"># dimension 0 is the batch dimension</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderAtt</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder_hidden_size, decoder_hidden_size, T)</span>:</span></span><br><span class="line">        super(DecoderAtt, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.T = T</span><br><span class="line">        self.encoder_hidden_size = encoder_hidden_size</span><br><span class="line">        self.decoder_hidden_size = decoder_hidden_size</span><br><span class="line"></span><br><span class="line">        self.attn_layer = nn.Sequential(nn.Linear(<span class="number">2</span> * decoder_hidden_size + encoder_hidden_size, encoder_hidden_size),</span><br><span class="line">                                        nn.Tanh(), nn.Linear(encoder_hidden_size, <span class="number">1</span>))</span><br><span class="line">        self.lstm_layer = nn.LSTM(input_size=<span class="number">1</span>, hidden_size=decoder_hidden_size)</span><br><span class="line">        self.fc = nn.Linear(encoder_hidden_size + <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc_final = nn.Linear(decoder_hidden_size + encoder_hidden_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.fc.weight.data.normal_()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_encoded, y_history)</span>:</span></span><br><span class="line">        <span class="comment"># input_encoded: batch_size * T - 1 * encoder_hidden_size</span></span><br><span class="line">        <span class="comment"># y_history: batch_size * (T-1)</span></span><br><span class="line">        <span class="comment"># Initialize hidden and cell, 1 * batch_size * decoder_hidden_size</span></span><br><span class="line">        hidden = self.init_hidden(input_encoded)</span><br><span class="line">        cell = self.init_hidden(input_encoded)</span><br><span class="line">        <span class="comment"># hidden.requires_grad = False</span></span><br><span class="line">        <span class="comment"># cell.requires_grad = False</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(self.T - <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># Eqn. 12-13: compute attention weights</span></span><br><span class="line">            <span class="comment">## batch_size * T * (2*decoder_hidden_size + encoder_hidden_size)</span></span><br><span class="line">            x = torch.cat((hidden.repeat(self.T - <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>),</span><br><span class="line">                           cell.repeat(self.T - <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), input_encoded), dim=<span class="number">2</span>)</span><br><span class="line">            x = F.softmax(self.attn_layer(x.view(<span class="number">-1</span>, <span class="number">2</span> * self.decoder_hidden_size + self.encoder_hidden_size</span><br><span class="line">                                                 )).view(<span class="number">-1</span>, self.T - <span class="number">1</span>))  <span class="comment"># batch_size * T - 1, row sum up to 1</span></span><br><span class="line">            <span class="comment"># Eqn. 14: compute context vector</span></span><br><span class="line">            context = torch.bmm(x.unsqueeze(<span class="number">1</span>), input_encoded)[:, <span class="number">0</span>, :]  <span class="comment"># batch_size * encoder_hidden_size</span></span><br><span class="line">            <span class="keyword">if</span> t &lt; self.T - <span class="number">1</span>:</span><br><span class="line">                <span class="comment"># Eqn. 15</span></span><br><span class="line">                y_tilde = self.fc(torch.cat((context, y_history[:, t].unsqueeze(<span class="number">1</span>)), dim=<span class="number">1</span>))  <span class="comment"># batch_size * 1</span></span><br><span class="line">                <span class="comment"># Eqn. 16: LSTM</span></span><br><span class="line">                self.lstm_layer.flatten_parameters()</span><br><span class="line">                _, lstm_output = self.lstm_layer(y_tilde.unsqueeze(<span class="number">0</span>), (hidden, cell))</span><br><span class="line">                hidden = lstm_output[<span class="number">0</span>]  <span class="comment"># 1 * batch_size * decoder_hidden_size</span></span><br><span class="line">                cell = lstm_output[<span class="number">1</span>]  <span class="comment"># 1 * batch_size * decoder_hidden_size</span></span><br><span class="line">        <span class="comment"># Eqn. 22: final output</span></span><br><span class="line">        y_pred = self.fc_final(torch.cat((hidden[<span class="number">0</span>], context), dim=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># self.logger.info("hidden %s context %s y_pred: %s", hidden[0][0][:10], context[0][:10], y_pred[:10])</span></span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> Variable(x.data.new(<span class="number">1</span>, x.size(<span class="number">0</span>), self.decoder_hidden_size).zero_())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_data</span><span class="params">(dat, col_names)</span>:</span></span><br><span class="line">    scale = StandardScaler().fit(dat)</span><br><span class="line">    proc_dat = scale.transform(dat)</span><br><span class="line"></span><br><span class="line">    mask = np.ones(proc_dat.shape[<span class="number">1</span>], dtype=bool)</span><br><span class="line">    dat_cols = list(dat.columns)</span><br><span class="line">    <span class="keyword">for</span> col_name <span class="keyword">in</span> col_names:</span><br><span class="line">        mask[dat_cols.index(col_name)] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    feats = proc_dat[:, mask]</span><br><span class="line">    targs = proc_dat[:, ~mask]</span><br><span class="line">    <span class="keyword">return</span> feats, targs, scale</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">da_rnn</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, file_data, encoder_hidden_size=<span class="number">64</span>, decoder_hidden_size=<span class="number">64</span>, T=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 learning_rate=<span class="number">0.01</span>, batch_size=<span class="number">128</span>, parallel=True, debug=False)</span>:</span></span><br><span class="line">        self.T = T</span><br><span class="line">        dat = pd.read_csv(file_data, nrows=<span class="number">100</span> <span class="keyword">if</span> debug <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">        <span class="comment"># read first 100 rows</span></span><br><span class="line">        <span class="comment"># self.logger.info("Shape of data: %s.\nMissing in data: %s.", dat.shape, dat.isnull().sum().sum())</span></span><br><span class="line">        <span class="comment"># scale = StandardScaler().fit(dat.values)</span></span><br><span class="line">        <span class="comment"># dat = pd.DataFrame(scale.transform(dat.values))</span></span><br><span class="line">        <span class="comment"># self.X = dat.loc[:, [x for x in dat.columns.tolist() if x != 'NDX']].as_matrix()</span></span><br><span class="line">        self.X, self.y, self.scaler = preprocess_data(dat, (<span class="string">"NDX"</span>,))</span><br><span class="line">        <span class="comment"># select matrix without NDX</span></span><br><span class="line">        <span class="comment"># (ndarray:(40560,81))</span></span><br><span class="line">        self.y = (self.y).reshape((self.y).shape[<span class="number">0</span>],)</span><br><span class="line">        <span class="comment"># self.y = np.array(dat.NDX)</span></span><br><span class="line">        <span class="comment"># (ndarray:(40560,))</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        <span class="comment"># 128</span></span><br><span class="line">        self.encoder = EncoderAtt(input_size=self.X.shape[<span class="number">1</span>], hidden_size=encoder_hidden_size, T=T).to(device)</span><br><span class="line">        self.decoder = DecoderAtt(encoder_hidden_size=encoder_hidden_size, decoder_hidden_size=decoder_hidden_size, T=T).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> parallel:</span><br><span class="line">            self.encoder = nn.DataParallel(self.encoder)</span><br><span class="line">            self.decoder = nn.DataParallel(self.decoder)</span><br><span class="line">        <span class="comment">#  multiple GPU training</span></span><br><span class="line"></span><br><span class="line">        self.encoder_optimizer = optim.Adam(params=filter(<span class="keyword">lambda</span> p: p.requires_grad, self.encoder.parameters()),</span><br><span class="line">                                           lr=learning_rate)</span><br><span class="line">        self.decoder_optimizer = optim.Adam(params=filter(<span class="keyword">lambda</span> p: p.requires_grad, self.decoder.parameters()),</span><br><span class="line">                                           lr=learning_rate)</span><br><span class="line">        <span class="comment"># self.learning_rate = learning_rate</span></span><br><span class="line"></span><br><span class="line">        self.train_size = int(self.X.shape[<span class="number">0</span>] * <span class="number">0.7</span>)</span><br><span class="line">        <span class="comment"># &#123;int&#125; 28392</span></span><br><span class="line">        <span class="comment"># self.y = self.y - np.mean(self.y[:self.train_size])</span></span><br><span class="line">        <span class="comment"># self.y = (self.y - np.mean(self.y[:self.train_size])) / np.std(self.y[:self.train_size])</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Question: why Adam requires data to be normalized?</span></span><br><span class="line">        <span class="comment"># self.logger.info("Training size: %d.", self.train_size)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, n_epochs=<span class="number">10</span>)</span>:</span></span><br><span class="line">        iter_per_epoch = int(np.ceil(self.train_size * <span class="number">1.</span> / self.batch_size))</span><br><span class="line">        print(<span class="string">"Iterations per epoch: %3.3f ~ %d."</span>, self.train_size * <span class="number">1.</span> / self.batch_size, iter_per_epoch)</span><br><span class="line">        self.iter_losses = np.zeros(n_epochs * iter_per_epoch)</span><br><span class="line">        self.epoch_losses = np.zeros(n_epochs)</span><br><span class="line"></span><br><span class="line">        self.loss_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">        n_iter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        learning_rate = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">            perm_idx = np.random.permutation(self.train_size - self.T)</span><br><span class="line">            j = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> j &lt; self.train_size:</span><br><span class="line">                batch_idx = perm_idx[j:(j + self.batch_size)]</span><br><span class="line">                X = np.zeros((len(batch_idx), self.T - <span class="number">1</span>, self.X.shape[<span class="number">1</span>]))</span><br><span class="line">                y_history = np.zeros((len(batch_idx), self.T - <span class="number">1</span>))</span><br><span class="line">                y_target = self.y[batch_idx + self.T]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(len(batch_idx)):</span><br><span class="line">                    X[k, :, :] = self.X[batch_idx[k] : (batch_idx[k] + self.T - <span class="number">1</span>), :]</span><br><span class="line">                    y_history[k, :] = self.y[batch_idx[k]: (batch_idx[k] + self.T - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">                loss = self.train_iteration(X, y_history, y_target)</span><br><span class="line">                self.iter_losses[int(i * iter_per_epoch + j / self.batch_size)] = loss</span><br><span class="line">                <span class="comment">#if (j / self.batch_size) % 50 == 0:</span></span><br><span class="line">                <span class="comment">#    self.logger.info("Epoch %d, Batch %d: loss = %3.3f.", i, j / self.batch_size, loss)</span></span><br><span class="line">                j += self.batch_size</span><br><span class="line">                n_iter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> n_iter % <span class="number">10000</span> == <span class="number">0</span> <span class="keyword">and</span> n_iter &gt; <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">for</span> param_group <span class="keyword">in</span> self.encoder_optimizer.param_groups:</span><br><span class="line">                        param_group[<span class="string">'lr'</span>] = param_group[<span class="string">'lr'</span>] * <span class="number">0.9</span></span><br><span class="line">                    <span class="keyword">for</span> param_group <span class="keyword">in</span> self.decoder_optimizer.param_groups:</span><br><span class="line">                        param_group[<span class="string">'lr'</span>] = param_group[<span class="string">'lr'</span>] * <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line">            self.epoch_losses[i] = np.mean(self.iter_losses[range(i * iter_per_epoch, (i + <span class="number">1</span>) * iter_per_epoch)])</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Epoch %d, loss: %3.3f."</span> % (i, self.epoch_losses[i]))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                y_train_pred = self.predict(on_train=<span class="literal">True</span>)  <span class="comment"># 28383</span></span><br><span class="line">                y_test_pred = self.predict(on_train=<span class="literal">False</span>)  <span class="comment"># 12168</span></span><br><span class="line">                y_pred = np.concatenate((y_train_pred, y_test_pred))    <span class="comment"># 40551</span></span><br><span class="line">                <span class="comment"># X (40560,)</span></span><br><span class="line">                <span class="comment"># y (40560,)</span></span><br><span class="line">                print(y_train_pred.shape, y_test_pred.shape, y_pred.shape)</span><br><span class="line">                print((self.y).shape,(self.X).shape)</span><br><span class="line">                <span class="comment"># (40560,) (40560, 81)</span></span><br><span class="line">                true = concatenate(((self.y).reshape(self.y.shape[<span class="number">0</span>], <span class="number">1</span>), self.X), axis=<span class="number">1</span>)</span><br><span class="line">                true = self.scaler.inverse_transform(true)</span><br><span class="line">                self.y = true[:, <span class="number">0</span>]</span><br><span class="line">                <span class="comment"># true [1,40560] len = 40560</span></span><br><span class="line">                print(self.T, len(y_train_pred) + self.T)</span><br><span class="line">                <span class="comment"># 10 28393</span></span><br><span class="line">                print(self.T + len(y_train_pred), len(self.y) + <span class="number">1</span>)</span><br><span class="line">                <span class="comment"># 28393 40561</span></span><br><span class="line">                <span class="comment"># y_train_pred = concatenate((y_train_pred.reshape(y_train_pred.shape[0], 1), self.X[self.T-1: len(y_train_pred) + self.T-1]), axis=1)</span></span><br><span class="line">                y_train_pred = concatenate((y_train_pred.reshape(y_train_pred.shape[<span class="number">0</span>], <span class="number">1</span>),</span><br><span class="line">                                            self.X[: len(y_train_pred)]), axis=<span class="number">1</span>)</span><br><span class="line">                y_train_pred = self.scaler.inverse_transform(y_train_pred)</span><br><span class="line">                y_train_pred = y_train_pred[:, <span class="number">0</span>]</span><br><span class="line">                <span class="comment"># y_train_pred [10,28392] len = 28383</span></span><br><span class="line">                <span class="comment"># y_test_pred = concatenate((y_test_pred.reshape(y_test_pred.shape[0], 1), self.X[self.T + len(y_train_pred)-1:]), axis=1)</span></span><br><span class="line">                y_test_pred = concatenate(</span><br><span class="line">                    (y_test_pred.reshape(y_test_pred.shape[<span class="number">0</span>], <span class="number">1</span>), self.X[len(y_train_pred):len(y_train_pred)+len(y_test_pred)]), axis=<span class="number">1</span>)</span><br><span class="line">                y_test_pred = self.scaler.inverse_transform(y_test_pred)</span><br><span class="line">                y_test_pred = y_test_pred[:, <span class="number">0</span>]</span><br><span class="line">                <span class="comment"># y_test_pred [28393,40560] len = 12168</span></span><br><span class="line">                plt.figure()</span><br><span class="line">                plt.plot(range(<span class="number">1</span>, <span class="number">1</span> + len(self.y)), self.y, label=<span class="string">"True"</span>)</span><br><span class="line">                plt.plot(range(self.T, len(y_train_pred) + self.T), y_train_pred, label = <span class="string">'Predicted - Train'</span>)</span><br><span class="line">                plt.plot(range(self.T + len(y_train_pred), len(self.y) + <span class="number">1</span>), y_test_pred, label = <span class="string">'Predicted - Test'</span>)</span><br><span class="line">                plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">                plt.savefig(<span class="string">'./resultpic/epoch_%d.jpg'</span> % i)</span><br><span class="line">                plt.show()</span><br><span class="line"></span><br><span class="line">        y_train_pred = self.predict(on_train=<span class="literal">True</span>)</span><br><span class="line">        y_test_pred = self.predict(on_train=<span class="literal">False</span>)</span><br><span class="line">        y_pred = np.concatenate((y_train_pred, y_test_pred))</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.plot(range(<span class="number">1</span>, <span class="number">1</span> + len(self.y)), self.y, label=<span class="string">"True"</span>)</span><br><span class="line">        plt.plot(range(self.T, len(y_train_pred) + self.T), y_train_pred, label=<span class="string">'Predicted - Train'</span>)</span><br><span class="line">        plt.plot(range(self.T + len(y_train_pred), len(self.y) + <span class="number">1</span>), y_test_pred, label=<span class="string">'Predicted - Test'</span>)</span><br><span class="line">        plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">        plt.savefig(<span class="string">'./resultpic/final.jpg'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_iteration</span><span class="params">(self, X, y_history, y_target)</span>:</span></span><br><span class="line">        self.encoder_optimizer.zero_grad()</span><br><span class="line">        self.decoder_optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        input_weighted, input_encoded = self.encoder(Variable(torch.from_numpy(X).type(torch.FloatTensor).to(device)))</span><br><span class="line">        y_pred = self.decoder(input_encoded, Variable(torch.from_numpy(y_history).type(torch.FloatTensor).to(device)))</span><br><span class="line">        y_pred = y_pred.view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># print('y_pred', y_pred.shape)</span></span><br><span class="line">        y_true = Variable(torch.from_numpy(y_target).type(torch.FloatTensor).to(device))</span><br><span class="line">        <span class="comment"># print('y_true', y_true.shape)</span></span><br><span class="line">        loss = self.loss_func(y_pred, y_true)</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        self.encoder_optimizer.step()</span><br><span class="line">        self.decoder_optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, on_train = False)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> on_train:</span><br><span class="line">            y_pred = np.zeros(self.train_size - self.T + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_pred = np.zeros(self.X.shape[<span class="number">0</span>] - self.train_size)</span><br><span class="line"></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; len(y_pred):</span><br><span class="line">            batch_idx = np.array(range(len(y_pred)))[i : (i + self.batch_size)]</span><br><span class="line">            X = np.zeros((len(batch_idx), self.T - <span class="number">1</span>, self.X.shape[<span class="number">1</span>]))</span><br><span class="line">            y_history = np.zeros((len(batch_idx), self.T - <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(batch_idx)):</span><br><span class="line">                <span class="keyword">if</span> on_train:</span><br><span class="line">                    X[j, :, :] = self.X[range(batch_idx[j], batch_idx[j] + self.T - <span class="number">1</span>), :]</span><br><span class="line">                    y_history[j, :] = self.y[range(batch_idx[j],  batch_idx[j]+ self.T - <span class="number">1</span>)]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    X[j, :, :] = self.X[range(batch_idx[j] + self.train_size - self.T, batch_idx[j] + self.train_size - <span class="number">1</span>), :]</span><br><span class="line">                    y_history[j, :] = self.y[range(batch_idx[j] + self.train_size - self.T,  batch_idx[j]+ self.train_size - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">            y_history = Variable(torch.from_numpy(y_history).type(torch.FloatTensor).to(device))</span><br><span class="line">            _, input_encoded = self.encoder(Variable(torch.from_numpy(X).type(torch.FloatTensor).to(device)))</span><br><span class="line">            y_pred[i:(i + self.batch_size)] = self.decoder(input_encoded, y_history).cpu().data.numpy()[:, <span class="number">0</span>]</span><br><span class="line">            i += self.batch_size</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">io_dir = <span class="string">'nasdaq100_padding.csv'</span></span><br><span class="line"></span><br><span class="line">model = da_rnn(file_data=<span class="string">'&#123;&#125;'</span>.format(io_dir), parallel=<span class="literal">False</span>, learning_rate=<span class="number">.001</span>)</span><br><span class="line"></span><br><span class="line">model.train(n_epochs=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">y_pred = model.predict()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.semilogy(range(len(model.iter_losses)), model.iter_losses)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.semilogy(range(len(model.epoch_losses)), model.epoch_losses)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(y_pred, label = <span class="string">'Predicted'</span>)</span><br><span class="line">plt.plot(model.y[model.train_size:], label = <span class="string">"True"</span>)</span><br><span class="line">plt.legend(loc = <span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Bugs fix for&lt;br&gt;&lt;a href=&quot;https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py&quot;&gt;https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Learn-Pytorch-Variable</title>
    <link href="http://arithmeticjia.github.io/2019/12/10/Learn-Pytorch-Variable/"/>
    <id>http://arithmeticjia.github.io/2019/12/10/Learn-Pytorch-Variable/</id>
    <published>2019-12-10T11:01:51.000Z</published>
    <updated>2019-12-10T11:14:08.493Z</updated>
    
    <content type="html"><![CDATA[<p>Tensor是Pytorch的一个完美组件(可以生成高维数组)，但是要构建神经网络还是远远不够的，我们需要能够计算图的Tensor，那就是Variable。Variable是对Tensor的一个封装，操作和Tensor是一样的，但是每个Variable都有三个属性，Varibale的Tensor本身的.data，对应Tensor的梯度.grad，以及这个Variable是通过什么方式得到的.grad_fn<br><a id="more"></a></p><h4 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h4><p>autograd.Variable 是包的核心类. 它包装了张量, 并且支持几乎所有的操作. 一旦你完成了你的计算, 你就可以调用 .backward() 方法, 然后所有的梯度计算会自动进行.你还可以通过 .data 属性来访问原始的张量, 而关于该 variable（变量）的梯度会被累计到 .grad上去.还有一个针对自动求导实现来说非常重要的类 - Function.Variable 和 Function 是相互联系的, 并且它们构建了一个非循环的图, 编码了一个完整的计算历史信息. 每一个 variable（变量）都有一个 .grad_fn 属性, 它引用了一个已经创建了 Variable 的 Function （除了用户创建的 Variable <code>之外 - 它们的</code>grad_fn is None ）.如果你想计算导数, 你可以在 Variable 上调用 .backward() 方法. 如果 Variable 是标量的形式（例如, 它包含一个元素数据）, 你不必指定任何参数给 backward(), 但是, 如果它有更多的元素. 你需要去指定一个 grad_output 参数, 该参数是一个匹配 shape（形状）的张量.</p><h4 id="创建一个2×2的变量"><a href="#创建一个2×2的变量" class="headerlink" title="创建一个2×2的变量"></a>创建一个2×2的变量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">View more, visit my tutorial page: https://arithmeticjia.github.io</span></span><br><span class="line"><span class="string">My Blog: https://www.guanacossj.com</span></span><br><span class="line"><span class="string">Dependencies:</span></span><br><span class="line"><span class="string">torch: 1.3.0</span></span><br><span class="line"><span class="string">matplotlib</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># Variable in torch is to build a computational graph,</span></span><br><span class="line"><span class="comment"># but this graph is dynamic compared with a static graph in Tensorflow or Theano.</span></span><br><span class="line"><span class="comment"># So torch does not have placeholder, torch can just pass variable to the computational graph.</span></span><br><span class="line"></span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])            <span class="comment"># build a tensor</span></span><br><span class="line">variable = Variable(tensor, requires_grad=<span class="literal">True</span>)      <span class="comment"># build a variable, usually for compute gradients</span></span><br><span class="line"></span><br><span class="line">print(tensor)       <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br><span class="line">print(variable)     <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]])</span><br><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]], requires_grad=True)</span><br></pre></td></tr></table></figure><h4 id="计算变量的点乘积、梯度"><a href="#计算变量的点乘积、梯度" class="headerlink" title="计算变量的点乘积、梯度"></a>计算变量的点乘积、梯度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">View more, visit my tutorial page: https://arithmeticjia.github.io</span></span><br><span class="line"><span class="string">My Blog: https://www.guanacossj.com</span></span><br><span class="line"><span class="string">Dependencies:</span></span><br><span class="line"><span class="string">torch: 1.3.0</span></span><br><span class="line"><span class="string">matplotlib</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># Variable in torch is to build a computational graph,</span></span><br><span class="line"><span class="comment"># but this graph is dynamic compared with a static graph in Tensorflow or Theano.</span></span><br><span class="line"><span class="comment"># So torch does not have placeholder, torch can just pass variable to the computational graph.</span></span><br><span class="line"></span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])            <span class="comment"># build a tensor</span></span><br><span class="line">variable = Variable(tensor, requires_grad=<span class="literal">True</span>)      <span class="comment"># build a variable, usually for compute gradients</span></span><br><span class="line"></span><br><span class="line">print(tensor)       <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br><span class="line">print(variable)     <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># till now the tensor and variable seem the same.</span></span><br><span class="line"><span class="comment"># However, the variable is a part of the graph, it's a part of the auto-gradient.</span></span><br><span class="line"></span><br><span class="line">t_out = torch.mean(tensor*tensor)       <span class="comment"># x^2</span></span><br><span class="line">v_out = torch.mean(variable*variable)   <span class="comment"># x^2</span></span><br><span class="line">print(t_out)</span><br><span class="line">print(v_out)                            <span class="comment"># 7.5</span></span><br><span class="line"></span><br><span class="line">print(variable*variable)</span><br><span class="line"><span class="comment"># 点乘操作</span></span><br><span class="line">print(torch.mm(variable,variable))</span><br><span class="line"><span class="comment"># 矩阵相乘</span></span><br><span class="line">v_out.backward()    <span class="comment"># backpropagation from v_out</span></span><br><span class="line"><span class="comment"># v_out = 1/4 * sum(variable*variable)</span></span><br><span class="line"><span class="comment"># the gradients w.r.t the variable, d(v_out)/d(variable) = 1/4*2*variable = variable/2</span></span><br><span class="line">print(variable.grad)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"> 0.5000  1.0000</span></span><br><span class="line"><span class="string"> 1.5000  2.0000</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]])</span><br><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]], requires_grad=True)</span><br><span class="line">tensor(7.5000)</span><br><span class="line">tensor(7.5000, grad_fn=&lt;MeanBackward0&gt;)</span><br><span class="line">tensor([[ 1.,  4.],</span><br><span class="line">        [ 9., 16.]], grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line">tensor([[ 7., 10.],</span><br><span class="line">        [15., 22.]], grad_fn=&lt;MmBackward&gt;)</span><br><span class="line">tensor([[0.5000, 1.0000],</span><br><span class="line">        [1.5000, 2.0000]])</span><br></pre></td></tr></table></figure><p>注意这里的变量的点乘和相乘的区别</p><h4 id="查看变量的数据"><a href="#查看变量的数据" class="headerlink" title="查看变量的数据"></a>查看变量的数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">print(variable)     <span class="comment"># this is data in variable format</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">print(variable.data)    <span class="comment"># this is data in tensor format</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]], requires_grad=True)</span><br><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]])</span><br></pre></td></tr></table></figure><h4 id="Variable转numpy"><a href="#Variable转numpy" class="headerlink" title="Variable转numpy"></a>Variable转numpy</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(variable.data.numpy())    <span class="comment"># numpy format</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[ 1.  2.]</span></span><br><span class="line"><span class="string"> [ 3.  4.]]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[1. 2.]</span><br><span class="line"> [3. 4.]]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Tensor是Pytorch的一个完美组件(可以生成高维数组)，但是要构建神经网络还是远远不够的，我们需要能够计算图的Tensor，那就是Variable。Variable是对Tensor的一个封装，操作和Tensor是一样的，但是每个Variable都有三个属性，Varibale的Tensor本身的.data，对应Tensor的梯度.grad，以及这个Variable是通过什么方式得到的.grad_fn&lt;br&gt;
    
    </summary>
    
    
      <category term="Pytorch" scheme="http://arithmeticjia.github.io/categories/Pytorch/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://arithmeticjia.github.io/tags/pytorch/"/>
    
  </entry>
  
</feed>
