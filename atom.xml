<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>è¯·å«æˆ‘ç®—æœ¯å˜‰çš„åšå®¢</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://arithmeticjia.github.io/"/>
  <updated>2020-04-20T07:46:10.345Z</updated>
  <id>http://arithmeticjia.github.io/</id>
  
  <author>
    <name>è¯·å«æˆ‘ç®—æœ¯å˜‰</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>VAE-å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨</title>
    <link href="http://arithmeticjia.github.io/2020/04/20/VAE-%E5%8F%98%E5%88%86%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8/"/>
    <id>http://arithmeticjia.github.io/2020/04/20/VAE-%E5%8F%98%E5%88%86%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8/</id>
    <published>2020-04-20T07:20:14.000Z</published>
    <updated>2020-04-20T07:46:10.345Z</updated>
    
    <content type="html"><![CDATA[<p>Variational Auto-Encoder<br><a id="more"></a></p><h3 id="é¢„å¤‡çŸ¥è¯†"><a href="#é¢„å¤‡çŸ¥è¯†" class="headerlink" title="é¢„å¤‡çŸ¥è¯†"></a>é¢„å¤‡çŸ¥è¯†</h3><h4 id="ä¿¡æ¯é‡"><a href="#ä¿¡æ¯é‡" class="headerlink" title="ä¿¡æ¯é‡"></a>ä¿¡æ¯é‡</h4><script type="math/tex; mode=display">I(x) = -logp(x)</script><p>p(x) ä¸ºäº‹ä»¶xå‘ç”Ÿçš„æ¦‚ç‡ï¼Œå½“logåº•æ•°ä¸ºeæ—¶ï¼Œä¿¡æ¯é‡çš„å•ä½ä¸ºnatï¼ˆå¥ˆç‰¹ï¼‰ï¼Œå½“logåº•æ•°ä¸º2æ—¶ï¼Œä¿¡æ¯é‡çš„å•ä½ä¸ºbitï¼ˆæ¯”ç‰¹ï¼‰ã€‚</p><h4 id="ä¿¡æ¯ç†µ"><a href="#ä¿¡æ¯ç†µ" class="headerlink" title="ä¿¡æ¯ç†µ"></a>ä¿¡æ¯ç†µ</h4><p>è¡¨ç¤ºéšæœºå˜é‡xåœ¨ç¦»æ•£å’Œè¿ç»­æƒ…å†µä¸‹çš„ä¿¡æ¯ç†µH:</p><script type="math/tex; mode=display">H = \sum -\log p(x) * p(x)</script><script type="math/tex; mode=display">H = \int -\log p(x) * p(x)d(x)</script><h3 id="K-Læ•£åº¦ï¼ˆKullback-Leibler-divergenceï¼‰"><a href="#K-Læ•£åº¦ï¼ˆKullback-Leibler-divergenceï¼‰" class="headerlink" title="K-Læ•£åº¦ï¼ˆKullback-Leibler divergenceï¼‰"></a>K-Læ•£åº¦ï¼ˆKullback-Leibler divergenceï¼‰</h3><p>K-Læ•£åº¦åˆè¢«ç§°ä¸ºç›¸å¯¹ç†µï¼ˆrelative entropyï¼‰ï¼Œæ˜¯å¯¹ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒé—´å·®å¼‚çš„éå¯¹ç§°æ€§åº¦é‡ã€‚</p><h3 id="è´å¶æ–¯å…¬å¼ï¼ˆBayes-Ruleï¼‰"><a href="#è´å¶æ–¯å…¬å¼ï¼ˆBayes-Ruleï¼‰" class="headerlink" title="è´å¶æ–¯å…¬å¼ï¼ˆBayes Ruleï¼‰"></a>è´å¶æ–¯å…¬å¼ï¼ˆBayes Ruleï¼‰</h3><script type="math/tex; mode=display">p(z|x) = \frac{p(z,x)}{p(x)} = \frac{p(x|z)p(z)}{p(x)}</script><h3 id="AE-è‡ªç¼–ç å™¨"><a href="#AE-è‡ªç¼–ç å™¨" class="headerlink" title="AE-è‡ªç¼–ç å™¨"></a>AE-è‡ªç¼–ç å™¨</h3><p>encoder-decoderç»“æ„</p><p><img src="https://pic3.zhimg.com/80/v2-8ae0e598375aeeed45488edd064e1cfa_1440w.jpg" alt=""></p><h3 id="VAE-å·®åˆ†è‡ªç¼–ç å™¨"><a href="#VAE-å·®åˆ†è‡ªç¼–ç å™¨" class="headerlink" title="VAE-å·®åˆ†è‡ªç¼–ç å™¨"></a>VAE-å·®åˆ†è‡ªç¼–ç å™¨</h3><p>å‡è®¾ç»™å®šæ ·æœ¬xï¼Œå¸Œæœ›åœ¨ç»™å®šxçš„æ¡ä»¶ä¸‹æ¨å‡ºzçš„åˆ†å¸ƒï¼Œå³p(z|x)</p><p>æ ¹æ®è´å¶æ–¯å…¬å¼:</p><script type="math/tex; mode=display">p(z|x) = \frac{p(z,x)}{p(x)} = \frac{p(x|z)p(z)}{p(x)}</script><p>å› ä¸ºæ— æ³•å¾—çŸ¥p(x),ä½¿ç”¨q(z|x)å»è¿‘ä¼¼p(z|x),æ»¡è¶³minKL(q(z|x)||p(z|x))</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Variational Auto-Encoder&lt;br&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="http://arithmeticjia.github.io/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-04-17å‘¨æŠ¥-å•æ²™å˜‰</title>
    <link href="http://arithmeticjia.github.io/2020/04/17/2020-04-17%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/04/17/2020-04-17%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-04-17T10:50:20.000Z</published>
    <updated>2020-04-17T10:51:48.719Z</updated>
    
    <content type="html"><![CDATA[<p>ä¸çŸ¥é“è¯´å•¥<br><a id="more"></a></p><p>1ã€å †å LSTM</p><p><img src="https://upload-images.jianshu.io/upload_images/7311123-f4f81fb3930f84a0.png?imageMogr2/auto-orient/strip|imageView2/2/w/640/format/webp.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;ä¸çŸ¥é“è¯´å•¥&lt;br&gt;
    
    </summary>
    
    
      <category term="å‘¨æŠ¥" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-04-03å‘¨æŠ¥-å•æ²™å˜‰</title>
    <link href="http://arithmeticjia.github.io/2020/04/02/2020-04-03%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/04/02/2020-04-03%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-04-02T09:08:46.000Z</published>
    <updated>2020-04-03T11:50:56.026Z</updated>
    
    <content type="html"><![CDATA[<p>A Memory-Network Based Solution for Multivariate Time-Series Forecasting<br><a id="more"></a></p><h3 id="By-the-way"><a href="#By-the-way" class="headerlink" title="By the way"></a>By the way</h3><p>Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks(SIGIR2018)</p><ul><li>LSTNet</li><li>åˆ©ç”¨æ•°æ®ä¸­çš„å‘¨æœŸæ¨¡å¼ï¼ŒGRUä¸­è®¡ç®—$t$æ—¶åˆ»çš„éšå‘é‡æ—¶ï¼Œä¸æ˜¯ä»¥$t-1$æ—¶åˆ»çš„éšå‘é‡ä¸ºè¾“å…¥ï¼Œè€Œæ˜¯ä»¥$t-p$æ—¶åˆ»çš„éšå‘é‡ä¸ºè¾“å…¥($p$ä¸ºå‘¨æœŸ)</li></ul><p><img src="https://pic2.zhimg.com/80/v2-2b3ff23e31f2a6fbc85058f5754b2c09_1440w.jpg" alt=""></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li>ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹MTNet</li><li>MTNetç”±ä¸€ä¸ªå¤§çš„å†…å­˜ç»„ä»¶ã€ä¸‰ä¸ªç‹¬ç«‹çš„ç¼–ç å™¨å’Œä¸€ä¸ªè”åˆè®­ç»ƒçš„è‡ªå›å½’ç»„ä»¶ç»„æˆ</li><li>å¯è§£é‡Šæ€§</li></ul><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>DA-RNNä¸è€ƒè™‘å¤–æºæ•°æ®ä¸åŒæˆåˆ†é—´çš„ç©ºé—´ç›¸å…³æ€§ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œåœ¨ç¬¬äºŒé˜¶æ®µDA-RNNè¿›è¡Œçš„ç‚¹å¼æ³¨æ„å¯èƒ½ä¸åˆé€‚æ•æ‰è¿ç»­çš„å‘¨æœŸæ¨¡å¼</p></li><li><p>è®¤ä¸ºLSTNetç½‘ç»œä¸ºäº†è€ƒè™‘å‘¨æœŸä¿¡æ¯ï¼Œéœ€è¦å¼•å…¥è¶…å‚$p$ï¼Œåœ¨å‘¨æœŸé•¿åº¦ä¼šå‘ç”Ÿå˜åŒ–çš„ç¯å¢ƒä¸­ï¼Œè¿™ä¸ªè¶…å‚pæ˜¯æœªçŸ¥çš„</p></li></ul><h3 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h3><h4 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h4><script type="math/tex; mode=display">Y = {y_{1},y_{2},...,y_{T}},y_{t} \in R^{D}</script><p>Dè¡¨ç¤ºå˜é‡çš„ç»´åº¦</p><h4 id="Memory-Time-series-Network"><a href="#Memory-Time-series-Network" class="headerlink" title="Memory Time-series Network"></a>Memory Time-series Network</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/1585821126570.jpg" alt=""></p><p>long-term time series historical data</p><p>{$X_{i}$} = <script type="math/tex">X_{1},...,X_{n}</script></p><p>that are to be store in the memory</p><p>$Q$<br>a short-term historical time series data</p><p><strong>è¿™é‡Œçš„{$X_{i}$}å’ŒQä¸é‡å </strong></p><p>Encoder Architecture</p><p>ä½¿ç”¨éæ± åŒ–çš„å·ç§¯å±‚æ¥æå–æ—¶é—´ç»´åº¦ä¸Šçš„çŸ­æœŸæ¨¡å¼å’Œå˜é‡ä¹‹é—´çš„å±€éƒ¨ä¾èµ–å…³ç³»</p><script type="math/tex; mode=display">X \in T Ã— D</script><p>å·ç§¯å±‚ç”±å¤šä¸ªå†…æ ¸ç»„æˆï¼Œå°ºå¯¸éƒ½æ˜¯w Ã— Dï¼Œæ‰€ä»¥æœ€åçš„å·ç§¯ç»“æœå°±æ˜¯(T-w+1) Ã— 1</p><p>ä¸€å…±æœ‰$d_{c}$ä¸ªè¿‡æ»¤å™¨ï¼ˆå·ç§¯æ ¸ï¼‰ï¼Œå¾—åˆ°äº†</p><script type="math/tex; mode=display">d_{c} Ã— T_{c}</script><p>çŸ©é˜µ</p><p>åŠ ä¸Šattentionå±‚ï¼Œå†ä½¿ç”¨RNN(GRU)</p><p>é¦–å…ˆï¼Œä¸€æ¡æ—¶åºæ•°æ®è¢«åˆ†ä¸ºé•¿æœŸçš„å†å²æ•°æ®{$X_{i}$}å’Œæœ€è¿‘çš„å†å²æ•°æ®$Q$ ï¼Œ{$X_{i}$}å’Œ$Q$æ²¡æœ‰é‡åˆéƒ¨åˆ†ã€‚<br>æ¯ä¸€ä¸ªX_{i}é€šè¿‡$Encoder_{m}$ç½‘ç»œå¾—åˆ°è¡¨ç¤º$m_{i}$ ï¼›$Q$é€šè¿‡$Encoder_{in}$ç½‘ç»œå¾—åˆ°è¡¨ç¤º$u$ã€‚<br>å…¶ä¸­$Encoder_{m}$å’Œ$Encoder_{in}$çš„ç»“æ„éƒ½ä¸ºä¸ŠèŠ‚æ‰€è¿°çš„ Encoder ç½‘ç»œã€‚</p><p>å°†$u$å’Œæ¯ä¸€ä¸ª$m_{i}$åšå†…ç§¯ï¼Œåœ¨é€šè¿‡Softmaxå‡½æ•°å½’ä¸€åŒ–ï¼Œå¾—åˆ°ä¸€ç³»åˆ—çš„æƒé‡$p_{i}$ã€‚</p><script type="math/tex; mode=display">p_{i} = Softmax(u^{T}m_{i})</script><p><strong>æœ¬æ–‡çš„äº®ç‚¹æ˜¯attentionçš„è®¾è®¡ï¼Œå³æƒé‡$p_{i}$çš„å¾—åˆ°ã€‚æƒé‡$p_{i}$è¶Šå¤§è¡¨ç¤ºQå’ŒX_{i}ä½ç½®ä¹‹å‰çš„ä¸€æ®µåºåˆ—çš„ç›¸ä¼¼ç¨‹åº¦å¤§ï¼Œåˆ™è¯¥æ®µåºåˆ—å¯¹äºå½“å‰çš„é¢„æµ‹æ›´é‡è¦</strong></p><p>æ¯ä¸€ä¸ª$X_{i}$å†é€šè¿‡$Encoder_{c}$ç½‘ç»œå¾—åˆ°è¡¨ç¤º$c_{i}$ã€‚æ¯ä¸€ä¸ª$c_{i}$å†å’Œ$p_{i}$ç›¸ä¹˜å¾—åˆ°$o_{i}$ã€‚</p><script type="math/tex; mode=display">o_{i} = p_{i} * c_{i}</script><p>å°†æ‰€æœ‰çš„$o_{i}$å’Œ$u$æ‹¼æ¥åé€šè¿‡WçŸ©é˜µè½¬æ¢ï¼Œå¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹è¾“å‡ºï¼š</p><script type="math/tex; mode=display">y_{t}^{D} = W^{D}[u;o_{1};o_{2};...;o_{T}] + b</script><p>Autoregressive Componentï¼ˆè‡ªå›å½’æ¨¡å‹ï¼‰</p><script type="math/tex; mode=display">y_{t,i}^{L} = \sum_{k=0}^{s^{ar}-1}w_{k}^{ar}q_{t-k,i}+b^{ar}</script><p>The final prediction of MTNet</p><script type="math/tex; mode=display">y_{t} = y_{t}^{D} + y_{t}^{L}</script><p>å°†éçº¿æ€§ï¼ˆMTNet ä¸­çš„ç¥ç»ç½‘ç»œéƒ¨åˆ†ï¼‰å’Œçº¿æ€§ï¼ˆMTNet ä¸­çš„è‡ªå›å½’éƒ¨åˆ†ï¼‰æ¨¡å‹è¿›è¡Œensembleã€‚å¾—åˆ°æœ€ç»ˆçš„é¢„æµ‹è¾“å‡ºã€‚</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h4 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h4><p>2 univariate datasets:  Beijing PM2.51, GEFCom(2014) Electricity Price (Hong et al. 2016),</p><p>4 multivariate datasets: Traffic, Solar-Energy, Electricity,Exchange-Rate. </p><h4 id="Methods-for-comparison"><a href="#Methods-for-comparison" class="headerlink" title="Methods for comparison"></a>Methods for comparison</h4><h4 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h4><p>univariate: Root Mean Squared Error (RMSE)/Mean Absolute Error (MAE)</p><p>multivariate: Root Relative Squared Error (RRSE)/Empirical Correlation Coefficient (CORR)</p><h4 id="Interpretability-of-MTNet"><a href="#Interpretability-of-MTNet" class="headerlink" title="Interpretability of MTNet"></a>Interpretability of MTNet</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/1585836468820.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A Memory-Network Based Solution for Multivariate Time-Series Forecasting&lt;br&gt;
    
    </summary>
    
    
      <category term="å‘¨æŠ¥" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-3-27å‘¨æŠ¥-å•æ²™å˜‰</title>
    <link href="http://arithmeticjia.github.io/2020/03/26/2020-3-27%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/03/26/2020-3-27%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-03-26T09:05:17.000Z</published>
    <updated>2020-03-27T12:17:38.751Z</updated>
    
    <content type="html"><![CDATA[<p>PTMs-Pre-trained Models,PTMs(é¢„è®­ç»ƒæ¨¡å‹)<br><a id="more"></a></p><h3 id="å‰è¨€"><a href="#å‰è¨€" class="headerlink" title="å‰è¨€"></a>å‰è¨€</h3><p>åœ¨å›¾åƒé¢†åŸŸï¼Œé¢„è®­ç»ƒè¿‡ç¨‹æ˜¯ä¸€ä¸ªæ¯”è¾ƒå¸¸è§„çš„åšæ³•ï¼Œå¯¹äºå›¾åƒæ¥è¯´ä¸€èˆ¬æ˜¯CNNçš„å¤šå±‚å åŠ ç½‘ç»œç»“æ„ã€‚ä¸€èˆ¬æ­¥éª¤å¦‚ä¸‹ï¼š</p><ul><li>å…ˆç°åœ¨ä¸€ä¸ªå¤§æ•°æ®é›†Aä¸Šé¢„è®­ç»ƒæ¨¡å‹ï¼Œæ¨¡å‹å‚æ•°ä¿å­˜ä¸‹æ¥ï¼Œè®¾ä¸ºModel_A</li><li>å¯¹äºtargetä»»åŠ¡Bï¼Œåœ¨Model_Açš„åŸºç¡€ä¸Šè¿›è¡Œå†è®­ç»ƒModel_Bï¼Œè¿™æ—¶å€™ä¸€èˆ¬ç­–ç•¥æ˜¯ä½¿ç”¨Model_Açš„åº•å±‚ç½‘ç»œå‚æ•°åˆå§‹åŒ–Model_Bï¼Œä¸Šå±‚å‚æ•°éšæœºåˆå§‹åŒ–å¹¶è®­ç»ƒ</li></ul><p>å¯¹äºåº•å±‚å‚æ•°ï¼Œä¸€èˆ¬æœ‰ä¸¤ç§ç­–ç•¥ï¼š</p><ul><li>ä¸€ç§æ˜¯Frozenï¼Œå³è®­ç»ƒModel_Bè¿‡ç¨‹ä¸­åº•å±‚ç½‘ç»œå‚æ•°è®¾ä¸ºä¸å¯è®­ç»ƒçš„ï¼Œç›´æ¥ä½¿ç”¨Model_Aè®­ç»ƒå¥½çš„å‚æ•°</li><li>å¦ä¸€ç§æ˜¯Fine-tuningï¼Œå³è®­ç»ƒModel_Bè¿‡ç¨‹ä¸­åº•å±‚ç½‘ç»œå‚æ•°è®¾ä¸ºå¯è®­ç»ƒçš„ï¼Œåœ¨Model_Aè®­ç»ƒå¥½çš„å‚æ•°åŸºç¡€ä¸Šå¾®è°ƒ</li></ul><p>åœ¨NLPé¢†åŸŸï¼Œä½¿ç”¨word embedding</p><p>Word2vecâ€”-&gt;ELMO(Embedding from Language Models)â€”-&gt;GPT(Generative Pre-Training)â€”-&gt;BERT(Bidirectional Encoder Representations from Transformers)</p><h3 id="word-embeddingï¼ˆè¯åµŒå…¥ï¼‰"><a href="#word-embeddingï¼ˆè¯åµŒå…¥ï¼‰" class="headerlink" title="word embeddingï¼ˆè¯åµŒå…¥ï¼‰"></a>word embeddingï¼ˆè¯åµŒå…¥ï¼‰</h3><p>é«˜ç»´è¯å‘é‡åµŒå…¥åˆ°ä¸€ä¸ªä½ç»´ç©ºé—´</p><p>Embeddingæ˜¯æ•°å­¦é¢†åŸŸçš„æœ‰åè¯ï¼Œæ˜¯æŒ‡æŸä¸ªå¯¹è±¡ X è¢«åµŒå…¥åˆ°å¦å¤–ä¸€ä¸ªå¯¹è±¡ Y ä¸­ï¼Œæ˜ å°„ f : X â†’ Y </p><p>Word Embedding æ˜¯NLPä¸­ä¸€ç»„è¯­è¨€æ¨¡å‹å’Œç‰¹å¾å­¦ä¹ æŠ€æœ¯çš„æ€»ç§°ï¼ŒæŠŠè¯æ±‡è¡¨ä¸­çš„å•è¯æˆ–è€…çŸ­è¯­æ˜ å°„æˆç”±å®æ•°æ„æˆçš„å‘é‡ä¸Š(æ˜ å°„)</p><h4 id="One-Hot-Representation"><a href="#One-Hot-Representation" class="headerlink" title="One-Hot-Representation"></a>One-Hot-Representation</h4><p><img src="https://pic2.zhimg.com/80/v2-09e1bda72c4b903e25db203ab4aa6dc6_1440w.jpg" alt=""></p><p>åœ¨one hot representationç¼–ç çš„æ¯ä¸ªå•è¯éƒ½æ˜¯ä¸€ä¸ªç»´åº¦ï¼Œå½¼æ­¤independent</p><p>è¯­æ–™åº“<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">John likes to watch movies.Marry likes too.</span><br><span class="line">John also likes to watch football games.</span><br></pre></td></tr></table></figure><br>è¯å…¸<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "John": 1ï¼Œ</span><br><span class="line">    "likes": 2,</span><br><span class="line">    "to": 3,</span><br><span class="line">    "watch": 4,</span><br><span class="line">    "movies": 5,</span><br><span class="line">    "also": 6,</span><br><span class="line">    "football": 7, </span><br><span class="line">    "games": 8,</span><br><span class="line">    "Marry": 9,</span><br><span class="line">    "too": 10</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>one-hotè¡¨ç¤º<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">John:[1,0,0,0,0,0,0,0,0,0]</span><br><span class="line">likes:[0,1,0,0,0,0,0,0,0,0]</span><br><span class="line">...</span><br><span class="line">too:[0,0,0,0,0,0,0,0,0,1]</span><br></pre></td></tr></table></figure><br>å†ä¸¾ä¸ªä¾‹å­</p><p><img src="https://img-blog.csdnimg.cn/20190807181106401.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI2ODExMzc3,size_16,color_FFFFFF,t_70" alt=""><br>ä¸€å…±æœ‰å››ç§çŠ¶æ€ï¼Œ1ï¼Œ2ï¼Œ3ï¼Œ4<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1 -&gt; 0001</span><br><span class="line">2 -&gt; 0010</span><br><span class="line">3 -&gt; 0100</span><br><span class="line">4 -&gt; 1000</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/20190808002943747.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI2ODExMzc3,size_16,color_FFFFFF,t_70" alt=""></p><ul><li>æ— æ³•æ•æ‰ä¸¤ä¸ªwordä¹‹é—´çš„å…³ç³»ï¼Œä¹Ÿå°±æ˜¯æ²¡æœ‰åŠæ³•æ•æ‰è¯­ä¹‰ä¿¡æ¯</li><li>è¯å‘é‡å¯èƒ½éå¸¸é•¿</li></ul><h4 id="Distributed-Representation"><a href="#Distributed-Representation" class="headerlink" title="Distributed-Representation"></a>Distributed-Representation</h4><h4 id="N-Gram"><a href="#N-Gram" class="headerlink" title="N-Gram"></a>N-Gram</h4><p>N-Gramæ˜¯åŸºäºä¸€ä¸ªå‡è®¾ï¼šç¬¬nä¸ªè¯å‡ºç°ä¸å‰n-1ä¸ªè¯ç›¸å…³ï¼Œè€Œä¸å…¶ä»–ä»»ä½•è¯ä¸ç›¸å…³</p><script type="math/tex; mode=display">S = (w_{1},w_{2},w_{3},...,w_{n})</script><p>å‡è®¾æ¯ä¸€ä¸ªå•è¯$w_{i}$éƒ½è¦ä¾èµ–äºç¬¬ä¸€ä¸ªå•è¯åˆ°$w_{1}$åˆ°ä»–ä¹‹å‰çš„ä¸€ä¸ªå•è¯$w_{i-1}$çš„å½±å“</p><script type="math/tex; mode=display">p(S) = p(w_{1},w_{2},w_{3},...,w_{n})=p(w_{1})p(w_{2}|w_{1})...p(w_{n}|w_{n-1}w_{n-2}...w_{1})</script><p>ä¸å¦¨åˆ©ç”¨é©¬å°”ç§‘å¤«å‡è®¾<br>å³å½“å‰è¿™ä¸ªè¯ä»…ä»…è·Ÿå‰é¢å‡ ä¸ªæœ‰é™çš„è¯ç›¸å…³ï¼Œå› æ­¤ä¹Ÿå°±ä¸å¿…è¿½æº¯åˆ°æœ€å¼€å§‹çš„é‚£ä¸ªè¯ï¼Œè¿™æ ·ä¾¿å¯ä»¥å¤§å¹…ç¼©å‡ä¸Šè¿°ç®—å¼çš„é•¿åº¦</p><script type="math/tex; mode=display">p(S) = p(w_{1},w_{2},w_{3},...,w_{n})=\prod p(w_{i}|w_{i-1}...w_{1})â‰ˆ\prod p(w_{i}|w_{i-1}...w_{i-N+1})</script><p>å½“N=2æ—¶ï¼Œç§°ä¸ºBi-Gram<br>å½“N=3æ—¶ï¼Œç§°ä¸ºTri-Gram<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">I am John</span><br><span class="line">John I am</span><br><span class="line">I like learning</span><br></pre></td></tr></table></figure><br>æ­¤æ—¶Tri-Gramä¸‹</p><script type="math/tex; mode=display">p(am|I) = 2/3</script><p>ä¸¤ä¸ªé‡è¦åº”ç”¨åœºæ™¯</p><ul><li>è¯„ä¼°å¥å­ä¹‹é—´å·®å¼‚æ€§</li><li>è¯„ä¼°ä¸€ä¸ªå¥å­æ˜¯å¦åˆç†<br>N-Gramè·ç¦»<script type="math/tex; mode=display">s = "ABCD"</script><script type="math/tex; mode=display">t = "ABC"</script>å½“N=2æ—¶ï¼Œç¬¬ä¸€ä¸ªå­—ç¬¦ä¸²å¯ä»¥æ‹†æˆ<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(AB,BC,CD)</span><br></pre></td></tr></table></figure>ç¬¬äºŒä¸ªå­—ç¬¦ä¸²å¯ä»¥æ‹†æˆ<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(AB,BC)</span><br></pre></td></tr></table></figure>è·ç¦»å…¬å¼<script type="math/tex; mode=display">|G_{N}(s)| + |G_{N}(t)| - 2 \times |G_{N}(s) \cap G_{N}(t)|</script>d = 4 + 3 - 2 * 1 = 1</li></ul><h3 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h3><p>é¢„å¤‡çŸ¥è¯†å›é¡¾</p><h4 id="sigmoidå‡½æ•°"><a href="#sigmoidå‡½æ•°" class="headerlink" title="sigmoidå‡½æ•°"></a>sigmoidå‡½æ•°</h4><script type="math/tex; mode=display">\sigma (x) = \frac{1}{1+e^{-x}}</script><p><img src="https://www.guanacossj.com/media/articlebodypics/sigmoid.jpg" alt=""><br>æ±‚å¯¼</p><script type="math/tex; mode=display">\sigma^{'} (x) = \sigma (x)[1-\sigma (x)]</script><p>æ˜“å¾—</p><script type="math/tex; mode=display">[log\sigma (x)]^{'} = 1 - \sigma (x)</script><script type="math/tex; mode=display">[log(1-\sigma (x))]^{'} = - \sigma (x)</script><h4 id="è´å¶æ–¯å…¬å¼"><a href="#è´å¶æ–¯å…¬å¼" class="headerlink" title="è´å¶æ–¯å…¬å¼"></a>è´å¶æ–¯å…¬å¼</h4><script type="math/tex; mode=display">P(A|B) = \frac {P(A,B)}{P(B)}</script><script type="math/tex; mode=display">P(B|A) = \frac {P(A,B)}{P(A)}</script><script type="math/tex; mode=display">P(A|B) = P(A) \frac {P(B|A)}{P(A)}</script><h4 id="é€»è¾‘å›å½’äºŒåˆ†ç±»å™¨"><a href="#é€»è¾‘å›å½’äºŒåˆ†ç±»å™¨" class="headerlink" title="é€»è¾‘å›å½’äºŒåˆ†ç±»å™¨"></a>é€»è¾‘å›å½’äºŒåˆ†ç±»å™¨</h4><p>è®¾</p><script type="math/tex; mode=display">\{\{x_{i},y_{i}\}\}_{i=1}^{m}</script><p>äºŒåˆ†ç±»å‡½æ•°é•¿è¿™æ ·</p><script type="math/tex; mode=display">h_{\theta }(x) = \sigma (\theta _{0}+\theta _{1}x_{1}+\theta _{1}x_{1}+...+\theta _{n}x_{n})</script><p>ä»¤</p><script type="math/tex; mode=display">\theta =(\theta_{0},\theta_{1},\theta_{2},...,\theta_{n})^{T}</script><p>å…¶ä¸­Î¸ä¸ºå¾…å®šå‚æ•°</p><p>ç®€åŒ–äºŒåˆ†ç±»å‡½æ•°</p><script type="math/tex; mode=display">h_{\theta }(x) = \sigma (\theta ^{T}x)=\frac{1}{1+e^{-\theta ^{T}x}}</script><p>å–é˜ˆå€¼T=0.5</p><h4 id="Huffmanæ ‘-ç¼–ç "><a href="#Huffmanæ ‘-ç¼–ç " class="headerlink" title="Huffmanæ ‘-ç¼–ç "></a>Huffmanæ ‘-ç¼–ç </h4><p>æœ€ä¼˜äºŒå‰æ ‘â€”-å¸¦æƒè·¯å¾„é•¿åº¦æœ€çŸ­çš„äºŒå‰æ ‘</p><p>â€œæˆ‘â€ï¼Œâ€å–œæ¬¢â€ï¼Œâ€è§‚çœ‹â€ï¼Œâ€å·´è¥¿â€ï¼Œâ€è¶³çƒâ€ï¼Œâ€ä¸–ç•Œæ¯â€<br> 15     8      6      5      3       1</p><p>é€‰æ ¹èŠ‚ç‚¹æœ€å°çš„æ ‘åˆå¹¶</p><p><img src="https://www.guanacossj.com/media/articlebodypics/1585241496316.jpg" alt=""></p><p>è¯é¢‘è¶Šå¤§çš„è¯ç¦»æ ¹èŠ‚ç‚¹è¶Šè¿‘<br>æ˜¾ç„¶è¯é¢‘è¶Šå°ï¼Œæƒé‡è¶Šå°</p><p><img src="https://www.guanacossj.com/media/articlebodypics/1585241408762.jpg" alt=""></p><h4 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h4><p>ä½¿ç”¨ä¸€ä¸ªè¯é¢„æµ‹ä¸Šä¸‹æ–‡</p><h4 id="CBOW-Continues-Bag-of-Words-Model"><a href="#CBOW-Continues-Bag-of-Words-Model" class="headerlink" title="CBOW-Continues Bag-of-Words Model"></a>CBOW-Continues Bag-of-Words Model</h4><p>ä½¿ç”¨ä¸€ä¸ªè¯è¯­çš„ä¸Šä¸‹æ–‡ä½œä¸ºè¾“å…¥ï¼Œæ¥é¢„æµ‹è¿™ä¸ªè¯è¯­æœ¬èº«</p><p><img src="https://www.guanacossj.com/media/articlebodypics/1585297454343.jpg" alt=""></p><p><strong>è¾“å…¥å±‚åˆ°éšè—å±‚</strong></p><p>è¾“å…¥å±‚æ˜¯å››ä¸ªè¯çš„one-hotå‘é‡è¡¨ç¤ºï¼Œåˆ†åˆ«æ˜¯$x_{t-2}$,$x_{t-1}$,$x_{t+1}$,$x_{t+2}$ï¼Œç»´åº¦éƒ½æ˜¯VÃ—1ï¼ŒVæ˜¯æ¨¡å‹çš„è®­ç»ƒæœ¬æ–‡ä¸­æ‰€æœ‰è¯çš„ä¸ªæ•°</p><p>è¾“å…¥å±‚åˆ°éšè—å±‚çš„æƒé‡çŸ©é˜µä¸ºWï¼Œç»´åº¦ä¸ºVÃ—dï¼Œdæ˜¯è®¤ä¸ºç»™å®šçš„è¯å‘é‡ç»´åº¦ï¼Œéšè—å±‚çš„å‘é‡ä¸ºhï¼Œç»´åº¦æ˜¯dÃ—1</p><script type="math/tex; mode=display">h = \frac{W*x_{t-2}+W*x_{t-1}+W*x_{t+1}+W*x_{t+2}}{4}</script><p><strong>éšè—å±‚åˆ°è¾“å‡ºå±‚</strong></p><p>è®°éšè—å±‚åˆ°è¾“å‡ºå±‚çš„æƒé‡çŸ©é˜µä¸ºUï¼Œç»´åº¦ä¸ºdÃ—Vï¼Œè¾“å‡ºå‘é‡ä¸ºyï¼Œç»´åº¦ä¸ºVÃ—1ï¼Œé‚£ä¹ˆ</p><script type="math/tex; mode=display">y = softmax(U^{T}*h)</script><p>æ­¤æ—¶è¾“å‡ºå±‚çš„å‘é‡yå’Œè¾“å…¥å±‚çš„å‘é‡xï¼Œè™½ç„¶ç»´åº¦ä¸€æ ·ï¼Œä½†æ˜¯yå¹¶ä¸æ˜¯one-hotå‘é‡ï¼Œå‡è®¾è®­ç»ƒæ ·æœ¬æ˜¯</p><p>â€œI like to eat appleâ€ï¼Œæ­¤æ—¶ç”¨â€Iâ€,â€likeâ€,â€eatâ€,â€appleâ€é¢„æµ‹â€toâ€ï¼Œè¾“å‡ºçš„yå‘é‡å¤§æ¦‚æ˜¯è¿™æ ·</p><p><img src="https://pic4.zhimg.com/80/v2-918b97c077fe15b4a67e0afddb62bfa3_1440w.jpg" alt=""></p><p>ç›®çš„æ˜¯æ„é€ æœ€å¤§åŒ–å‡½æ•°L</p><script type="math/tex; mode=display">L = \prod_{t=1}^{V}p(w_{t}|w_{t-k},w_{t-k+1},...,w_{t-1},w_{t+1},...,w_{t+k-1},w_{t+k})</script><h4 id="å±‚æ¬¡softmaxå’Œè´Ÿé‡‡æ ·"><a href="#å±‚æ¬¡softmaxå’Œè´Ÿé‡‡æ ·" class="headerlink" title="å±‚æ¬¡softmaxå’Œè´Ÿé‡‡æ ·"></a>å±‚æ¬¡softmaxå’Œè´Ÿé‡‡æ ·</h4><p>å±‚æ¬¡softmaxæ˜¯ä¸€æ£µhuffmanæ ‘ï¼Œæ ‘çš„å¶å­èŠ‚ç‚¹æ˜¯è®­ç»ƒæ–‡æœ¬ä¸­æ‰€æœ‰çš„è¯ï¼Œéå¶å­èŠ‚ç‚¹éƒ½æ˜¯ä¸€ä¸ªé€»è¾‘å›å½’äºŒåˆ†ç±»å™¨ï¼Œæ¯ä¸ªé€»è¾‘å›å½’åˆ†ç±»å™¨çš„å‚æ•°éƒ½ä¸åŒï¼Œåˆ†åˆ«ç”¨$Î¸_{*}$è¡¨ç¤ºã€‚</p><p><img src="https://www.guanacossj.com/media/articlebodypics/1585301521653.jpg" alt=""></p><p>åˆ†ç±»å™¨çš„è¾“å…¥æ˜¯å‘é‡h(éšè—å±‚å‘é‡)</p><p>é‡‡æ ·åˆ° I çš„æ¦‚ç‡$p(I|context) = (1-\sigma(\theta_{1}h)) * (1-\sigma(\theta_{3}h))$</p><p>é‡‡æ ·åˆ° eat çš„æ¦‚ç‡$p(eat|context) = (1-\sigma(\theta_{1}h)) * \sigma(\theta_{3}h)$</p><p>é‡‡æ ·åˆ° to çš„æ¦‚ç‡$p(to|context) = \sigma(\theta_{1}h) * (1-\sigma(\theta_{2}h))$</p><p>æ­£æ ·æœ¬</p><script type="math/tex; mode=display">p(w|context(w)) = \sigma(\theta^{w}h^{context(w})</script><p>è´Ÿæ ·æœ¬</p><script type="math/tex; mode=display">p(w|NEG(w)) = \sigma(\theta^{w}h^{NEG(w})</script><script type="math/tex; mode=display">L_{CBOW} = \prod_{t=1}^{V} p(w^{t}|context(w^{t}))p(w^{t}|NEG(w^{t}))</script><p>æœ€å¤§åŒ–â€”-&gt;æ¢¯åº¦ä¸Šå‡æ³•</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PTMs-Pre-trained Models,PTMs(é¢„è®­ç»ƒæ¨¡å‹)&lt;br&gt;
    
    </summary>
    
    
      <category term="å‘¨æŠ¥" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-3-20å‘¨æŠ¥-å•æ²™å˜‰</title>
    <link href="http://arithmeticjia.github.io/2020/03/20/2020-3-20%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/03/20/2020-3-20%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-03-20T09:08:59.000Z</published>
    <updated>2020-03-20T09:40:19.918Z</updated>
    
    <content type="html"><![CDATA[<p>searching and mining trillions of time series subsquences under dynamic time warping<br><a id="more"></a></p><h3 id="Assumptions"><a href="#Assumptions" class="headerlink" title="Assumptions"></a>Assumptions</h3><ul><li>Time Series Subsequences must be Normalized ï¼ˆæ—¶é—´åºåˆ—å­åºåˆ—å¿…é¡»ç»è¿‡å½’ä¸€åŒ–å¤„ç†ï¼‰</li><li>Dynamic Time Warping is the Best Measure</li><li>Arbitrary Query Lengths cannot be Indexed</li><li>There Exists Data Mining Problems that we are Willing to Wait Some Hours to Answer</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;searching and mining trillions of time series subsquences under dynamic time warping&lt;br&gt;
    
    </summary>
    
    
      <category term="å‘¨æŠ¥" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-3-13å‘¨æŠ¥-å•æ²™å˜‰</title>
    <link href="http://arithmeticjia.github.io/2020/03/10/2020-3-13%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/03/10/2020-3-13%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-03-10T13:13:01.000Z</published>
    <updated>2020-03-13T05:36:21.778Z</updated>
    
    <content type="html"><![CDATA[<p>DTW(Dynamic Time Warping)<br>åŠ¨æ€æ—¶é—´è§„æ•´<br><a id="more"></a></p><h3 id="DTW-Dynamic-Time-Warping"><a href="#DTW-Dynamic-Time-Warping" class="headerlink" title="DTW(Dynamic Time Warping)"></a>DTW(Dynamic Time Warping)</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/dtw01.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/dtw02.jpg" alt=""></p><p>å‡è®¾æœ‰ä¸¤ä¸ªåºåˆ—ï¼š</p><p>a = [2, 0, 1, 1, 2, 4, 2, 1, 2, 0]</p><p>b = [1, 1, 2, 4, 2, 1, 2, 0]</p><p>ç”¨æ¬§å¼è·ç¦»è®¡ç®—å‡ºæ¯åºåˆ—çš„æ¯ä¸¤ç‚¹ä¹‹é—´çš„è·ç¦»</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[1. 1. 0. 2. 0. 1. 0. 2.]</span><br><span class="line"> [1. 1. 2. 4. 2. 1. 2. 0.]</span><br><span class="line"> [0. 0. 1. 3. 1. 0. 1. 1.]</span><br><span class="line"> [0. 0. 1. 3. 1. 0. 1. 1.]</span><br><span class="line"> [1. 1. 0. 2. 0. 1. 0. 2.]</span><br><span class="line"> [3. 3. 2. 0. 2. 3. 2. 4.]</span><br><span class="line"> [1. 1. 0. 2. 0. 1. 0. 2.]</span><br><span class="line"> [0. 0. 1. 3. 1. 0. 1. 1.]</span><br><span class="line"> [1. 1. 0. 2. 0. 1. 0. 2.]</span><br><span class="line"> [1. 1. 2. 4. 2. 1. 2. 0.]]</span><br></pre></td></tr></table></figure><p>waring path W</p><p>$W = w_{1},w_{2},w_{3},â€¦,w_{k}$ </p><p>$max(m,n) &lt;= k &lt; m+n-1$</p><p>$w_{k} = (i,j)_{k}$</p><script type="math/tex; mode=display">DTW(Q,C) = \min (\frac{\sum_{k=1}^{K}w_{k}}{K})</script><p>åˆ†æ¯ä¸­çš„Kä¸»è¦æ˜¯ç”¨æ¥å¯¹ä¸åŒçš„é•¿åº¦çš„è§„æ•´è·¯å¾„åšè¡¥å¿</p><p>é‡‡ç”¨åŠ¨æ€è§„åˆ’ç®—æ³•ã€‚å‡è®¾æˆ‘ä»¬è¦æ±‚åˆ°ä½ç½®(ğ‘–,ğ‘—)çš„æœ€å°ç´¯è®¡è·ç¦»ğ·(ğ‘–,ğ‘—)ï¼Œé‚£ä¹ˆå®ƒåªèƒ½ç”±ğ·(ğ‘–âˆ’1,ğ‘—)ï¼Œğ·(ğ‘–,ğ‘—âˆ’1)å’Œğ·(ğ‘–âˆ’1,ğ‘—âˆ’1)è¿™ä¸‰ä¸ªä½ç½®çš„æœ€å°ç´¯è®¡è·ç¦»ä¸­å¯»æ‰¾ï¼Œä¹Ÿå°±æ˜¯</p><script type="math/tex; mode=display">ğ·(i,j)=d_{i,j}+ğ‘šğ‘–ğ‘›[ğ·(iâˆ’1,j),ğ·(i,jâˆ’1),ğ·(iâˆ’1,jâˆ’1)]</script><p><img src="https://www.guanacossj.com/media/articlebodypics/dtw.jpg" alt=""></p><h3 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a>æ€»ç»“</h3><p>DTWæ–¹æ³•æ˜¯æ¬§æ°è·ç¦»æ–¹æ³•çš„æ”¹è¿›ï¼Œåªæ”¹è¿›äº†å…¶ä¸èƒ½å¤„ç†local time shiftingçš„é—®é¢˜ã€‚æ²¡æœ‰å¼•å…¥ä»»ä½•é˜ˆå€¼å‚æ•°ï¼Œå› æ­¤å¯¹æ—¶é—´ä¸Šçš„åç§»ï¼ˆå™ªå£°å’Œç¦»ç¾¤ç‚¹ï¼‰çš„æŠ‘åˆ¶å¹¶ä¸å¥½ï¼Œä¸”å¯¹æ—¶é—´ä¸Šçš„åç§»çš„é€‚åº”æ€§ä¹Ÿä¸å¥½ã€‚</p><p>ä¼˜ç‚¹ï¼šä½¿ç”¨åŠ¨æ€è§„åˆ’çš„æ€æƒ³ï¼Œå®ç°äº†å¯¹æŸäº›ç‚¹çš„é‡å¤ä½¿ç”¨ï¼Œç¡®ä¿é‡å¤ä½¿ç”¨çš„ç‚¹è¾¾æˆçš„è·¯å¾„æœ€ä¼˜çš„ï¼Œä»è€Œè¾ƒä¸ºé«˜æ•ˆåœ°è§£å†³äº†æ•°æ®ä¸å¯¹é½çš„é—®é¢˜ã€‚</p><p>ç¼ºç‚¹ï¼šè¿˜æ˜¯æ— æ³•å¤„ç†ç¦»ç¾¤ç‚¹ã€å¼‚å¸¸ç‚¹ï¼Œå¯¹äºå™ªå£°çš„æŠ‘åˆ¶æ²¡æœ‰è¿›è¡Œå¤„ç†ã€‚è™½ç„¶èƒ½å¤Ÿå¤„ç†local time shiftingï¼Œä½†æ˜¯å¯¹æ—¶é—´ä¸Šçš„åç§»åšçš„ä¹Ÿä¸å¥½ã€‚ç®—æ³•ä¹Ÿä¸æ˜¯metricç±»å‹çš„ã€‚</p><h3 id="åº”ç”¨"><a href="#åº”ç”¨" class="headerlink" title="åº”ç”¨"></a>åº”ç”¨</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DTW(Dynamic Time Warping)&lt;br&gt;åŠ¨æ€æ—¶é—´è§„æ•´&lt;br&gt;
    
    </summary>
    
    
      <category term="å‘¨æŠ¥" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-3.6å‘¨æŠ¥-å•æ²™å˜‰</title>
    <link href="http://arithmeticjia.github.io/2020/03/06/2020-3-6%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/03/06/2020-3-6%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-03-06T04:34:50.000Z</published>
    <updated>2020-03-11T08:49:17.306Z</updated>
    
    <content type="html"><![CDATA[<p>Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models<br><a id="more"></a></p><h3 id="Shape-and-Time-Distortion-Loss-for-Training-DeepTime-Series-Forecasting-Models"><a href="#Shape-and-Time-Distortion-Loss-for-Training-DeepTime-Series-Forecasting-Models" class="headerlink" title="Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models"></a>Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models</h3><p>NeurIPS 2019</p><p>è®­ç»ƒæ·±åº¦æ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹çš„å½¢çŠ¶å’Œæ—¶é—´å¤±çœŸæŸå¤±</p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>This paper addresses the problem of time series forecasting for non-stationary signals and multiple future steps prediction. </p><p>DILATE (DIstortion Loss including shApe and TimE) å½¢çŠ¶å’Œæ—¶é—´å¤±çœŸæŸå¤±</p><p>DILATE aims at accurately predicting sudden changes, and explicitly incorporates two terms supporting precise shape and temporal change detection.</p><p><img src="https://img-blog.csdn.net/20180824212209631?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMxODIxNjc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><p><img src="https://img-blog.csdn.net/20180824212233242?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMxODIxNjc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><p><img src="https://img-blog.csdn.net/2018082421225311?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMxODIxNjc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>Time series forecasting [6] consists in analyzing the dynamics and correlations between historical data for predicting future behavior</p><p>In one-step prediction problems [39, 30], future prediction reduces to a single scalar value. This is in sharp contrast with multi-step time series prediction [49, 2, 48], which consists in predicting a complete trajectory[trÉ™ËˆdÊ’ektÉ™ri] of future data at a rather long temporal extent. Multi-step forecasting thus requires to accurately describe time series evolution.</p><p><img src="https://pic1.zhimg.com/v2-b872cd50b4a341901005bf4246493fa0_r.jpg" alt=""></p><p>(a) Non informative prediction éä¿¡æ¯æ€§é¢„æµ‹<br>(b) Correct shape, time delay<br>(c) Correct time, inaccurate shape</p><p>In contrast, the DILATE loss proposed in this work, which disentangles shape and temporal decay terms,<br>supports predictions (b) and (c) over prediction (a) that does not capture the sharp change of regime.</p><h4 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h4><p>Time series forecasting Traditional methods for time series forecasting include linear autoregressive models, such as the ARIMA model [6], and Exponential[ËŒekspÉ™ËˆnenÊƒl] Smoothing [27], which both fall into the broad category of linear State Space Models (SSMs) [17].</p><h4 id="Training-Deep-Neural-Networks-with-DILATE-DIstortion-Loss-including-shApe-and-TimE"><a href="#Training-Deep-Neural-Networks-with-DILATE-DIstortion-Loss-including-shApe-and-TimE" class="headerlink" title="Training Deep Neural Networks with DILATE((DIstortion Loss including shApe and TimE))"></a>Training Deep Neural Networks with DILATE((DIstortion Loss including shApe and TimE))</h4><p><img src="https://pic3.zhimg.com/v2-fc51e16266d817369cbd3bcbd6624552_b.jpg" alt=""></p><p>a set of N input time series:</p><script type="math/tex; mode=display">A=\{ X_{i} \}_{i\in \{1:N\}}</script><p>å¯¹äº</p><script type="math/tex; mode=display">x_{i} = (x_{i}^{1},...,x_{i}^{n})</script><p>predicts the future <strong>k-step</strong> ahead trajectory </p><script type="math/tex; mode=display">\hat{y}_{i} = (\hat{y}_{i}^{1},...,\hat{y}_{i}^{k})</script><p>actual ground truth future trajectory</p><script type="math/tex; mode=display">\dot{y}_{i} = (\dot{y}_{i}^{1},...,\dot{y}_{i}^{k})</script><p><img src="https://www.guanacossj.com/media/articlebodypics/1583471554346.jpg" alt=""></p><p><script type="math/tex">\alpha \in [0,1]</script>  hyper parameter [ËˆhaÉªpÉ™(r) pÉ™ËˆrÃ¦mÉªtÉ™(r)] </p><p>Notations and definitions(ç¬¦å·å’Œå®šä¹‰):</p><ul><li>A: a warping path as a binary matrix(äºŒå€¼çŸ©é˜µ)</li></ul><h4 id="Shape-and-temporal-terms"><a href="#Shape-and-temporal-terms" class="headerlink" title="Shape and temporal terms"></a>Shape and temporal terms</h4><h5 id="Shape-term"><a href="#Shape-term" class="headerlink" title="Shape term"></a>Shape term</h5><p>Shape term Our shape loss function is based on the Dynamic Time Warping (DTW)</p><p>The DTW loss focuses on the structural shape dissimilarity between signals</p><p>Temporal term Our second term Ltemporal in Eq (1) aims at penalizing temporal distortions between $\hat{y}_{i}$ $\dot{y}_{i}$</p><h4 id="DILATE-Efficient-Forward-and-Backward-Implementation"><a href="#DILATE-Efficient-Forward-and-Backward-Implementation" class="headerlink" title="DILATE Efficient Forward and Backward Implementation"></a>DILATE Efficient Forward and Backward Implementation</h4><h4 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h4><p>To illustrate the relevance of DILATE, we carry out experiments on 3 non-stationary time series datasets from different domains </p><p><img src="https://pic3.zhimg.com/v2-b22b26e263592f889b52486b8c1f85ce_b.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models&lt;br&gt;
    
    </summary>
    
    
      <category term="å‘¨æŠ¥" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-2-28å‘¨æŠ¥-å•æ²™å˜‰</title>
    <link href="http://arithmeticjia.github.io/2020/02/28/2020-2-28%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/02/28/2020-2-28%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-02-28T04:22:34.000Z</published>
    <updated>2020-02-28T06:32:20.573Z</updated>
    
    <content type="html"><![CDATA[<p>Memory In Memoryï¼ˆå­¦ä¹ é«˜é˜¶éå¹³ç¨³ç‰¹å¾ä¿¡æ¯ï¼‰<br><a id="more"></a><br>Memory In Memory: A Predictive Neural Network for Learning Higher-Order Non-Stationarity from Spatiotemporal Dynamics<br>ä¸€ç§ç”¨äºé«˜é˜¶å­¦ä¹ çš„é¢„æµ‹ç¥ç»ç½‘ç»œæ—¶ç©ºåŠ¨åŠ›å­¦ä¸­çš„éå¹³ç¨³æ€§</p><p>cvpr2019</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>any non-stationary process can be decomposed into deterministic, time-variant polynomials[ËŒpÉ’lÉªËˆnÉ™ÊŠmiÉ™lz] , plus a zero-mean stochastic term. </p><p>ä»»ä½•ä¸€ä¸ªéå¹³ç¨³è¿‡ç¨‹éƒ½å¯ä»¥åˆ†è§£ä¸ºï¼šç¡®å®šé¡¹+æ—¶é—´å˜é‡å¤šé¡¹å¼+é›¶å‡å€¼éšæœºé¡¹</p><p>By applying differencing operations appropriately, we may turn time-variant polynomials into a constant, making the deterministic[dÉªËŒtÉœËmÉªËˆnÉªstÉªk] component predictable.</p><p>é€šè¿‡å·®åˆ†çš„æ“ä½œï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæ—¶é—´å˜é‡å¤šé¡¹å¼è½¬æ¢æˆä¸€ä¸ªå¸¸é‡ï¼Œä½¿ç¡®å®šæ€§çš„ç»„æˆéƒ¨åˆ†å¯é¢„æµ‹</p><p>We propose the Memory In Memory (MIM) networks and corresponding recurrent blocks for this purpose. The MIM blocks exploit the differential signals between adjacent recurrent states to model the non-stationary and approximately stationary properties in spatiotemporal dynamics with two cascaded, self-renewed memory modules.</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>We attempt to resolve this problem by proposing a generic RNNs architecture that is more effective in non-stationarity modeling. </p><p>In particular, the forget gates in the recent PredRNN model [32] does not work appropriately on precipitation forecasting: about 80% of them are saturated over all timestamps, implying almost timeinvariant memory state transitions. </p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h4 id="ARIMA-Autoregressive-Integrated-Moving-Average-Model"><a href="#ARIMA-Autoregressive-Integrated-Moving-Average-Model" class="headerlink" title="ARIMA(Autoregressive Integrated Moving Average Model)"></a>ARIMA(Autoregressive Integrated Moving Average Model)</h4><p>A time-series random variable whose power spectrum remains constant over time can be viewed as a combination of signal and noise. </p><p>åŠŸç‡è°±æ˜¯åŠŸç‡è°±å¯†åº¦å‡½æ•°ï¼ˆPSDï¼‰çš„ç®€ç§°ï¼Œå®ƒå®šä¹‰ä¸ºå•ä½é¢‘å¸¦å†…çš„ä¿¡å·åŠŸç‡</p><p><img src="https://www.guanacossj.com/media/articlebodypics/w.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/w_f.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/p.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/f.jpg" alt=""></p><script type="math/tex; mode=display">|S(f)|^2</script><script type="math/tex; mode=display">\lim_{T->\propto }\frac{1}{T}|S(f)|^2</script><h4 id="Deterministic-Spatiotemporal-Prediction"><a href="#Deterministic-Spatiotemporal-Prediction" class="headerlink" title="Deterministic Spatiotemporal Prediction"></a>Deterministic Spatiotemporal Prediction</h4><h4 id="Stochastic-Spatiotemporal-Prediction"><a href="#Stochastic-Spatiotemporal-Prediction" class="headerlink" title="Stochastic Spatiotemporal Prediction"></a>Stochastic Spatiotemporal Prediction</h4><h3 id="Memory-In-Memory"><a href="#Memory-In-Memory" class="headerlink" title="Memory In Memory"></a>Memory In Memory</h3><p><img src="https://pic1.zhimg.com/v2-dc4a2024ce0201315661daf3b43c6ab8_r.jpg" alt=""></p><p>å·¦è¾¹æ˜¯ST-LSTMç»“æ„ï¼Œå³è¾¹æ˜¯æ›´æ”¹çš„</p><p>ST-LSTMä¸­çš„å¿˜è®°é—¨åŸºæœ¬æ˜¯é¥±å’Œçš„ï¼Œæ‰€ä»¥å®ƒåŸºæœ¬ä¸Šåªè·å–äº†å¹³ç¨³çš„ä¿¡æ¯ï¼Œè€Œæ•´ä¸ªç›´æ¥è”ç³»å°±æ˜¯CçŠ¶æ€å€¼ï¼Œå†åŠ ä¸Šä¸‹é¢çš„è¾“å…¥ä¸ºå·®åˆ†ï¼Œè€Œå·®åˆ†çš„è½¬æ¢å…¶å®å°±æ˜¯éå¹³ç¨³çš„ä¿¡æ¯</p><p><img src="https://pic1.zhimg.com/v2-69b9793882f1a8739d6b1d37336cc808_r.jpg" alt=""></p><p><img src="https://pic3.zhimg.com/v2-f1eac55dddaa00ab46da1b4ea116072e_r.jpg" alt=""></p><p><img src="https://pic3.zhimg.com/80/v2-6ec88fa03dd40e82a037c13ff6b64bd2_1440w.jpg" alt=""></p><h3 id="Memory-In-Memory-Networks"><a href="#Memory-In-Memory-Networks" class="headerlink" title="Memory In Memory Networks"></a>Memory In Memory Networks</h3><p><img src="https://pic3.zhimg.com/v2-06741d37a4c01fce37ef43901f5b311e_r.jpg" alt=""></p><p>çº¢è‰²ç®­å¤´ï¼šç”¨äºå¾®åˆ†å»ºæ¨¡çš„Hçš„å¯¹è§’çŠ¶æ€è½¬ç§»è·¯å¾„</p><p>è“è‰²ç®­å¤´ï¼šå­˜å‚¨å•å…ƒCï¼ŒNå’ŒSçš„æ°´å¹³è½¬æ¢è·¯å¾„</p><p>é»‘è‰²ç®­å¤´ï¼šä¹‹å­—å½¢çŠ¶æ€</p><h3 id="experience"><a href="#experience" class="headerlink" title="experience"></a>experience</h3><p>æ¨¡å‹å‚æ•°ï¼šä¸€å…±å››å±‚ï¼Œç¬¬ä¸€å±‚æ˜¯ST-LSTMï¼Œå…¶ä½™ä¸‰å±‚ä¸ºMIMï¼ŒMIMçš„feature channelä¸º64ï¼Œåˆ©ç”¨l2æŸå¤±ï¼ŒADAM optimizerï¼Œlrä¸º0.001ï¼Œåˆ©ç”¨äº†ä¸¤ä¸ªtrickï¼Œä¸ºlayer nomalizationå’Œscheduled sampling</p><p><img src="https://pic3.zhimg.com/v2-fec039cb2962195deb890b6680c5395a_r.jpg" alt=""></p><h3 id="GluonTS-AWS"><a href="#GluonTS-AWS" class="headerlink" title="GluonTS(AWS)"></a>GluonTS(AWS)</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/gluonts_all.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/gluonts67.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Memory In Memoryï¼ˆå­¦ä¹ é«˜é˜¶éå¹³ç¨³ç‰¹å¾ä¿¡æ¯ï¼‰&lt;br&gt;
    
    </summary>
    
    
      <category term="å‘¨æŠ¥" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-2-21å‘¨æŠ¥-å•æ²™å˜‰</title>
    <link href="http://arithmeticjia.github.io/2020/02/20/2020-2-21%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/02/20/2020-2-21%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-02-20T12:53:19.000Z</published>
    <updated>2020-02-21T07:20:17.207Z</updated>
    
    <content type="html"><![CDATA[<p>PredRNN++â€¦<br><a id="more"></a></p><h3 id="PredRNN"><a href="#PredRNN" class="headerlink" title="PredRNN++:"></a>PredRNN++:</h3><p>Towards A Resolution of the Deep-in-Time Dilemma in Spatiotemporal Predictive Learning</p><p>æ—¨åœ¨è§£å†³æ—¶ç©ºé¢„æµ‹çš„æ·±å±‚æ¬¡æ—¶é—´å›°å¢ƒ</p><p>ICML2018 Tsinghua</p><h4 id="PredRNN-1"><a href="#PredRNN-1" class="headerlink" title="PredRNN"></a>PredRNN</h4><p>nips2017 Tsinghua</p><p>PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs</p><p>ç”¨ST-LSTMçš„é¢„æµ‹å­¦ä¹ å¾ªç¯ç¥ç»ç½‘ç»œ(spatial + temporal)</p><p>PredRNNåˆ©ç”¨äº†ä¸€ç§åŒé‡è®°å¿†æœºåˆ¶ï¼Œé€šè¿‡ç®€å•çš„é—¨æ§çº§è”ï¼Œå°†æ°´å¹³æ›´æ–°çš„æ—¶é—´è®°å¿†Cä¸å‚ç›´è½¬æ¢çš„ç©ºé—´è®°å¿†Mç»“åˆèµ·æ¥</p><p>å…ˆæ¥å›å¿†ä¸€ä¸‹LSTM</p><p><img src="https://pic2.zhimg.com/v2-810f2d553fa6e6f43854efdc881be8a1_r.jpg" alt=""></p><ol><li>$h_{t-1}$ä¸$X_{t}$åšconcatæ“ä½œï¼Œä¹‹åç»è¿‡sigmoidå½¢æˆ[0, 1]çš„å¿˜è®°é—¨ï¼Œè¾“å…¥é—¨ï¼Œè¾“å‡ºé—¨</li><li>Ct-1é€šè¿‡å¿˜è®°é—¨ </li><li>ht-1ä¸Xtåšconcatæ“ä½œé€šè¿‡tanhæ¿€æ´»å‡½æ•°ï¼Œé€šè¿‡è¾“å…¥é—¨ï¼ˆè¿™é‡Œåœ¨é€šè¿‡è¾“å…¥é—¨ä¹‹å‰ç›¸å½“äºç”Ÿæˆäº†æ­¤æ—¶çš„è¾“å…¥ç”ŸæˆçŠ¶æ€ï¼‰ </li><li>ä»¥ä¸Šé€šè¿‡é—å¿˜é—¨å’Œè¾“å…¥é—¨çš„ä¸¤ä¸ªå‘é‡ç›¸åŠ å°±æ˜¯æœ€åçš„Ctï¼Œä¹Ÿå°±æ˜¯æ­¤æ—¶çš„cell state</li><li>æœ€åï¼Œè¿™ä¸ªcell stateé€šè¿‡å†ä¸€æ¬¡çš„éçº¿æ€§å˜åŒ–tanh æœ€ç»ˆé€šè¿‡è¾“å‡ºé—¨è¾“å‡ºå¾—åˆ°æœ€åçš„ht</li></ol><p>Spatiotemporal memory flow</p><p><img src="https://pic4.zhimg.com/80/v2-7c898aed50f1e1d9aee647e4c273ad33_hd.jpg" alt=""></p><p><img src="https://pic1.zhimg.com/80/v2-f5c836f08237baea9393aefce80d0fd8_hd.jpg" alt=""></p><p>ç¼ºç‚¹:</p><ol><li>å»æ‰æ°´å¹³æ–¹å‘çš„æ—¶é—´æµï¼Œä¼šç‰ºç‰²æ—¶é—´ä¸Šçš„ä¸€è‡´æ€§ï¼Œå› ä¸ºåœ¨åŒä¸€å±‚çš„ä¸åŒæ—¶é—´æ²¡æœ‰æ—¶é—´æµäº†ã€‚ </li><li>è®°å¿†éœ€è¦åœ¨é¥è¿œçš„çŠ¶æ€ä¹‹é—´æµåŠ¨æ›´é•¿çš„è·¯å¾„ï¼Œæ›´å®¹æ˜“é€ æˆæ¢¯åº¦æ¶ˆå¤±ã€‚ æ‰€ä»¥å¼•å…¥äº†ä¸€ä¸ªæ–°çš„building blocksä¸ºST-LSTMã€‚</li></ol><p>Spatiotemporal LSTM</p><p><img src="https://pic4.zhimg.com/v2-f3cd76086384ede22b29e6c8a5f7f45b_r.jpg" alt=""></p><p><img src="https://pic2.zhimg.com/80/v2-fdb1465c439cd9f3eea4ee52bf2b4125_hd.jpg" alt=""></p><p>éœ‡æƒŠï¼ï¼ï¼</p><ul><li>ä¸ŠåŠéƒ¨åˆ†å°±æ˜¯LSTM(Standard Temporal Memory)</li><li>ä¸‹åŠéƒ¨åˆ†ç›¸å½“äºæŠŠcå’Œhä¸€èµ·æ›´æ”¹ä¸ºMï¼ŒMå³æ—¶ç©ºè®°å¿†çŠ¶æ€(Spatiotemporal Memory)</li></ul><p><img src="https://pic2.zhimg.com/80/v2-bbe7560a50ff9d511746fb94562ebd39_hd.jpg" alt=""></p><h4 id="PredRNN-2"><a href="#PredRNN-2" class="headerlink" title="PredRNN++"></a>PredRNN++</h4><ul><li>Stacked ConvLSTMs(nips2015)</li><li>Deep Transition ConvLSTMs</li><li>Pred RNN çº¢çº¿è¡¨ç¤ºç©ºé—´è®°å¿†çš„æ·±åº¦è¿‡æ¸¡è·¯å¾„ï¼Œæ°´å¹³çš„é»‘è‰²ç®­å¤´è¡¨ç¤ºæ—¶é—´è®°å¿†çš„æ›´æ–°æ–¹å‘<br><img src="https://img-blog.csdnimg.cn/20191222210421460.png?#pic_center" alt=""></li></ul><p>Causal LSTM(å› æœé•¿çŸ­æœŸè®°å¿†)<br>é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå°†è·å¾—æ›´å¼ºå¤§çš„å»ºæ¨¡èƒ½åŠ›ï¼Œä»¥å®ç°æ›´å¼ºçš„ç©ºé—´ç›¸å…³æ€§å’ŒçŸ­æœŸåŠ¨æ€</p><p><img src="https://img-blog.csdnimg.cn/20191222205916540.png?#pic_center" alt=""></p><ul><li>æ¯ä¸ªé—¨ä¸æ˜¯ç”±Xå’ŒHå†³å®šï¼Œè€Œæ˜¯ç”±Xå’ŒHä»¥åŠCå†³å®šï¼Œé€šè¿‡è¾“å…¥é—¨ä¹‹å‰çš„çŠ¶æ€ä¹Ÿæ˜¯ç”±ä¸‰è€…å†³å®šçš„</li><li>ä¸¤ä¸ªmemoryç»“æ„ï¼Œå³Cå’ŒMï¼ŒCä¸ºtemporal stateï¼ŒMä¸ºspatial stateï¼Œå› ä¸ºè¾“å…¥Cä¸ºä¸Šä¸€ä¸ªæ—¶åˆ»çš„Cï¼ŒMæ˜¯ä¸Šä¸€å±‚çš„Mï¼Œæ‰€ä»¥è¿™é‡ŒCä¸æ—¶é—´ç»´åº¦æœ‰å…³ï¼ŒMä¸ç©ºé—´ç»´åº¦æœ‰å…³</li><li>Mä½œä¸ºç¬¬äºŒéƒ¨åˆ†çš„stateè¾“å…¥ï¼Œå¹¶ä¸”é€šè¿‡å¿˜è®°é—¨ä¹‹å‰åšäº†ä¸€ä¸ªéçº¿æ€§æ“ä½œtanh</li></ul><p>å¯¹æ¯”ST-LSTMæ¥è¯´ï¼ŒCausal LSTMå¯¹äºMå’ŒHå®šä¹‰æ›´åŠ æ¸…æ™°ï¼Œå¹¶ä¸”ä¸æ˜¯ç®€å•çš„concatï¼Œè€Œæ˜¯é‡‡ç”¨äº†ä¸€ä¸ªé€’å½’æ·±åº¦æ›´æ·±çš„ä¸€ä¸ªçº§è”ç»“æ„æœ€ç»ˆè¾“å‡ºH</p><p>Gradient Highway(é«˜é€Ÿæ¢¯åº¦)</p><p>é€šè¿‡Recurrent Highway Networksçš„æ€æƒ³èƒ½å¤Ÿè¯æ˜é«˜é€Ÿç½‘ç»œèƒ½å¤Ÿæœ‰æ•ˆçš„åœ¨éå¸¸æ·±çš„ç½‘ç»œä¸­ä¼ é€’æ¢¯åº¦ï¼Œç»§è€Œé˜²æ­¢é•¿æ—¶å¯¼è‡´çš„æ¢¯åº¦æ¶ˆå¤±</p><p><img src="https://pic3.zhimg.com/80/v2-3b541bda171d5f4c9299c77326e13702_hd.jpg" alt=""></p><p><img src="https://pic1.zhimg.com/80/v2-cf06c70d336a89700435195a0574b39c_hd.jpg" alt=""></p><p>æ€»ä½“æ¶æ„</p><p><img src="https://pic3.zhimg.com/80/v2-61d4c59ff38010cb9613574bf0290c9a_hd.jpg" alt=""></p><p>GHUè¿æ¥äº†å½“å‰æ—¶åˆ»ä»¥åŠå‰ä¸€ä¸ªæ—¶åˆ»çš„è¾“å…¥ï¼Œå¼•å¯¼çš„ç»“æœå°±æ˜¯æ¢¯åº¦ä¸å†æ˜¯ä¸€è‚¡çº¿ä¼ æ’­äº†ï¼Œè€Œæ˜¯å¯ä»¥ç›´æ¥åœ¨ç¬¬ä¸€å±‚ä¸ç¬¬äºŒå±‚ä¹‹é—´æœ‰ä¸ªé«˜é€Ÿçš„ä¼ æ’­ï¼Œæ¢å¥è¯è®²å°±æ˜¯ä¼ æ’­çš„è·ç¦»ç¼©çŸ­äº†ï¼Œä¹Ÿå°±å˜å¾—æ²¡æœ‰ä¹‹å‰çš„é‚£ä¹ˆâ€™æ·±â€˜äº†ï¼Œå¯ä»¥æœ‰æ•ˆçš„è§£å†³æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜</p><h3 id="Block-Hankel-Tensor-ARIMA-for-Multiple-Short-Time-Series-Forecasting"><a href="#Block-Hankel-Tensor-ARIMA-for-Multiple-Short-Time-Series-Forecasting" class="headerlink" title="Block Hankel Tensor ARIMA for Multiple Short Time Series Forecasting"></a>Block Hankel Tensor ARIMA for Multiple Short Time Series Forecasting</h3><h4 id="Hankel-æ±‰å…‹å°”çŸ©é˜µï¼ˆæ¯ä¸€æ¡é€†å¯¹è§’çº¿ä¸Šçš„å…ƒç´ éƒ½ç›¸ç­‰çš„çŸ©é˜µï¼‰"><a href="#Hankel-æ±‰å…‹å°”çŸ©é˜µï¼ˆæ¯ä¸€æ¡é€†å¯¹è§’çº¿ä¸Šçš„å…ƒç´ éƒ½ç›¸ç­‰çš„çŸ©é˜µï¼‰" class="headerlink" title="Hankel:æ±‰å…‹å°”çŸ©é˜µï¼ˆæ¯ä¸€æ¡é€†å¯¹è§’çº¿ä¸Šçš„å…ƒç´ éƒ½ç›¸ç­‰çš„çŸ©é˜µï¼‰"></a>Hankel:æ±‰å…‹å°”çŸ©é˜µï¼ˆæ¯ä¸€æ¡é€†å¯¹è§’çº¿ä¸Šçš„å…ƒç´ éƒ½ç›¸ç­‰çš„çŸ©é˜µï¼‰</h4><script type="math/tex; mode=display">\begin{bmatrix} 1&  2&  3&  4&  5&  6& 7\\  2&  3&  4&  5&  6&  7& 8\\  3&  4&  5&  6&  7&  8& 9\end{bmatrix}</script><h4 id="Tuckeråˆ†è§£"><a href="#Tuckeråˆ†è§£" class="headerlink" title="Tuckeråˆ†è§£"></a>Tuckeråˆ†è§£</h4><p>è¿™æ˜¯ä¸€ä¸ªä¸‰é˜¶å¼ é‡</p><p><img src="/Users/Arithmetic/Pictures/tensor.png" alt=""></p><p>ç§©ä¸€å¼ é‡ï¼šå¦‚æœä¸€ä¸ªKé˜¶å¼ é‡èƒ½å¤Ÿè¡¨ç¤ºæˆKä¸ªå‘é‡çš„å¤–ç§¯ï¼Œé‚£ä¹ˆè¯¥å¼ é‡ç§°ä¸ºç§©ä¸€å¼ é‡<br>[[3 4],[6 8]]è¿™ä¸ªäºŒé˜¶å¼ é‡å¯ä»¥è¡¨ç¤ºä¸º[1 2]â—‹[3 4]çš„å¤–ç§¯ï¼Œé‚£ä¹ˆè¿™å°±æ˜¯ä¸€ä¸ªäºŒé˜¶ç§©ä¸€å¼ é‡<br><img src="https://www.guanacossj.com/media/articlebodypics/1582207111495.jpg" alt=""></p><p>CPåˆ†è§£<br>CPåˆ†è§£å…¶å®å°±æ˜¯å¤šä¸ªrank-one tensorsçš„å’Œ</p><p><img src="/Users/Arithmetic/Pictures/cp.png" alt=""></p><p>å…¬å¼è¡¨ç¤ºå¦‚ä¸‹ï¼š</p><p><img src="/Users/Arithmetic/Pictures/cp_f.png" alt=""></p><p>tuckeråˆ†è§£</p><p><img src="http://www.xiongfuli.com/assets/img/201606/tucker.png" alt=""></p><p><img src="/Users/Arithmetic/Pictures/tucker_f.png" alt=""></p><p>è¿™é‡ŒA$\in$R$^{I\times P}$,B$\in$R$^{J\times Q}$,C$\in$R$^{K\times R}$æ˜¯å› å­çŸ©é˜µï¼ˆé€šå¸¸æ˜¯æ­£äº¤çš„ï¼‰ï¼Œå¯ä»¥å½“åšæ˜¯æ¯ä¸€ç»´ä¸Šçš„ä¸»è¦æˆåˆ†ã€‚æ ¸å¼ é‡è¡¨ç¤ºæ¯ä¸€ç»´æˆåˆ†ä¹‹é—´çš„è”ç³»<br>å› æ­¤ï¼Œå¯¹äºä¸€ä¸ªä¸‰é˜¶å¼ é‡ï¼Œå¯ä»¥é€šè¿‡tuckeråˆ†è§£ä¸ºä¸‰ä¸ªäºŒé˜¶å› å­çŸ©é˜µå’Œä¸€ä¸ªä¸‰é˜¶æ ¸å‘é‡</p><h4 id="Step1-Block-Hankel-Tensor-via-MDT"><a href="#Step1-Block-Hankel-Tensor-via-MDT" class="headerlink" title="Step1: Block Hankel Tensor via MDT"></a>Step1: Block Hankel Tensor via MDT</h4><p>MDT:multi-way delay embedding transform(å¤šè·¯å»¶è¿Ÿå˜æ¢)<br>ç›®çš„æ˜¯åˆ©ç”¨MDTå°†å¤šä¸ªTSè½¬æ¢æˆä¸€ä¸ªé«˜é˜¶çš„å—Hankelå¼ é‡<br>å‡è®¾æœ‰1000æ¡æ—¶é—´åºåˆ—ï¼Œæ¯æ¡åºåˆ—çš„é•¿åº¦ä¸º40ï¼Œå³ I = 1000ï¼ŒT=40ï¼Œè®¾ç½®å‚æ•°t = 5<br>ç»MDTæ²¿ç€æ—¶é—´ç»´åº¦å˜æ¢åï¼Œå¾—åˆ°ä¸€ä¸ª1000âœ–ï¸5*âœ–ï¸ï¼ˆ40-5+1ï¼‰=1000âœ–ï¸5âœ–ï¸36çš„ä¸‰ç»´å¼ é‡</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PredRNN++â€¦&lt;br&gt;
    
    </summary>
    
    
      <category term="å‘¨æŠ¥" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-2.14å‘¨æŠ¥-å•æ²™å˜‰</title>
    <link href="http://arithmeticjia.github.io/2020/02/13/2020-2-14%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/02/13/2020-2-14%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-02-13T13:27:22.000Z</published>
    <updated>2020-02-14T07:33:34.201Z</updated>
    
    <content type="html"><![CDATA[<p>DA-RNN + Transformer + CW-RNN<br><a id="more"></a></p><h3 id="å½©è›‹"><a href="#å½©è›‹" class="headerlink" title="å½©è›‹"></a>å½©è›‹</h3><ul><li>å†™äº†ä¸€ç¯‡å…³äºä½¿ç”¨ç™¾åº¦Echartsç»˜åˆ¶æ–°å‹å† çŠ¶ç—…æ¯’å…¨å›½åˆ†å¸ƒå›¾çš„åšå®¢</li><li>æ”¶åˆ°æ¥è‡ªçš–å—åŒ»å­¦é™¢å¼‹çŸ¶å±±åŒ»é™¢æ•™è‚²å¤„é‡‘æ¥æ¶¦çš„é‚®ä»¶å¸Œæœ›åˆä½œ</li><li>å…¶å®å°±æ˜¯ä»–ä»¬å›¢é˜Ÿæ”¶é›†äº†ä¸€ç‚¹ç–«æƒ…æ•°æ®æ‰“ç®—å‘ä¸€ç¯‡æ–‡ç« ï¼Œå¸Œæœ›æˆ‘æŒ‰ç…§ä»–ä»¬çš„è¦æ±‚ç»™ä»–ä»¬ç”»ä¸ªå›¾</li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/QQ20200214-0.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/QQ20200214-1.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/echarts.png" alt=""></p><h3 id="DA-RNN"><a href="#DA-RNN" class="headerlink" title="DA-RNN"></a>DA-RNN</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-67-1.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-67-2.jpg" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.256</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.348</span><br></pre></td></tr></table></figure><h3 id="CW-RNN-Clock-Work-RNN"><a href="#CW-RNN-Clock-Work-RNN" class="headerlink" title="CW-RNN(Clock Work RNN)"></a>CW-RNN(Clock Work RNN)</h3><p>ICML2014</p><p>æœ¬æ–‡ä»‹ç»äº†å¯¹æ ‡å‡†RNNä½“ç³»ç»“æ„çš„ä¸€ä¸ªç®€å•è€Œå¼ºå¤§çš„æ”¹è¿›ï¼Œå³æ—¶é’Ÿå·¥ä½œRNNï¼ˆCW-RNNï¼‰ï¼Œå®ƒå°†éšè—å±‚åˆ’åˆ†ä¸ºä¸åŒçš„æ¨¡å—ï¼Œæ¯ä¸ªå¤„ç†ä»¥è‡ªå·±çš„æ—¶é—´ç²’åº¦è¾“å…¥ï¼Œä»…ä»¥æŒ‡å®šçš„æ—¶é’Ÿé€Ÿç‡è¿›è¡Œè®¡ç®—ã€‚<br>CW-RNNæ²¡æœ‰ä½¿æ ‡å‡†RNNæ¨¡å‹æ›´åŠ å¤æ‚ï¼Œè€Œæ˜¯å‡å°‘äº†RNNå‚æ•°çš„æ•°é‡ï¼Œæ˜¾è‘—æé«˜äº†æµ‹è¯•ä»»åŠ¡çš„æ€§èƒ½ï¼ŒåŠ å¿«äº†ç½‘ç»œè¯„ä¼°çš„é€Ÿåº¦</p><p>Input = ($x_{1}$,$x_{2}$,â€¦,$x_{t}$,â€¦)</p><p>Output = ($y_{1}$,$y_{2}$,â€¦,$y_{t}$,â€¦)</p><p><img src="https://www.guanacossj.com/media/articlebodypics/cwrnn.jpg" alt=""></p><ul><li>æŠŠéšå«å±‚èŠ‚ç‚¹åˆ†æˆäº†è‹¥å¹²ä¸ªæ¨¡å—ï¼ˆåœ¨å›¾ä¸­åˆ†æˆäº†3ä¸ªæ¨¡å—ï¼Œæ˜¯ä¸ºäº†æ–¹ä¾¿è¯´æ˜ï¼Œå®é™…ä¸­çš„æ¨¡å—ä¸ªæ•°å¯ä»¥è‡ªå®šä¹‰ï¼‰ï¼Œè€Œä¸”æ¯ä¸ªæ¨¡å—éƒ½åˆ†é…äº†ä¸€ä¸ªæ—¶é’Ÿå‘¨æœŸï¼ˆTiï¼‰ï¼Œä¾¿äºç‹¬ç«‹ç®¡ç†</li><li>éšå«å±‚ä¹‹é—´çš„è¿æ¥ï¼Œåœ¨ä¸€ä¸ªæ¨¡å—å†…éƒ¨æ˜¯å…¨è¿æ¥ï¼Œä½†æ˜¯æ¨¡å—ä¹‹é—´æ˜¯æœ‰æ–¹å‘çš„ã€‚æ¨¡å—ä¹‹é—´çš„è¿æ¥æ˜¯ä»é«˜æ—¶é’Ÿé¢‘ç‡çš„æ¨¡å—æŒ‡å‘ä½æ—¶é’Ÿé¢‘ç‡çš„æ¨¡å—</li><li>æ ‡å‡†RNN<script type="math/tex; mode=display">y_{H}^{(t)} = f_{H}(W_{H}*y^{(t-1)}+W_{I}*x^{(t)})</script><script type="math/tex; mode=display">y_{O}^{(t)} = f_{O}(W_{O}*y_{H}^{(t)})</script></li><li>åªæœ‰å½“$t$ MOD $T_{i}$ = 0æ—¶æ‰ä¼šè¢«æ‰§è¡Œ</li><li>{$T_{1}$,â€¦,$T_{g}$}çš„è®¾ç½®æ˜¯ä»»æ„çš„ï¼Œåœ¨è®ºæ–‡ä¸­ä½¿ç”¨$T_{i}=2^{i-1}$</li></ul><p>$T_{1}=1$,$T_{2}=2$,$T_{3}=4$,$T_{4}=8$,$T_{5}=16$,$T_{6}=32$,$T_{7}=64$</p><ul><li>åˆ†å—<script type="math/tex; mode=display">W_{H} = \begin{pmatrix}\\ W_{H_{1}}\\ .\\ .\\ W_{H_{g}}\end{pmatrix}</script><script type="math/tex; mode=display">W_{I} = \begin{pmatrix}\\ W_{I_{1}}\\ .\\ .\\ W_{I_{g}}\end{pmatrix}</script></li><li>ä¸å‚ä¸è¿ç®—çš„éƒ¨åˆ†ç½®é›¶<script type="math/tex; mode=display">\left\{\begin{matrix}\\ W_{H_{i}},t mod T_{i} = 0\\ 0,otherwise\end{matrix}\right.</script></li><li>å°†$W_{h}$å¼ºåˆ¶è½¬æˆä¸Šä¸‰è§’<script type="math/tex; mode=display">W_{H_{i}}={0_{1},...,0_{i-1},W_{H_{i,i}},...,W_{H_{i,g}}}</script></li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/cwrnn-2.jpg" alt=""></p><p>æˆ‘ä»¬è¦å¤„ç†åºåˆ—ä¸­ç¬¬6ï¼ˆt=6ï¼‰ä¸ªå…ƒç´ çš„æ—¶å€™ï¼Œé€šè¿‡tä¸æ¯ä¸ªæ¨¡å—çš„æ—¶é’Ÿå‘¨æœŸè¿›è¡ŒMODï¼ˆæ±‚ä½™æ•°ï¼‰è®¡ç®—åå¯ä»¥å¾—åˆ°åªæœ‰å‰ä¸¤ä¸ªæ¨¡å—ä¼šå‚ä¸è¿ç®—ã€‚æ‰€ä»¥$W_{h}$å’Œ$W_{x}$çŸ©é˜µé™¤äº†ä¸Šé¢ä¸¤è¡Œä¹‹å¤–ï¼Œå…¶ä»–å…ƒç´ çš„å€¼éƒ½æ˜¯0ã€‚ç»è¿‡è®¡ç®—ä¹‹åï¼Œå¾—åˆ°çš„$h_{t}$ä¹Ÿåªæœ‰å‰ä¸¤ä¸ªæ¨¡å—æœ‰å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æŠŠCW-RNNè¿‡ç¨‹çœ‹æˆæ˜¯é€šè¿‡ä¸€äº›äººå·¥çš„å¹²é¢„ï¼Œé€‰æ‹©ä¸åŒçš„éšå«å±‚èŠ‚ç‚¹è¿›è¡Œå·¥ä½œ</p><p><img src="http://ir.dlut.edu.cn/Uploads/ue/image/20151201/6358457411082925904194442.jpg" alt=""></p><ul><li>ä½æ—¶é’Ÿé€Ÿç‡æ¨¡å—å¤„ç†ã€ä¿ç•™å’Œè¾“å‡ºä»è¾“å…¥ä¸­è·å¾—çš„é•¿æœŸä¿¡æ¯åºåˆ—ï¼Œ</li><li>é«˜æ—¶é’Ÿé€Ÿç‡æ¨¡å—åˆ™ä¾§é‡äºæœ¬åœ°çš„é«˜é¢‘ä¿¡æ¯</li></ul><p>ä½œè€…å¯¹æ¯”äº†ä¼ ç»ŸRNNã€LSTMã€CW-RNNï¼Œåœ¨å–å±€éƒ¨å›¾çš„æ—¶å€™å¯ä»¥è§‚å¯Ÿåˆ°ï¼ŒLSTMçš„å›å½’æ•ˆæœç›¸å¯¹å¹³æ»‘ï¼Œè€ŒCW-RNNå¹¶æ²¡æœ‰è¿™ç§ç¼ºé™·<br><img src="https://www.guanacossj.com/media/articlebodypics/comparecwrnn.jpg" alt=""></p><p>æˆ‘å¤ç°äº†ä¸‹è‚¡ç¥¨æ•°æ®é›†ä¸Šçš„æ•ˆæœï¼Œå®éªŒè¿˜æ²¡å®Œå…¨å®Œæˆ<br><img src="https://www.guanacossj.com/media/articlebodypics/cwrnn-stock.jpg" alt=""></p><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/multi_head_net.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/transf-67-1.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/transf-67-2.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DA-RNN + Transformer + CW-RNN&lt;br&gt;
    
    </summary>
    
    
      <category term="å‘¨æŠ¥" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-1-17å‘¨æŠ¥-å•æ²™å˜‰</title>
    <link href="http://arithmeticjia.github.io/2020/01/17/2020-1-17%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/01/17/2020-1-17%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-01-17T08:38:22.000Z</published>
    <updated>2020-01-17T09:58:07.962Z</updated>
    
    <content type="html"><![CDATA[<p>Django+uwsgi+Nginx<br><a id="more"></a></p><h3 id="WSGI"><a href="#WSGI" class="headerlink" title="WSGI"></a>WSGI</h3><p>WSGI Web Server Gateway Interface</p><p><img src="https://pic1.zhimg.com/80/v2-6c4572c783816364f2569af961814430_hd.jpg" alt=""></p><p>WSGIæ˜¯ä¸€ç§é€šä¿¡åè®®ï¼ŒWSGI ä¸æ˜¯æ¡†æ¶ï¼Œä¹Ÿä¸æ˜¯ä¸€ä¸ªæ¨¡å—ï¼Œè€Œæ˜¯ä»‹äº Webåº”ç”¨ç¨‹åºï¼ˆWebæ¡†æ¶ï¼‰ä¸ Web Server ä¹‹é—´äº¤äº’çš„ä¸€ç§è§„èŒƒã€‚</p><h3 id="uwsgi"><a href="#uwsgi" class="headerlink" title="uwsgi"></a>uwsgi</h3><ul><li>äºŒè¿›åˆ¶åè®®ï¼Œå¯ä»¥æºå¸¦ä»»ä½•ç±»å‹çš„æ•°æ®ã€‚ä¸€ä¸ªuwsgiåˆ†ç»„çš„å¤´4ä¸ªå­—èŠ‚æè¿°äº†è¿™ä¸ªåˆ†ç»„åŒ…å«çš„æ•°æ®ç±»å‹ã€‚</li><li>uwsgiæ˜¯ä¸€ç§çº¿è·¯åè®®è€Œä¸æ˜¯é€šä¿¡åè®®ï¼Œåœ¨æ­¤å¸¸ç”¨äºåœ¨uWSGIæœåŠ¡å™¨ä¸å…¶ä»–ç½‘ç»œæœåŠ¡å™¨çš„æ•°æ®é€šä¿¡ã€‚</li></ul><h3 id="uWSGI"><a href="#uWSGI" class="headerlink" title="uWSGI"></a>uWSGI</h3><p>uWSGIæ˜¯å®ç°äº†uwsgiå’ŒWSGIä¸¤ç§åè®®çš„WebæœåŠ¡å™¨ï¼Œä½¿ç”¨cè¯­è¨€å¼€å‘ã€‚</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> uwsgi</span><br></pre></td></tr></table></figure><ul><li>ä¸¤çº§ç»“æ„ åœ¨è¿™ç§ç»“æ„é‡Œï¼ŒuWSGIä½œä¸ºæœåŠ¡å™¨ï¼Œå®ƒç”¨åˆ°äº†HTTPåè®®ä»¥åŠwsgiåè®®ï¼Œflaskåº”ç”¨ä½œä¸ºapplicationï¼Œå®ç°äº†wsgiåè®®ã€‚å½“æœ‰å®¢æˆ·ç«¯å‘æ¥è¯·æ±‚ï¼ŒuWSGIæ¥å—è¯·æ±‚ï¼Œè°ƒç”¨flask appå¾—åˆ°ç›¸åº”ï¼Œä¹‹åç›¸åº”ç»™å®¢æˆ·ç«¯ã€‚ è¿™é‡Œè¯´ä¸€ç‚¹ï¼Œé€šå¸¸æ¥è¯´ï¼ŒFlaskç­‰webæ¡†æ¶ä¼šè‡ªå·±é™„å¸¦ä¸€ä¸ªwsgiæœåŠ¡å™¨(è¿™å°±æ˜¯flaskåº”ç”¨å¯ä»¥ç›´æ¥å¯åŠ¨çš„åŸå› )ï¼Œä½†æ˜¯è¿™åªæ˜¯åœ¨å¼€å‘é˜¶æ®µç”¨åˆ°çš„ï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒæ˜¯ä¸å¤Ÿç”¨çš„ï¼Œæ‰€ä»¥ç”¨åˆ°äº†uwsgiè¿™ä¸ªæ€§èƒ½é«˜çš„wsgiæœåŠ¡å™¨ã€‚</li><li>ä¸‰çº§ç»“æ„ åœ¨è¿™ç§ç»“æ„é‡Œï¼ŒuWSGIä½œä¸ºä¸­é—´ä»¶ï¼Œå®ƒç”¨åˆ°äº†uwsgiåè®®(ä¸nginxé€šä¿¡)ï¼Œwsgiåè®®(è°ƒç”¨Flask app)ã€‚</li><li>æé«˜web serveræ€§èƒ½(uWSGIå¤„ç†é™æ€èµ„æºä¸å¦‚nginxï¼›nginxä¼šåœ¨æ”¶åˆ°ä¸€ä¸ªå®Œæ•´çš„httpè¯·æ±‚åå†è½¬å‘ç»™wWSGI)ã€‚</li><li>nginxå¯ä»¥åšè´Ÿè½½å‡è¡¡(å‰ææ˜¯æœ‰å¤šä¸ªæœåŠ¡å™¨)ï¼Œä¿æŠ¤äº†å®é™…çš„webæœåŠ¡å™¨(å®¢æˆ·ç«¯æ˜¯å’Œnginxäº¤äº’è€Œä¸æ˜¯uWSGI)ã€‚</li></ul><h3 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a>Nginx</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install nginx</span><br></pre></td></tr></table></figure><p>Nginxæ˜¯ä¸€æ¬¾è½»é‡çº§çš„WebæœåŠ¡å™¨ã€åå‘ä»£ç†æœåŠ¡å™¨ï¼Œç”±äºå®ƒçš„å†…å­˜å ç”¨å°‘ï¼Œå¯åŠ¨æå¿«ï¼Œé«˜å¹¶å‘èƒ½åŠ›å¼ºï¼Œåœ¨äº’è”ç½‘é¡¹ç›®ä¸­å¹¿æ³›åº”ç”¨ã€‚</p><p><img src="https://pic2.zhimg.com/80/v2-4787a512240b238ebf928cd0651e1d99_hd.jpg" alt=""></p><h3 id="Django-uWSGI-Nginx"><a href="#Django-uWSGI-Nginx" class="headerlink" title="Django + uWSGI + Nginx"></a>Django + uWSGI + Nginx</h3><p><img src="https://img-blog.csdnimg.cn/20181216174304355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d5bWFpc3ls,size_16,color_FFFFFF,t_70" alt=""></p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">user</span> www-data;</span><br><span class="line"><span class="attribute">worker_processes</span> auto;</span><br><span class="line"><span class="attribute">pid</span> /run/nginx.pid;</span><br><span class="line"><span class="attribute">include</span> /etc/nginx/modules-enabled/<span class="regexp">*.conf</span>;</span><br><span class="line"></span><br><span class="line"><span class="section">events</span> &#123;</span><br><span class="line"><span class="attribute">worker_connections</span> <span class="number">768</span>;</span><br><span class="line"><span class="comment"># multi_accept on;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">http</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Basic Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="section">server</span> &#123;   <span class="comment"># è¿™ä¸ªserveræ ‡è¯†æˆ‘è¦é…ç½®äº†</span></span><br><span class="line"><span class="attribute">listen</span> <span class="number">80</span>;  <span class="comment"># æˆ‘è¦ç›‘å¬é‚£ä¸ªç«¯å£</span></span><br><span class="line"><span class="attribute">server_name</span> <span class="number">118.25.79.249</span> ;  <span class="comment"># ä½ è®¿é—®çš„è·¯å¾„å‰é¢çš„urlåç§°</span></span><br><span class="line"><span class="attribute">charset</span>  utf-<span class="number">8</span>; <span class="comment"># Nginxç¼–ç </span></span><br><span class="line"><span class="attribute">gzip</span> <span class="literal">on</span>;  <span class="comment"># å¯ç”¨å‹ç¼©,è¿™ä¸ªçš„ä½œç”¨å°±æ˜¯ç»™ç”¨æˆ·ä¸€ä¸ªç½‘é¡µ,æ¯”å¦‚3Må‹ç¼©å1Mè¿™æ ·ä¼ è¾“é€Ÿåº¦å°±ä¼šæé«˜å¾ˆå¤š</span></span><br><span class="line"><span class="attribute">gzip_types</span> text/plain application/x-javascript text/css text/javascript application/x-httpd-php application/json text/json image/jpeg image/gif image/png application/octet-stream;  <span class="comment"># æ”¯æŒå‹ç¼©çš„ç±»å‹</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">error_page</span>  <span class="number">404</span>           /<span class="number">404</span>.html;  <span class="comment"># é”™è¯¯é¡µé¢</span></span><br><span class="line"><span class="attribute">error_page</span>   <span class="number">500</span> <span class="number">502</span> <span class="number">503</span> <span class="number">504</span>  /50x.html;  <span class="comment"># é”™è¯¯é¡µé¢</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># æŒ‡å®šé¡¹ç›®è·¯å¾„uwsgi</span></span><br><span class="line"><span class="attribute">location</span> / &#123;        <span class="comment"># è¿™ä¸ªlocationå°±å’Œå’±ä»¬Djangoçš„url(r'^admin/', admin.site.urls),</span></span><br><span class="line"><span class="attribute">include</span> uwsgi_params;  <span class="comment"># å¯¼å…¥ä¸€ä¸ªNginxæ¨¡å—ä»–æ˜¯ç”¨æ¥å’ŒuWSGIè¿›è¡Œé€šè®¯çš„</span></span><br><span class="line"><span class="attribute">uwsgi_connect_timeout</span> <span class="number">30</span>;  <span class="comment"># è®¾ç½®è¿æ¥uWSGIè¶…æ—¶æ—¶é—´</span></span><br><span class="line"><span class="attribute">uwsgi_pass</span>  <span class="number">127.0.0.1:8000</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># æŒ‡å®šé™æ€æ–‡ä»¶è·¯å¾„</span></span><br><span class="line"><span class="attribute">location</span> /static/ &#123;</span><br><span class="line"><span class="attribute">alias</span>  /home/mysite/static/;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="attribute">sendfile</span> <span class="literal">on</span>;</span><br><span class="line"><span class="attribute">tcp_nopush</span> <span class="literal">on</span>;</span><br><span class="line"><span class="attribute">tcp_nodelay</span> <span class="literal">on</span>;</span><br><span class="line"><span class="attribute">keepalive_timeout</span> <span class="number">65</span>;</span><br><span class="line"><span class="attribute">types_hash_max_size</span> <span class="number">2048</span>;</span><br><span class="line"><span class="comment"># server_tokens off;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># server_names_hash_bucket_size 64;</span></span><br><span class="line"><span class="comment"># server_name_in_redirect off;</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">include</span> /etc/nginx/mime.types;</span><br><span class="line"><span class="attribute">default_type</span> application/octet-stream;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># SSL Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">ssl_protocols</span> TLSv1 TLSv1.<span class="number">1</span> TLSv1.<span class="number">2</span>; <span class="comment"># Dropping SSLv3, ref: POODLE</span></span><br><span class="line"><span class="attribute">ssl_prefer_server_ciphers</span> <span class="literal">on</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Logging Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">access_log</span> /var/log/nginx/access.log;</span><br><span class="line"><span class="attribute">error_log</span> /var/log/nginx/error.log;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Gzip Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">gzip</span> <span class="literal">on</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># gzip_vary on;</span></span><br><span class="line"><span class="comment"># gzip_proxied any;</span></span><br><span class="line"><span class="comment"># gzip_comp_level 6;</span></span><br><span class="line"><span class="comment"># gzip_buffers 16 8k;</span></span><br><span class="line"><span class="comment"># gzip_http_version 1.1;</span></span><br><span class="line"><span class="comment"># gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Virtual Host Configs</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">include</span> /etc/nginx/conf.d/<span class="regexp">*.conf</span>;</span><br><span class="line"><span class="attribute">include</span> /etc/nginx/sites-enabled/*;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#mail &#123;</span></span><br><span class="line"><span class="comment">## See sample authentication script at:</span></span><br><span class="line"><span class="comment">## http://wiki.nginx.org/ImapAuthenticateWithApachePhpScript</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">## auth_http localhost/auth.php;</span></span><br><span class="line"><span class="comment">## pop3_capabilities "TOP" "USER";</span></span><br><span class="line"><span class="comment">## imap_capabilities "IMAP4rev1" "UIDPLUS";</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#server &#123;</span></span><br><span class="line"><span class="comment">#listen     localhost:110;</span></span><br><span class="line"><span class="comment">#protocol   pop3;</span></span><br><span class="line"><span class="comment">#proxy      on;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#server &#123;</span></span><br><span class="line"><span class="comment">#listen     localhost:143;</span></span><br><span class="line"><span class="comment">#protocol   imap;</span></span><br><span class="line"><span class="comment">#proxy      on;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[uwsgi] </span><br><span class="line">chdir = /home/mysite</span><br><span class="line">module = mysite.wsgi:application</span><br><span class="line">socket = 127.0.0.1:8000</span><br><span class="line">master = true </span><br><span class="line">processes = 1</span><br><span class="line">threads = 2</span><br><span class="line">max-requests = 6000</span><br><span class="line">chmod-socket = 666</span><br><span class="line">buffer-size = 65535</span><br><span class="line">logto = /var/log/mysite.log</span><br><span class="line">async</span><br><span class="line">ugreen =''</span><br><span class="line">http-timeout = 300</span><br><span class="line"><span class="comment">#plugins=python</span></span><br></pre></td></tr></table></figure><h3 id="Activemq"><a href="#Activemq" class="headerlink" title="Activemq"></a>Activemq</h3><p>ActiveMQ æ˜¯ Apache å‡ºå“ï¼Œæœ€æµè¡Œçš„ï¼Œèƒ½åŠ›å¼ºåŠ²çš„å¼€æºæ¶ˆæ¯æ€»çº¿ã€‚ActiveMQ æ˜¯ä¸€ä¸ªå®Œå…¨æ”¯æŒ JMS1.1 å’Œ J2EE 1.4 è§„èŒƒçš„ JMS Provider å®ç°ã€‚</p><h4 id="queue"><a href="#queue" class="headerlink" title="queue"></a>queue</h4><p><img src="https://pic4.zhimg.com/80/v2-b7edcfa850af9627bed67ef9e89f8d3f_hd.jpg" alt=""></p><h4 id="topic"><a href="#topic" class="headerlink" title="topic"></a>topic</h4><p><img src="https://pic2.zhimg.com/80/v2-b1874d392a6119fb4e497425dcc58609_hd.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Django+uwsgi+Nginx&lt;br&gt;
    
    </summary>
    
    
      <category term="å‘¨æŠ¥" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-1-10å‘¨æŠ¥-å•æ²™å˜‰</title>
    <link href="http://arithmeticjia.github.io/2020/01/07/2020-1-10%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/01/07/2020-1-10%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-01-07T06:07:24.000Z</published>
    <updated>2020-01-10T10:47:51.445Z</updated>
    
    <content type="html"><![CDATA[<p>â€œAll you need is attentionâ€<br><a id="more"></a></p><h3 id="LSTM-Attention"><a href="#LSTM-Attention" class="headerlink" title="LSTM + Attention"></a>LSTM + Attention</h3><p>FEED-FORWARD NETWORKS WITH ATTENTION CAN SOLVE SOME LONG-TERM MEMORY PROBLEMS</p><h4 id="FEED-FORWARD-ATTENTION"><a href="#FEED-FORWARD-ATTENTION" class="headerlink" title="FEED-FORWARD ATTENTION"></a>FEED-FORWARD ATTENTION</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/FEED-FORWARD-ATTENTION.jpg" alt=""></p><script type="math/tex; mode=display">e_{t} = a(h_{t})</script><script type="math/tex; mode=display">\alpha_{t} = \frac{exp(e_{t})}{\sum_{k=1}^{T}exp(e_{k})}</script><script type="math/tex; mode=display">c = \sum_{t=1}^{T}\alpha_{t}h_{t}</script><h4 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/all-lstmattention.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/test-lstmattention.png" alt=""></p><h3 id="Seq2Seq-Attention"><a href="#Seq2Seq-Attention" class="headerlink" title="Seq2Seq + Attention"></a>Seq2Seq + Attention</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/all-seq2seqattention.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/test-seq2seqattention.png" alt=""></p><h3 id="DA-RNN"><a href="#DA-RNN" class="headerlink" title="DA-RNN"></a>DA-RNN</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-gru.jpg" alt=""></p><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h4><p>é¡¾åæ€ä¹‰ï¼ŒæŒ‡çš„ä¸æ˜¯Targetå’ŒSourceä¹‹é—´çš„Attentionæœºåˆ¶ï¼Œè€Œæ˜¯Sourceå†…éƒ¨å…ƒç´ ä¹‹é—´æˆ–è€…Targetå†…éƒ¨å…ƒç´ ä¹‹é—´å‘ç”Ÿçš„Attentionæœºåˆ¶ï¼Œä¹Ÿå¯ä»¥ç†è§£ä¸ºTarget=Sourceè¿™ç§ç‰¹æ®Šæƒ…å†µä¸‹çš„æ³¨æ„åŠ›è®¡ç®—æœºåˆ¶</p><p>ç»™å‡ºä¿¡æ¯è¾“å…¥ï¼šç”¨X = [x1, Â· Â· Â· , xN ]è¡¨ç¤ºN ä¸ªè¾“å…¥ä¿¡æ¯ï¼›é€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°ä¸ºæŸ¥è¯¢å‘é‡åºåˆ—ï¼Œé”®å‘é‡åºåˆ—å’Œå€¼å‘é‡åºåˆ—ï¼Œå…¶ä¸­$W^{Q}$,$W^{K}$,$W^{V}$æ˜¯æˆ‘ä»¬æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å­¦ä¹ åˆ°çš„åˆé€‚çš„å‚æ•°</p><script type="math/tex; mode=display">Q = W^{Q}X</script><script type="math/tex; mode=display">K = W^{K}X</script><script type="math/tex; mode=display">V = W^{V}X</script><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix}*[v^{T}_{1},v^{T}_{2},...,v^{T}_{n}])*\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix} = softmax(QK^{T})V</script><p><img src="https://pic2.zhimg.com/v2-07c4c02a9bdecb23d9664992f142eaa5_r.jpg" alt=""></p><p>Sourceä¸­çš„æ„æˆå…ƒç´ æƒ³è±¡æˆæ˜¯ç”±ä¸€ç³»åˆ—çš„<Key,Value>æ•°æ®å¯¹æ„æˆ<br>Targetä¸­çš„æŸä¸ªå…ƒç´ Query<br>(åœ¨Seq2Se2ä¸­ï¼ŒQæ˜¯Decoderçš„éšè—æ€ï¼ŒKå’ŒVéƒ½æ˜¯Encoderçš„éšè—æ€)</p><ul><li>1ã€æ ¹æ®Queryå’ŒKeyè®¡ç®—æƒé‡ç³»æ•°ï¼Œå¸¸ç”¨çš„ç›¸ä¼¼åº¦å‡½æ•°æœ‰ç‚¹ç§¯ï¼Œæ‹¼æ¥ï¼Œæ„ŸçŸ¥æœºç­‰</li><li>2ã€ä½¿ç”¨softmaxå‡½æ•°å¯¹è¿™äº›æƒé‡è¿›è¡Œå½’ä¸€åŒ–</li><li>3ã€æ ¹æ®æƒé‡ç³»æ•°å¯¹Valueè¿›è¡ŒåŠ æƒæ±‚å’Œå¾—åˆ°attention</li></ul><h4 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h4><p>é˜²æ­¢Qå’ŒKç‚¹ä¹˜ç§¯ç»“æœè¿‡å¤§ï¼Œä¼šé™¤ä»¥ä¸€ä¸ªå°ºåº¦æ ‡åº¦ </p><script type="math/tex; mode=display">Attention(Q,K,V) = sofrmax(\frac{QK^{T}}{\sqrt{d_{k}}})V</script><h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><ul><li>$Q$ï¼Œ$K$ï¼Œ$V$é¦–å…ˆè¿›è¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼Œç„¶åè¾“å…¥åˆ°æ”¾ç¼©ç‚¹ç§¯attention</li><li>æ¯æ¬¡$Q$ï¼Œ$K$ï¼Œ$V$è¿›è¡Œçº¿æ€§å˜æ¢çš„å‚æ•°$W$æ˜¯ä¸ä¸€æ ·çš„</li><li>é€šè¿‡$h$ä¸ªä¸åŒçš„çº¿æ€§å˜æ¢å¯¹$Q$ï¼Œ$K$ï¼Œ$V$è¿›è¡ŒæŠ•å½±ï¼Œæœ€åå°†ä¸åŒçš„attentionç»“æœæ‹¼æ¥èµ·æ¥</li></ul><script type="math/tex; mode=display">Multihead(Q,K,V) = Concat(head_{1},...,head_{h})W^{O}</script><script type="math/tex; mode=display">head_{i} = Attention(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})</script><h4 id="Experiment-1"><a href="#Experiment-1" class="headerlink" title="Experiment"></a>Experiment</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/all-transformer.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/test-transformer.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;â€œAll you need is attentionâ€&lt;br&gt;
    
    </summary>
    
    
      <category term="å‘¨æŠ¥" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>è¯´è¯´LSTM</title>
    <link href="http://arithmeticjia.github.io/2019/12/29/%E8%AF%B4%E8%AF%B4LSTM/"/>
    <id>http://arithmeticjia.github.io/2019/12/29/%E8%AF%B4%E8%AF%B4LSTM/</id>
    <published>2019-12-29T15:22:35.000Z</published>
    <updated>2019-12-29T15:34:02.931Z</updated>
    
    <content type="html"><![CDATA[<p>Long Short Term Memory<br><a id="more"></a></p><h4 id="ä»RNNå¼€å§‹"><a href="#ä»RNNå¼€å§‹" class="headerlink" title="ä»RNNå¼€å§‹"></a>ä»RNNå¼€å§‹</h4><p>RNN(Recurrent Neural Network)æ˜¯ä¸€ç±»ç”¨äºå¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œæ“…é•¿å¯¹åºåˆ—æ•°æ®è¿›è¡Œå»ºæ¨¡å¤„ç†ã€‚LSTM(Long Short-Term Memory) åœ¨ä¼ ç»Ÿçš„ RNN çš„åŸºç¡€ä¸Šå¢åŠ äº†çŠ¶æ€$c$ï¼Œç§°ä¸ºè®°å¿†å•å…ƒæ€ (cell state)ï¼Œç”¨ä»¥å–ä»£ä¼ ç»Ÿçš„éšå«ç¥ç»å…ƒèŠ‚ç‚¹ã€‚å®ƒè´Ÿè´£æŠŠè®°å¿†ä¿¡æ¯ä»åºåˆ—çš„åˆå§‹ä½ç½®ï¼Œä¼ é€’åˆ°åºåˆ—çš„æœ«ç«¯ã€‚</p><h4 id="LSTMçš„ç»„æˆ"><a href="#LSTMçš„ç»„æˆ" class="headerlink" title="LSTMçš„ç»„æˆ"></a>LSTMçš„ç»„æˆ</h4><p>åœ¨$t$æ—¶åˆ»ï¼Œå½“å‰ç¥ç»å…ƒçš„è¾“å…¥æœ‰ä¸‰ä¸ªï¼šå½“å‰æ—¶åˆ»è¾“å…¥å€¼$x_{t}$ã€å‰ä¸€æ—¶åˆ»è¾“å‡ºå€¼$s_{t-1}$,å’Œå‰ä¸€æ—¶åˆ»çš„è®°å¿†å•å…ƒçŠ¶æ€$c_{t-1}$, è¾“å‡ºæœ‰ä¸¤ä¸ªï¼Œå½“å‰æ—¶åˆ»LSTMçš„è¾“å‡ºå€¼$s_{t}$å’Œå½“å‰æ—¶åˆ»çš„è®°å¿†å•å…ƒçŠ¶æ€$c_{t}$ã€‚<br>LSTMé€šè¿‡ä¸‰ä¸ªé—¨æ§å¼€å…³ä¼ é€’è®°å¿†çŠ¶æ€ã€‚</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Long Short Term Memory&lt;br&gt;
    
    </summary>
    
    
      <category term="LSTM" scheme="http://arithmeticjia.github.io/categories/LSTM/"/>
    
    
      <category term="lstm" scheme="http://arithmeticjia.github.io/tags/lstm/"/>
    
      <category term="deeplearning" scheme="http://arithmeticjia.github.io/tags/deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode78Pascal-Triangle-2-Java</title>
    <link href="http://arithmeticjia.github.io/2019/12/29/Leetcode78Pascal-Triangle-2-Java/"/>
    <id>http://arithmeticjia.github.io/2019/12/29/Leetcode78Pascal-Triangle-2-Java/</id>
    <published>2019-12-29T03:54:18.000Z</published>
    <updated>2019-12-29T03:58:39.143Z</updated>
    
    <content type="html"><![CDATA[<p>Java æ‰¾è§„å¾‹æ³•<br><a id="more"></a><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">getRow</span><span class="params">(<span class="keyword">int</span> rowIndex)</span> </span>&#123;</span><br><span class="line">        List&lt;Integer&gt; res = <span class="keyword">new</span> ArrayList&lt;Integer&gt;();</span><br><span class="line">        <span class="keyword">long</span> k = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(rowIndex &gt;= <span class="number">0</span>)</span><br><span class="line">            res.add(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= rowIndex + <span class="number">1</span>; i++) &#123;</span><br><span class="line">            k = k * (rowIndex + <span class="number">2</span> - i) / (i-<span class="number">1</span>);</span><br><span class="line">            res.add((<span class="keyword">int</span>)k);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>è¿™é‡Œç”¨åˆ°äº†æ¨è¾‰ä¸‰è§’çš„è§„å¾‹ï¼Œç¬¬nè¡Œmä¸ªæ•°ç­‰äº</p><p>è­¬å¦‚ç¬¬ä¸‰è¡Œç¬¬äºŒä¸ªæ•°</p><script type="math/tex; mode=display">C_{3-1}^{2-1} = C_{2}^{1} = 2</script><p>è­¬å¦‚ç¬¬å››è¡Œç¬¬ä¸‰ä¸ªæ•°</p><script type="math/tex; mode=display">C_{4-1}^{3-1} = C_{3}^{2} = 3</script><p>é‚£è¿™ä¸ªå¯¹æˆ‘ä»¬çš„ç®—æ³•æœ‰å•¥å¸®åŠ©å‘¢ï¼Ÿ</p><p>ä¸¾ä¸ªæ —å­ï¼Œçœ‹ç¬¬å››è¡Œ</p><p>åº”è¯¥æ˜¯1 3 3 1</p><p>åœ¨æœ¬é¢˜ä¸­æ˜¯1 4 6 4 1</p><p>$C_{5-1}^{1-1} = C_{4}^{0} = 1$ï¼Œ$C_{5-1}^{2-1} = C_{4}^{1} = 4$ï¼Œ$C_{5-1}^{3-1} = C_{4}^{2} = 6$ï¼Œ$C_{5-1}^{4-1} = C_{4}^{3} = 4$ï¼Œ$C_{5-1}^{5-1} = C_{4}^{4} = 1$</p><p>æ‰¾è§„å¾‹å¦‚ä¸‹ï¼š</p><p>ç¬¬ä¸€ä¸ªæ•°ï¼š<script type="math/tex">C_{5-1}^{1-1} = C_{4}^{0} = 1</script></p><p>ç¬¬äºŒä¸ªæ•°ï¼š<script type="math/tex">C_{5-1}^{2-1} = C_{4}^{1} = C_{5-1}^{1-1} * \frac{(rowIndex-2+2)}{2-1}</script></p><p>ç¬¬nè¡Œmä¸ªæ•°ï¼šç¬¬m-1ä¸ªæ•° Ã— $ \frac{(rowIndex-m+2)}{m-1} $ï¼Œç¬¬nè¡Œç¬¬ä¸€ä¸ªæ•°æ°¸è¿œæ˜¯1</p><p>æ™šå®‰~~~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Java æ‰¾è§„å¾‹æ³•&lt;br&gt;
    
    </summary>
    
    
      <category term="Leetcode" scheme="http://arithmeticjia.github.io/categories/Leetcode/"/>
    
    
      <category term="leetcode" scheme="http://arithmeticjia.github.io/tags/leetcode/"/>
    
      <category term="java" scheme="http://arithmeticjia.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode[78]Pascal&#39;s Triangle II</title>
    <link href="http://arithmeticjia.github.io/2019/12/28/Leetcode78Pascal-Triangle-2/"/>
    <id>http://arithmeticjia.github.io/2019/12/28/Leetcode78Pascal-Triangle-2/</id>
    <published>2019-12-28T14:05:49.000Z</published>
    <updated>2019-12-28T14:07:35.863Z</updated>
    
    <content type="html"><![CDATA[<p>python3 æœ€ä¼˜é›…è§£æ³•<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getRow</span><span class="params">(self, rowIndex)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type rowIndex: int</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        res = [<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, rowIndex + <span class="number">1</span>):</span><br><span class="line">            res.insert(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">            <span class="comment"># jå¾ªç¯æ¯æ¬¡ç®—å‡ºr[0]...r[j-1]ï¼Œå†åŠ ä¸Šæœ€åä¸€ä¸ªæ°¸è¿œå­˜åœ¨çš„1ï¼Œæ­£å¥½æ˜¯rowIndex+1ä¸ªæ•°</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">                res[j] = res[j] + res[j + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;python3 æœ€ä¼˜é›…è§£æ³•&lt;br&gt;
    
    </summary>
    
    
      <category term="Leetcode" scheme="http://arithmeticjia.github.io/categories/Leetcode/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="leetcode" scheme="http://arithmeticjia.github.io/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>2019-12-27å‘¨æŠ¥-å•æ²™å˜‰</title>
    <link href="http://arithmeticjia.github.io/2019/12/27/2019-12-27%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2019/12/27/2019-12-27%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2019-12-27T07:40:46.000Z</published>
    <updated>2020-01-10T09:53:38.616Z</updated>
    
    <content type="html"><![CDATA[<p>RNN -&gt; LSTM -&gt; GRU -&gt; Seq2Seq -&gt; Attention -&gt; Transformer<br><a id="more"></a></p><h3 id="Encoder-Decoder-Seq2Seq"><a href="#Encoder-Decoder-Seq2Seq" class="headerlink" title="Encoder-Decoder(Seq2Seq)"></a>Encoder-Decoder(Seq2Seq)</h3><p><img src="https://pic4.zhimg.com/80/v2-77e8a977fc3d43bec8b05633dc52ff9f_hd.jpg" alt=""></p><ul><li>Encoder-Decoderç»“æ„å…ˆå°†è¾“å…¥æ•°æ®ç¼–ç æˆä¸€ä¸ªä¸Šä¸‹æ–‡å‘é‡$c$</li><li>æŠŠEncoderçš„æœ€åä¸€ä¸ªéšçŠ¶æ€èµ‹å€¼ç»™$c$,è¿˜å¯ä»¥å¯¹æœ€åçš„éšçŠ¶æ€åšä¸€ä¸ªå˜æ¢å¾—åˆ°$c$ï¼Œä¹Ÿå¯ä»¥å¯¹æ‰€æœ‰çš„éšçŠ¶æ€åšå˜æ¢</li><li>æ‹¿åˆ°cä¹‹åï¼Œå°±ç”¨å¦ä¸€ä¸ªRNNç½‘ç»œå¯¹å…¶è¿›è¡Œè§£ç (Decoder),å°†cå½“åšä¹‹å‰çš„åˆå§‹çŠ¶æ€$h_{0}$è¾“å…¥åˆ°Decoderä¸­</li><li>è¿˜æœ‰ä¸€ç§åšæ³•æ˜¯å°†$c$å½“åšæ¯ä¸€æ­¥çš„è¾“å…¥</li></ul><p><img src="https://pic4.zhimg.com/80/v2-e0fbb46d897400a384873fc100c442db_hd.jpg" alt=""></p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><ul><li>åœ¨Encoder-Decoderç»“æ„ä¸­ï¼ŒEncoderæŠŠæ‰€æœ‰çš„è¾“å…¥åºåˆ—éƒ½ç¼–ç æˆä¸€ä¸ªç»Ÿä¸€çš„è¯­ä¹‰ç‰¹å¾$c$å†è§£ç ï¼Œå› æ­¤ï¼Œ$c$ä¸­å¿…é¡»åŒ…å«åŸå§‹åºåˆ—ä¸­çš„æ‰€æœ‰ä¿¡æ¯ï¼Œå®ƒçš„é•¿åº¦å°±æˆäº†é™åˆ¶æ¨¡å‹æ€§èƒ½çš„ç“¶é¢ˆ</li><li>Attentionæœºåˆ¶é€šè¿‡åœ¨æ¯ä¸ªæ—¶é—´è¾“å…¥ä¸åŒçš„$c$æ¥è§£å†³è¿™ä¸ªé—®é¢˜</li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/v2-8da16d429d33b0f2705e47af98e66579_hd_gaitubao_525x551_gaitubao_345x362.jpg" alt=""></p><ul><li>æ¯ä¸€ä¸ª$c$ä¼šè‡ªåŠ¨å»é€‰å–ä¸å½“å‰æ‰€è¦è¾“å‡ºçš„$y$æœ€åˆé€‚çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬ç”¨$\alpha_{ij}$è¡¡é‡Encoderä¸­ç¬¬$j$é˜¶æ®µçš„$h_{j}$å’Œè§£ç æ—¶ç¬¬$i$é˜¶æ®µçš„ç›¸å…³æ€§ï¼Œæœ€ç»ˆDecoderä¸­ç¬¬$i$é˜¶æ®µçš„è¾“å…¥çš„ä¸Šä¸‹æ–‡ä¿¡æ¯$c_{i}$å°±æ¥è‡ªäºæ‰€æœ‰$h_{j}$å¯¹$\alpha_{ij}$çš„åŠ æƒå’Œã€‚</li><li>$\alpha_{ij}$å’ŒDecoderçš„ç¬¬$i$é˜¶æ®µçš„éšè—çŠ¶æ€ã€Encoderç¬¬$j$ä¸ªé˜¶æ®µçš„éšè—çŠ¶æ€æœ‰å…³</li><li>åœ¨Encoderçš„è¿‡ç¨‹ä¸­ä¿ç•™æ¯ä¸ªRNNå•å…ƒçš„éšè—çŠ¶æ€(hidden state)å¾—åˆ°($h_{1}$â€¦$h_{N}$)ï¼Œå–$h_{j}$ï¼Œè¡¨ç¤ºEncoderå±‚çš„éšå±‚ç¬¬$j$æ—¶åˆ»çš„è¾“å‡º</li><li>åœ¨Decoderçš„è¿‡ç¨‹ä¸­æ ¹æ®$x_{i}$å’Œ$hâ€™_{i-1}$(è¿™é‡Œå’ŒEncoderçš„$h_{i}$åŒºåˆ†ä¸€ä¸‹)å¾—åˆ°$hâ€™_{i}$ï¼Œè®¾ä¸º$s_{i}$</li><li>æ³¨ï¼šæœ€å¼€å§‹çš„è®ºæ–‡åœ¨Encoder-Decoderé‡Œé¢çš„å½“å‰Decoderçš„attentionå¾—åˆ†ç”¨çš„æ˜¯$s_{i-1}$å’Œ$h_{j}$æ¥ç®—ï¼Œä½†æ–¯å¦ç¦æ•™æä¸Šå›¾ä¸Šç¡®å®æ˜¯ç”»çš„$s_{i}$å’Œ$h_{j}$æ¥ç®—ï¼Œè€Œä¸”åç»­è®ºæ–‡å¤§å¤šæ˜¯ç”¨çš„è¿™ç§æ–¹å¼ï¼Œå³å½“å‰æ­¥çš„attention scoreç”¨çš„å½“å‰æ­¥çš„éšè—çŠ¶æ€$s_{i}$å’Œå‰é¢çš„$h_{j}$å»ç®—çš„</li><li>é€šè¿‡Decoderçš„hidden statesåŠ ä¸ŠEncoderçš„hidden statesæ¥è®¡ç®—ä¸€ä¸ªåˆ†æ•°ï¼Œç”¨äºè®¡ç®—æƒé‡<script type="math/tex; mode=display">e_{ij} = score(s_{i},h_{j})</script></li><li>æ³¨ï¼šè¿™é‡Œæœ‰å¾ˆå¤šè®¡ç®—æ–¹å¼<script type="math/tex; mode=display">score(s_{i},h_{j}) = \left\{\begin{matrix}s^{T}_{i}h_{j}\\ s^{T}_{i}W_{a}h_{j}\\ v^{T}_{a}tanh(W_{a}[s^{T}_{i};h_{j}])\end{matrix}\right.</script></li><li>softmaxæƒé‡å½’ä¸€åŒ–<script type="math/tex; mode=display">\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}</script></li><li>è®¡ç®—$c$<script type="math/tex; mode=display">c_{i} = \sum_{j=1}^{T_{x}}\alpha_{ij}h_{j}</script></li></ul><p><img src="https://pic4.zhimg.com/80/v2-8ddf993a95ee6e525fe2cd5ccd49bba7_hd.jpg" alt=""></p><p>(1)$h_{t} = RNN_{enc}(x_{t},h_{t-1})$, Encoderæ–¹é¢æ¥å—çš„æ˜¯æ¯ä¸€ä¸ªå•è¯word embeddingï¼Œå’Œä¸Šä¸€ä¸ªæ—¶é—´ç‚¹çš„hidden stateã€‚è¾“å‡ºçš„æ˜¯è¿™ä¸ªæ—¶é—´ç‚¹çš„hidden stateã€‚</p><p>(2)$s_{t} = RNN_{dnc}(y_{t},s_{t-1})$, Decoderæ–¹é¢æ¥å—çš„æ˜¯ç›®æ ‡å¥å­é‡Œå•è¯çš„word embeddingï¼Œå’Œä¸Šä¸€ä¸ªæ—¶é—´ç‚¹çš„hidden stateã€‚</p><p>(3)$c_{i} = \sum_{j=1}^{T_{x}}\alpha _{ij}h_{j}$, context vectoræ˜¯ä¸€ä¸ªå¯¹äºencoderè¾“å‡ºçš„hidden statesçš„ä¸€ä¸ªåŠ æƒå¹³å‡ã€‚</p><p>(4)$\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}$, æ¯ä¸€ä¸ªencoderçš„hidden stateså¯¹åº”çš„æƒé‡ã€‚</p><p>(5)$e_{ij} = score(s_{i},h_{j})$, é€šè¿‡decoderçš„hidden statesåŠ ä¸Šencoderçš„hidden statesæ¥è®¡ç®—ä¸€ä¸ªåˆ†æ•°ï¼Œç”¨äºè®¡ç®—æƒé‡(4)</p><p>(6)$\hat{s}_{t}=tanh(W_{c}[c_{t};s_{t}])$, å°†context vector å’Œ decoderçš„hidden states ä¸²èµ·æ¥ã€‚</p><p>(7)$p(y_{t}|y_{&lt;t},x) = softmax(W_{s}\hat{s}_{t})$, è®¡ç®—æœ€åçš„è¾“å‡ºæ¦‚ç‡ã€‚</p><h3 id="Transformerâ€”-Attention-Is-All-You-Need"><a href="#Transformerâ€”-Attention-Is-All-You-Need" class="headerlink" title="Transformerâ€”-Attention Is All You Need"></a>Transformerâ€”-Attention Is All You Need</h3><p><img src="https://pic1.zhimg.com/80/v2-4b53b731a961ee467928619d14a5fd44_hd.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/v2-4b53b731a961ee467928619d14a5fd44_r.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/4155986-208004e73fb93c97.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/4155986-e7fd5fcf3acc00a3.png" alt=""></p><ul><li>Transformer çš„ Encoder ç”± 6 ä¸ªç¼–ç å™¨å åŠ ç»„æˆï¼ŒDecoder ä¹Ÿç”± 6 ä¸ªè§£ç å™¨ç»„æˆï¼Œåœ¨ç»“æ„ä¸Šéƒ½æ˜¯ç›¸åŒçš„ï¼Œä½†å®ƒä»¬ä¸å…±äº«æƒé‡ã€‚</li><li>Encoderçš„æ¯ä¸€å±‚æœ‰ä¸¤ä¸ªæ“ä½œï¼Œåˆ†åˆ«æ˜¯Self-Attentionå’ŒFeed Forwardï¼›</li><li>Decoderçš„æ¯ä¸€å±‚æœ‰ä¸‰ä¸ªæ“ä½œï¼Œåˆ†åˆ«æ˜¯Self-Attentionã€Encoder-Decoder Attentionä»¥åŠFeed Forwardæ“ä½œã€‚</li><li>è¿™é‡Œçš„Self-Attentionå’ŒEncoder-Decoder Attentionéƒ½æ˜¯ç”¨çš„æ˜¯Multi-Head Attentionæœºåˆ¶</li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/v2-df2ca1b7a60d829245b7b7c37f80a3aa_r.jpg" alt=""></p><h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h4><ul><li>RNNçš„å¾ªç¯ç‰¹æ€§å¯¼è‡´å…¶ä¸åˆ©äºå¹¶è¡Œè®¡ç®—ï¼Œæ¨¡å‹è®­ç»ƒæ—¶é—´è¾ƒé•¿</li><li>åœ¨ä¼ ç»Ÿçš„seq2seqä¸­ï¼Œæˆ‘ä»¬é€šè¿‡RNNè·å–hidden stateå»åšattentionï¼Œé‚£ä¹ˆå½“æˆ‘ä»¬å®Œå…¨æŠ›å¼ƒRNNçš„æ—¶å€™ï¼Œæ€ä¹ˆå»åšattentionå‘¢ï¼Ÿ</li><li>å¯¹æ¯ä¸ªinputåšembeddingï¼Œä»£æ›¿hidden stateï¼Œembeddingé€šè¿‡ä¸‰ä¸ªä¸åŒçš„çº¿æ€§å±‚ç”Ÿæˆ$Q$ï¼Œ$K$ï¼Œ$V$ã€‚</li><li>Q: query;K: key; V: value</li><li>K = V = Q</li></ul><script type="math/tex; mode=display">Q = W_{Q}X</script><script type="math/tex; mode=display">K = W_{K}X</script><script type="math/tex; mode=display">V = W_{V}X</script><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix}*[v^{T}_{1},v^{T}_{2},...,v^{T}_{n}])*\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix} = softmax(QK^{T})V</script><p>ä¸¾ä¸ªæ —å­</p><p><img src="https://pic1.zhimg.com/80/v2-087b831f622f83e4529c1bbf646530f0_hd.jpg" alt=""></p><ul><li>å‡å¦‚æˆ‘ä»¬è¦ç¿»è¯‘ä¸€ä¸ªè¯ç»„Thinking Machinesï¼Œå…¶ä¸­Thinkingçš„è¾“å…¥çš„embedding vectorç”¨$x_{1}$è¡¨ç¤ºï¼ŒMachinesçš„embedding vectorç”¨$x_{2}$è¡¨ç¤º</li><li>$W^{Q}$ï¼Œ$W^{K}$ï¼Œ$W^{V}$æ˜¯æˆ‘ä»¬æ¨¡å‹è®­ç»ƒè¿‡ç¨‹å­¦ä¹ åˆ°çš„åˆé€‚çš„å‚æ•°</li><li>$x$ä¸$W^{Q}$ï¼Œ$W^{K}$ï¼Œ$W^{V}$ç›¸ä¹˜è·å¾—$q$ï¼Œ$k$ï¼Œ$v$</li><li>å¦‚ä¸Šå›¾ä¸­æ‰€ç¤ºæˆ‘ä»¬åˆ†åˆ«å¾—åˆ°äº†$q_{1}$ä¸$k_{1}$ï¼Œ$k_{2}$çš„ç‚¹ä¹˜ç§¯ï¼Œç„¶åæˆ‘ä»¬è¿›è¡Œå°ºåº¦ç¼©æ”¾ä¸softmaxå½’ä¸€åŒ–</li></ul><h4 id="Scaled-Dot-Product-Attention-ç¼©æ”¾äº†çš„ç‚¹ä¹˜æ³¨æ„åŠ›"><a href="#Scaled-Dot-Product-Attention-ç¼©æ”¾äº†çš„ç‚¹ä¹˜æ³¨æ„åŠ›" class="headerlink" title="Scaled Dot-Product Attention(ç¼©æ”¾äº†çš„ç‚¹ä¹˜æ³¨æ„åŠ›)"></a>Scaled Dot-Product Attention(ç¼©æ”¾äº†çš„ç‚¹ä¹˜æ³¨æ„åŠ›)</h4><script type="math/tex; mode=display">Attention(Q,K,V) = sofrmax(\frac{QK^{T}}{\sqrt{d_{k}}})V</script><ul><li>è¾“å…¥åŒ…å«$d_{k}$ç»´çš„queryå’Œkeyï¼Œä»¥åŠ$d_{v}$ç»´çš„valueã€‚é€šè¿‡è®¡ç®—queryå’Œå„ä¸ªkeyçš„ç‚¹ç§¯ï¼Œé™¤ä»¥$\sqrt{d_{k}}$å½’ä¸€åŒ–ï¼Œç„¶åç»è¿‡softmaxæ¿€æ´»å˜æˆæƒé‡ï¼Œæœ€åå†ä¹˜valueã€‚ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜ç‚¹æ˜¯é€Ÿåº¦å¿«ã€å ç”¨ç©ºé—´å°ã€‚</li></ul><h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><ul><li>$Q$ï¼Œ$K$ï¼Œ$V$é¦–å…ˆè¿›è¿‡ä¸€ä¸ªçº¿æ€§å˜æ¢ï¼Œç„¶åè¾“å…¥åˆ°æ”¾ç¼©ç‚¹ç§¯attention</li><li>æ¯æ¬¡$Q$ï¼Œ$K$ï¼Œ$V$è¿›è¡Œçº¿æ€§å˜æ¢çš„å‚æ•°$W$æ˜¯ä¸ä¸€æ ·çš„</li><li>é€šè¿‡$h$ä¸ªä¸åŒçš„çº¿æ€§å˜æ¢å¯¹$Q$ï¼Œ$K$ï¼Œ$V$è¿›è¡ŒæŠ•å½±ï¼Œæœ€åå°†ä¸åŒçš„attentionç»“æœæ‹¼æ¥èµ·æ¥</li></ul><script type="math/tex; mode=display">Multihead(Q,K,V) = Concat(head_{1},...,head_{h})W^{O}</script><script type="math/tex; mode=display">head_{i} = Attention(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})</script><h4 id="Position-wise-feed-forward-networks-ä½ç½®å…¨é“¾æ¥å‰é¦ˆç½‘ç»œ"><a href="#Position-wise-feed-forward-networks-ä½ç½®å…¨é“¾æ¥å‰é¦ˆç½‘ç»œ" class="headerlink" title="Position-wise feed-forward networks(ä½ç½®å…¨é“¾æ¥å‰é¦ˆç½‘ç»œ)"></a>Position-wise feed-forward networks(ä½ç½®å…¨é“¾æ¥å‰é¦ˆç½‘ç»œ)</h4><ul><li>ç”±ä¸¤ä¸ªçº¿æ€§å˜æ¢ï¼ˆWx+bï¼‰å’Œä¸€ä¸ªReLUï¼ˆreluçš„æ•°å­¦è¡¨è¾¾å¼å°±æ˜¯f(x)=max(0,x)ï¼‰<script type="math/tex; mode=display">FFN(x) = max(0,xW_{1} + b_{1})W_{2} + b_{2}</script></li></ul><h4 id="Positional-Encoding-ä½ç½®ç¼–ç "><a href="#Positional-Encoding-ä½ç½®ç¼–ç " class="headerlink" title="Positional Encoding(ä½ç½®ç¼–ç )"></a>Positional Encoding(ä½ç½®ç¼–ç )</h4><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1115<span class="string">-1120</span> after data smoothing</span><br><span class="line">T = 10</span><br><span class="line">features = 70</span><br><span class="line">train = all * 0.7</span><br><span class="line"><span class="keyword">test </span>= all * 0.3</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-1115-1120-all.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-1115-1120-test-m.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-1115-1120-test.jpg" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 3.955</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.289</span><br></pre></td></tr></table></figure><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nasdaq100_padding</span><br><span class="line">T = 10</span><br><span class="line">features = 81</span><br><span class="line">train = all * 0.7</span><br><span class="line"><span class="keyword">test </span>= all * 0.3</span><br></pre></td></tr></table></figure><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">Encoder:</span> LSTM</span><br><span class="line"><span class="symbol">Decoder:</span> LSTM</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-lstm.jpg" alt=""><br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.579</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.105</span><br></pre></td></tr></table></figure></p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">Encoder:</span> BiLSTM</span><br><span class="line"><span class="symbol">Decoder:</span> LSTM</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-bi-lstm.jpg" alt=""><br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.384</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.069</span><br></pre></td></tr></table></figure></p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">Encoder:</span> GRU</span><br><span class="line"><span class="symbol">Decoder:</span> LSTM</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-gru.jpg" alt=""><br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.252</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.046</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RNN -&amp;gt; LSTM -&amp;gt; GRU -&amp;gt; Seq2Seq -&amp;gt; Attention -&amp;gt; Transformer&lt;br&gt;
    
    </summary>
    
    
      <category term="å‘¨æŠ¥" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>Leetcode38Count-and-Say</title>
    <link href="http://arithmeticjia.github.io/2019/12/24/Leetcode38Count-and-Say/"/>
    <id>http://arithmeticjia.github.io/2019/12/24/Leetcode38Count-and-Say/</id>
    <published>2019-12-24T08:01:01.000Z</published>
    <updated>2019-12-24T08:02:18.323Z</updated>
    
    <content type="html"><![CDATA[<p>Nothing<br><a id="more"></a><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">countAndSay</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">1</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"1"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        String str = countAndSay(n-<span class="number">1</span>) + <span class="string">"*"</span>;<span class="comment">// è¿™æ ·æœ«å°¾çš„æ•°æ‰èƒ½è¢«å¾ªç¯å¤„ç†åˆ°</span></span><br><span class="line">        <span class="keyword">char</span>[] str_c = str.toCharArray();</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line">        StringBuilder temp = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (i &lt; str_c.length-<span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span>(str_c[i] == str_c[i+<span class="number">1</span>])&#123;</span><br><span class="line">                count++;  <span class="comment">//é‡åˆ°ç›¸åŒçš„è®¡æ•°å™¨åŠ </span></span><br><span class="line">                i++;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                temp.append(Integer.toString(count)+ str_c[i]);</span><br><span class="line">                <span class="comment">// é‡åˆ°ä¸åŒçš„ï¼Œå…ˆappendè®¡æ•°å™¨çš„å€¼ï¼Œå†appendæœ€åä¸€ä¸ªç›¸åŒçš„å€¼</span></span><br><span class="line">                <span class="comment">// temp.append("" + count + str_c[i]);</span></span><br><span class="line">                count = <span class="number">1</span>;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> temp.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Nothing&lt;br&gt;
    
    </summary>
    
    
      <category term="Leetcode" scheme="http://arithmeticjia.github.io/categories/Leetcode/"/>
    
    
      <category term="leetcode" scheme="http://arithmeticjia.github.io/tags/leetcode/"/>
    
      <category term="java" scheme="http://arithmeticjia.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Seq2seqæ¨¡å‹åŠæ³¨æ„åŠ›æœºåˆ¶æ¨¡å‹</title>
    <link href="http://arithmeticjia.github.io/2019/12/22/Seq2seq%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A8%A1%E5%9E%8B/"/>
    <id>http://arithmeticjia.github.io/2019/12/22/Seq2seq%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A8%A1%E5%9E%8B/</id>
    <published>2019-12-22T07:21:43.000Z</published>
    <updated>2019-12-24T10:48:12.961Z</updated>
    
    <content type="html"><![CDATA[<p>å¯¹äºå¤„ç†è¾“å‡ºåºåˆ—ä¸ºä¸å®šé•¿æƒ…å†µçš„é—®é¢˜ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ï¼Œä¾‹å¦‚è‹±æ–‡åˆ°æ³•è¯­çš„å¥å­ç¿»è¯‘ï¼Œè¾“å…¥å’Œè¾“å‡ºå‡ä¸ºä¸å®šé•¿ã€‚å‰äººæå‡ºäº†seq2seqæ¨¡å‹ï¼Œbasic ideaæ˜¯è®¾è®¡ä¸€ä¸ªencoderä¸decoderï¼Œå…¶ä¸­encoderå°†è¾“å…¥åºåˆ—ç¼–ç ä¸ºä¸€ä¸ªåŒ…å«è¾“å…¥åºåˆ—æ‰€æœ‰ä¿¡æ¯çš„context vector $ c $ï¼Œdecoderé€šè¿‡å¯¹$ c $çš„è§£ç è·å¾—è¾“å…¥åºåˆ—çš„ä¿¡æ¯ï¼Œä»è€Œå¾—åˆ°è¾“å‡ºåºåˆ—ã€‚encoderåŠdecoderéƒ½é€šå¸¸ä¸ºRNNå¾ªç¯ç¥ç»ç½‘ç»œ<br><a id="more"></a></p><h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><ul><li>input: å½“å‰æ—¶åˆ»è¾“å…¥å€¼$x_{t}$,ä¸Šä¸€æ—¶åˆ»LSTMçš„è¾“å‡ºå€¼$h_{t-1}$,ä¸Šä¸€æ—¶åˆ»çš„å•å…ƒçŠ¶æ€$c_{t-1}$</li><li>output: å½“å‰æ—¶åˆ»LSTMçš„è¾“å‡ºå€¼$h_{t}$,å½“å‰æ—¶åˆ»çš„å•å…ƒçŠ¶$c_{t}$</li><li>forget gate:</li></ul><script type="math/tex; mode=display">f_{t} = \sigma (W_{f}[h_{t-1};x_{t}]+b_{f})</script><p>$W_{f}$æ˜¯é—å¿˜é—¨çš„æƒé‡çŸ©é˜µï¼Œ$[h_{t-1};x_{t}]$è¡¨ç¤ºæŠŠä¸¤ä¸ªå‘é‡è¿æ¥æˆä¸€ä¸ªæ›´é•¿çš„å‘é‡ï¼Œ$b_{f}$æ˜¯é—å¿˜é—¨çš„åç½®é¡¹ï¼Œ$\sigma$æ˜¯sigmoidå‡½æ•°<br>å¦‚æœè¾“å…¥çš„ç»´åº¦æ˜¯$d_{x}$ï¼Œéšè—å±‚çš„ç»´åº¦æ˜¯$d_{h}$ï¼Œå•å…ƒçŠ¶æ€çš„ç»´åº¦æ˜¯$d_{c}$ï¼ˆé€šå¸¸$d_{c} = d_{h}$ï¼‰ï¼Œåˆ™é—å¿˜é—¨çš„æƒé‡çŸ©é˜µ$W_{f}$çš„ç»´åº¦æ˜¯$d_{c}Ã—(d_{h}+d_{x})$</p><ul><li><p>input gate:</p><script type="math/tex; mode=display">i_{t} = \sigma (W_{i}[h_{t-1};x_{t}]+b_{i})</script></li><li><p>output gate:</p><script type="math/tex; mode=display">o_{t} = \sigma (W_{o}[h_{t-1};x_{t}]+b_{o})</script></li><li><p>final out:</p><script type="math/tex; mode=display">\tilde{c}_{t}= tanh(W_{c}[h_{t-1};x_{t}]+b_{c})</script><script type="math/tex; mode=display">c_{t} = f_{t} * c_{t-1} + i_{t} * \tilde{c}_{t}</script><script type="math/tex; mode=display">h_{t} = o_{t} * tanh(c_{t})</script></li><li><p>å‰å‘è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„è¾“å‡ºå€¼ï¼Œå¯¹äºLSTMæ¥è¯´å°±æ˜¯$f_{t}$,$i_{t}$,$c_{t}$,$o_{t}$,$h_{t}$ 5ä¸ªå‘é‡çš„å€¼</p></li><li>åå‘è®¡ç®—æ¯ä¸ªç¥ç»å…ƒçš„è¯¯å·®é¡¹$\delta$ï¼ŒåŒ…æ‹¬ä¸¤ä¸ªæ–¹å‘ï¼Œä¸€æ˜¯æ²¿æ—¶é—´çš„åå‘ä¼ æ’­ï¼Œå³ä»å½“å‰tæ—¶åˆ»å¼€å§‹ï¼Œè®¡ç®—æ¯ä¸ªæ—¶åˆ»çš„è¯¯å·®é¡¹ï¼›å¦ä¸€ä¸ªæ˜¯å°†è¯¯å·®é¡¹å‘ä¸Šä¸€å±‚ä¼ æ’­</li><li>æ ¹æ®ç›¸åº”çš„è¯¯å·®é¡¹ï¼Œè®¡ç®—æ¯ä¸ªæƒé‡çš„æ¢¯åº¦</li><li>sigmoid</li></ul><script type="math/tex; mode=display">\delta (x) = \frac{1}{1+e^{-x}}</script><script type="math/tex; mode=display">\delta^{'} (x) = \frac{e^{-x}}{(1+e^{-x})^{2}}=\delta(x)(1-\delta(x))</script><ul><li>tanh</li></ul><script type="math/tex; mode=display">tanh(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}</script><script type="math/tex; mode=display">tanh^{'}(x) = 1 - tanh^{2}(x)</script><p>LSTMéœ€è¦å­¦ä¹ çš„å‚æ•°å…±æœ‰8ç»„ï¼Œåˆ†åˆ«æ˜¯ï¼š</p><ul><li>é—å¿˜é—¨çš„æƒé‡çŸ©é˜µ$W_{f}$å’Œåç½®é¡¹$b_{f}$</li><li>è¾“å…¥é—¨çš„æƒé‡çŸ©é˜µ$W_{i}$å’Œåç½®é¡¹$b_{i}$</li><li>è¾“å‡ºé—¨çš„æƒé‡çŸ©é˜µ$W_{o}$å’Œåç½®é¡¹$b_{o}$</li><li>è®¡ç®—å•å…ƒçŠ¶æ€çš„æƒé‡çŸ©é˜µ$W_{c}$å’Œåç½®é¡¹$b_{c}$</li></ul><h4 id="seq2seqæ¨¡å‹"><a href="#seq2seqæ¨¡å‹" class="headerlink" title="seq2seqæ¨¡å‹"></a>seq2seqæ¨¡å‹</h4><h5 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h5><p>ç¼–ç å™¨çš„ä½œç”¨æ˜¯æŠŠä¸€ä¸ªä¸å®šé•¿çš„è¾“å…¥åºåˆ—$ x_{1},x_{2},â€¦,x_{T} $è½¬åŒ–æˆä¸€ä¸ªå®šé•¿çš„context vector $c$. è¯¥context vectorç¼–ç äº†è¾“å…¥åºåˆ—$ x_{1},x_{2},â€¦,x_{T} $çš„åºåˆ—ã€‚å›å¿†ä¸€ä¸‹å¾ªç¯ç¥ç»ç½‘ç»œï¼Œå‡è®¾è¯¥å¾ªç¯ç¥ç»ç½‘ç»œå•å…ƒä¸º$f$ï¼ˆå¯ä»¥ä¸ºvanilla RNN, LSTM, GRU)ï¼Œé‚£ä¹ˆhidden stateä¸º</p><script type="math/tex; mode=display">h_{t} = f(x_{t},h_{t-1})</script><p>ç¼–ç å™¨çš„context vectoræ˜¯æ‰€æœ‰æ—¶åˆ»hidden stateçš„å‡½æ•°ï¼Œå³ï¼š</p><script type="math/tex; mode=display">c=q(h_{1},...,h_{T})</script><p>ç®€å•åœ°ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæœ€ç»ˆæ—¶åˆ»çš„hidden state[å…¬å¼]ä½œä¸ºcontext vecterã€‚å½“ç„¶æˆ‘ä»¬ä¹Ÿå¯ä»¥å–å„ä¸ªæ—¶åˆ»hidden statesçš„å¹³å‡ï¼Œä»¥åŠå…¶ä»–æ–¹æ³•ã€‚</p><h5 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h5><p>ç¼–ç å™¨æœ€ç»ˆè¾“å‡ºä¸€ä¸ªcontext vector $c$ï¼Œè¯¥context vectorç¼–ç äº†è¾“å…¥åºåˆ—$ x_{1},x_{2},â€¦,x_{T} $çš„ä¿¡æ¯ã€‚</p><p>å‡è®¾è®­ç»ƒæ•°æ®ä¸­çš„è¾“å‡ºåºåˆ—ä¸º$y_{1}y_{2},â€¦,y_{T}^{â€˜}$,æˆ‘ä»¬å¸Œæœ›æ¯ä¸ª$t$æ—¶åˆ»çš„è¾“å‡ºå³å–å†³äºä¹‹å‰çš„è¾“å‡ºä¹Ÿå–å†³äºcontext vectorï¼Œå³ä¼°è®¡$P(y_{tâ€™}|y_{1},â€¦,y_{tâ€™-1},c)$ï¼Œä»è€Œå¾—åˆ°è¾“å‡ºåºåˆ—çš„è”åˆæ¦‚ç‡åˆ†å¸ƒï¼š</p><script type="math/tex; mode=display">P(y_{1},...,y_{T'})=\prod_{t'-1}^{T'}P(y_{t'}|y_{1},...,y_{t'-1},c)</script><p>å¹¶å®šä¹‰è¯¥åºåˆ—çš„æŸå¤±å‡½æ•°loss function</p><script type="math/tex; mode=display">-\log P(y_{1},...,y_{T'})</script><p>é€šè¿‡æœ€å°åŒ–æŸå¤±å‡½æ•°æ¥è®­ç»ƒseq2seqæ¨¡å‹ã€‚</p><p>é‚£ä¹ˆå¦‚ä½•ä¼°è®¡$ P(y_{tâ€™}|y_{1},â€¦,y_{tâ€™-1},c) $ï¼Ÿ</p><p>æˆ‘ä»¬ä½¿ç”¨å¦ä¸€ä¸ªå¾ªç¯ç¥ç»ç½‘ç»œä½œä¸ºè§£ç å™¨ã€‚è§£ç å™¨ä½¿ç”¨å‡½æ•°$p$æ¥è¡¨ç¤º$tâ€™$æ—¶åˆ»è¾“å‡º$y_{tâ€™}$çš„æ¦‚ç‡</p><script type="math/tex; mode=display">P(y_{t'}|y_{1},...,y_{t'-1},c) = p(y_{t'-1},s_{t'},c)</script><p>ä¸ºäº†åŒºåˆ†ç¼–ç å™¨ä¸­çš„hidden state[å…¬å¼]ï¼Œå…¶ä¸­[å…¬å¼]ä¸º[å…¬å¼]æ—¶åˆ»è§£ç å™¨çš„hidden stateã€‚åŒºåˆ«äºç¼–ç å™¨ï¼Œè§£ç å™¨ä¸­çš„å¾ªç¯ç¥ç»ç½‘ç»œçš„è¾“å…¥é™¤äº†å‰ä¸€ä¸ªæ—¶åˆ»çš„è¾“å‡ºåºåˆ—[å…¬å¼]ï¼Œå’Œå‰ä¸€ä¸ªæ—¶åˆ»çš„hidden state[å…¬å¼]ä»¥å¤–ï¼Œè¿˜åŒ…å«äº†context vector[å…¬å¼]ã€‚å³ï¼š</p><script type="math/tex; mode=display">s_{t'} = g(y_{t'-1},s_{t'-1},c)</script><p>å…¶ä¸­å‡½æ•°gä¸ºè§£ç å™¨çš„å¾ªç¯ç¥ç»ç½‘ç»œå•å…ƒã€‚</p><h4 id="DA-RNN"><a href="#DA-RNN" class="headerlink" title="DA-RNN"></a>DA-RNN</h4><h5 id="ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶è‡ªé€‚åº”åœ°æå–æ¯ä¸ªæ—¶åˆ»çš„ç›¸å…³feature"><a href="#ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶è‡ªé€‚åº”åœ°æå–æ¯ä¸ªæ—¶åˆ»çš„ç›¸å…³feature" class="headerlink" title="ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶è‡ªé€‚åº”åœ°æå–æ¯ä¸ªæ—¶åˆ»çš„ç›¸å…³feature"></a>ç¬¬ä¸€é˜¶æ®µï¼Œä½¿ç”¨æ³¨æ„åŠ›æœºåˆ¶è‡ªé€‚åº”åœ°æå–æ¯ä¸ªæ—¶åˆ»çš„ç›¸å…³feature</h5><script type="math/tex; mode=display">e_{t}^{k}=v_{e}^{T}tanh(W_{e}[h_{t-1};c_{t-1}]+U_{e}x^{k})</script><ul><li>ç”¨softmaxå‡½æ•°å°†å…¶å½’ä¸€åŒ–<script type="math/tex; mode=display">\alpha _{t}^{k}=\frac{exp(e_{t}^{k})}{\sum_{i-1}^{n}exp(e_{t}^{i})}</script></li><li>å¾—åˆ°æ›´æ–°åçš„x<script type="math/tex; mode=display">\tilde{x} = (\alpha _{t}^{1}x_{t}^{1}, \alpha _{t}^{2}x_{t}^{2},...,\alpha _{t}^{n}x_{t}^{n})</script></li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/lstm.jpg" alt=""></p><ul><li><p>é€‰å–LSTMä½œä¸ºç¼–ç å™¨<script type="math/tex">f_{1}</script></p><script type="math/tex; mode=display">h_{t} = f_{1}(h_{t-1},  \tilde{x})</script></li><li><p>Encoderæ–¹é¢æ¥å—çš„æ˜¯æ¯ä¸€ä¸ªè¾“å…¥ï¼Œå’Œä¸Šä¸€ä¸ªæ—¶é—´ç‚¹çš„éšè—æ€ã€‚è¾“å‡ºçš„æ˜¯å½“å‰æ—¶é—´ç‚¹çš„éšè—æ€</p></li></ul><h5 id="ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨å¦ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶é€‰å–ä¸ä¹‹ç›¸å…³çš„encoder-hidden-states"><a href="#ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨å¦ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶é€‰å–ä¸ä¹‹ç›¸å…³çš„encoder-hidden-states" class="headerlink" title="ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨å¦ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶é€‰å–ä¸ä¹‹ç›¸å…³çš„encoder hidden states"></a>ç¬¬äºŒé˜¶æ®µï¼Œä½¿ç”¨å¦ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶é€‰å–ä¸ä¹‹ç›¸å…³çš„encoder hidden states</h5><ul><li><p>Decoderæ–¹é¢æ¥å—çš„æ˜¯ç›®æ ‡è¾“å…¥ï¼Œå’Œä¸Šä¸€ä¸ªæ—¶é—´ç‚¹çš„éšè—æ€</p></li><li><p>å¯¹æ‰€æœ‰æ—¶åˆ»çš„$h_{tâ€™}$å–åŠ æƒå¹³å‡ï¼Œå³ï¼š</p></li></ul><script type="math/tex; mode=display">c_{t}^{'} = \sum_{t-1}^{T}\beta _{t^{'}}^{t}h_{t}</script><ul><li><script type="math/tex">\beta _{t^{'}}^{t}</script>çš„è®¾è®¡ç±»ä¼¼äºBahanauçš„å·¥ä½œï¼ŒåŸºäºå‰ä¸€ä¸ªæ—¶åˆ»è§£ç å™¨çš„hidden state $ d_{tâ€™-1} $å’Œcell state$s_{tâ€™-1}^{â€˜}$è®¡ç®—å¾—åˆ°ï¼š</li></ul><script type="math/tex; mode=display">l_{t}^{t}=v_{d}^{T}tanh(W_{d}[d_{t-1};s_{t-1}^{'}]+U_{d}h_{t})</script><script type="math/tex; mode=display">\beta _{t}^{i}=\frac{exp(l_{t}^{i})}{\sum_{j=1}^{T}exp(l_{t}^{j})}</script><script type="math/tex; mode=display">c_{t}=\sum_{i=1}^{T}\beta _{t}^{i}h_{i}</script><ul><li>è§£ç å™¨çš„è¾“å…¥æ˜¯ä¸Šä¸€ä¸ªæ—¶åˆ»çš„ç›®æ ‡åºåˆ—$y_{tâ€™-1}$å’Œhidden state$d_{tâ€™-1}$ä»¥åŠcontext vector $c_{tâ€™-1}$ï¼Œå³<script type="math/tex; mode=display">d_{t'}=f_{2}(y_{t'-1},c_{t'-1},d_{t'-1})</script></li><li>è¿™é‡Œè®¾è®¡äº†$\tilde{y}_{tâ€™-1}$æ¥combie$y_{tâ€™-1}$ä¸$c_{tâ€™-1}$çš„ä¿¡æ¯ï¼Œå³<script type="math/tex; mode=display">\tilde{y}_{t'-1} = \tilde{\omega }^{T}[y_{t'-1};c_{t'-1}]+\tilde{b}</script></li><li>ç„¶å<script type="math/tex; mode=display">d_{t}=f_{2}(d_{t-1},\tilde{y}_{t-1})</script></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;å¯¹äºå¤„ç†è¾“å‡ºåºåˆ—ä¸ºä¸å®šé•¿æƒ…å†µçš„é—®é¢˜ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘ï¼Œä¾‹å¦‚è‹±æ–‡åˆ°æ³•è¯­çš„å¥å­ç¿»è¯‘ï¼Œè¾“å…¥å’Œè¾“å‡ºå‡ä¸ºä¸å®šé•¿ã€‚å‰äººæå‡ºäº†seq2seqæ¨¡å‹ï¼Œbasic ideaæ˜¯è®¾è®¡ä¸€ä¸ªencoderä¸decoderï¼Œå…¶ä¸­encoderå°†è¾“å…¥åºåˆ—ç¼–ç ä¸ºä¸€ä¸ªåŒ…å«è¾“å…¥åºåˆ—æ‰€æœ‰ä¿¡æ¯çš„context vector $ c $ï¼Œdecoderé€šè¿‡å¯¹$ c $çš„è§£ç è·å¾—è¾“å…¥åºåˆ—çš„ä¿¡æ¯ï¼Œä»è€Œå¾—åˆ°è¾“å‡ºåºåˆ—ã€‚encoderåŠdecoderéƒ½é€šå¸¸ä¸ºRNNå¾ªç¯ç¥ç»ç½‘ç»œ&lt;br&gt;
    
    </summary>
    
    
    
      <category term="seq2seq" scheme="http://arithmeticjia.github.io/tags/seq2seq/"/>
    
      <category term="attention" scheme="http://arithmeticjia.github.io/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>da-rnn-bug-fix</title>
    <link href="http://arithmeticjia.github.io/2019/12/21/da-rnn-bug-fix/"/>
    <id>http://arithmeticjia.github.io/2019/12/21/da-rnn-bug-fix/</id>
    <published>2019-12-21T14:19:27.000Z</published>
    <updated>2019-12-23T08:00:10.477Z</updated>
    
    <content type="html"><![CDATA[<p>Bugs fix for<br><a href="https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py" target="_blank" rel="noopener" title="https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py">https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py</a><br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> concatenate</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">filename = <span class="string">'nasdaq100_padding.csv'</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(filename)</span><br><span class="line"><span class="comment"># print(dataset.values)</span></span><br><span class="line"></span><br><span class="line">features = dataset.values.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 82</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderAtt</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, T)</span>:</span></span><br><span class="line">        <span class="comment"># input size: number of underlying factors (81)</span></span><br><span class="line">        <span class="comment"># T: number of time steps (10)</span></span><br><span class="line">        <span class="comment"># hidden_size: dimension of the hidden state</span></span><br><span class="line">        super(EncoderAtt, self).__init__()</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.T = T</span><br><span class="line"></span><br><span class="line">        self.lstm_layer = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=<span class="number">1</span>)</span><br><span class="line">        self.attn_linear = nn.Linear(in_features=<span class="number">2</span> * hidden_size + T - <span class="number">1</span>, out_features=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_data)</span>:</span></span><br><span class="line">        <span class="comment"># input_data: batch_size * T - 1 * input_size</span></span><br><span class="line">        input_weighted = Variable(input_data.data.new(input_data.size(<span class="number">0</span>), self.T - <span class="number">1</span>, self.input_size).zero_())</span><br><span class="line">        input_encoded = Variable(input_data.data.new(input_data.size(<span class="number">0</span>), self.T - <span class="number">1</span>, self.hidden_size).zero_())</span><br><span class="line">        <span class="comment"># hidden, cell: initial states with dimention hidden_size</span></span><br><span class="line">        hidden = self.init_hidden(input_data) <span class="comment"># 1 * batch_size * hidden_size</span></span><br><span class="line">        cell = self.init_hidden(input_data)</span><br><span class="line">        <span class="comment"># hidden.requires_grad = False</span></span><br><span class="line">        <span class="comment"># cell.requires_grad = False</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(self.T - <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># Eqn. 8: concatenate the hidden states with each predictor</span></span><br><span class="line">            x = torch.cat((hidden.repeat(self.input_size, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>),</span><br><span class="line">                           cell.repeat(self.input_size, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>),</span><br><span class="line">                           input_data.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)), dim = <span class="number">2</span>) <span class="comment"># batch_size * input_size * (2*hidden_size + T - 1)</span></span><br><span class="line">            <span class="comment"># Eqn. 9: Get attention weights</span></span><br><span class="line">            x = self.attn_linear(x.view(<span class="number">-1</span>, self.hidden_size * <span class="number">2</span> + self.T - <span class="number">1</span>)) <span class="comment"># (batch_size * input_size) * 1</span></span><br><span class="line">            attn_weights = F.softmax(x.view(<span class="number">-1</span>, self.input_size)) <span class="comment"># batch_size * input_size, attn weights with values sum up to 1.</span></span><br><span class="line">            <span class="comment"># Eqn. 10: LSTM</span></span><br><span class="line">            weighted_input = torch.mul(attn_weights, input_data[:, t, :]) <span class="comment"># batch_size * input_size</span></span><br><span class="line">            <span class="comment"># Fix the warning about non-contiguous memory</span></span><br><span class="line">            <span class="comment"># see https://discuss.pytorch.org/t/dataparallel-issue-with-flatten-parameter/8282</span></span><br><span class="line">            self.lstm_layer.flatten_parameters()</span><br><span class="line">            _, lstm_states = self.lstm_layer(weighted_input.unsqueeze(<span class="number">0</span>), (hidden, cell))</span><br><span class="line">            hidden = lstm_states[<span class="number">0</span>]</span><br><span class="line">            cell = lstm_states[<span class="number">1</span>]</span><br><span class="line">            <span class="comment"># Save output</span></span><br><span class="line">            input_weighted[:, t, :] = weighted_input</span><br><span class="line">            input_encoded[:, t, :] = hidden</span><br><span class="line">        <span class="keyword">return</span> input_weighted, input_encoded</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># No matter whether CUDA is used, the returned variable will have the same type as x.</span></span><br><span class="line">        <span class="keyword">return</span> Variable(x.data.new(<span class="number">1</span>, x.size(<span class="number">0</span>), self.hidden_size).zero_()) <span class="comment"># dimension 0 is the batch dimension</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderAtt</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder_hidden_size, decoder_hidden_size, T)</span>:</span></span><br><span class="line">        super(DecoderAtt, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.T = T</span><br><span class="line">        self.encoder_hidden_size = encoder_hidden_size</span><br><span class="line">        self.decoder_hidden_size = decoder_hidden_size</span><br><span class="line"></span><br><span class="line">        self.attn_layer = nn.Sequential(nn.Linear(<span class="number">2</span> * decoder_hidden_size + encoder_hidden_size, encoder_hidden_size),</span><br><span class="line">                                        nn.Tanh(), nn.Linear(encoder_hidden_size, <span class="number">1</span>))</span><br><span class="line">        self.lstm_layer = nn.LSTM(input_size=<span class="number">1</span>, hidden_size=decoder_hidden_size)</span><br><span class="line">        self.fc = nn.Linear(encoder_hidden_size + <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc_final = nn.Linear(decoder_hidden_size + encoder_hidden_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.fc.weight.data.normal_()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_encoded, y_history)</span>:</span></span><br><span class="line">        <span class="comment"># input_encoded: batch_size * T - 1 * encoder_hidden_size</span></span><br><span class="line">        <span class="comment"># y_history: batch_size * (T-1)</span></span><br><span class="line">        <span class="comment"># Initialize hidden and cell, 1 * batch_size * decoder_hidden_size</span></span><br><span class="line">        hidden = self.init_hidden(input_encoded)</span><br><span class="line">        cell = self.init_hidden(input_encoded)</span><br><span class="line">        <span class="comment"># hidden.requires_grad = False</span></span><br><span class="line">        <span class="comment"># cell.requires_grad = False</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(self.T - <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># Eqn. 12-13: compute attention weights</span></span><br><span class="line">            <span class="comment">## batch_size * T * (2*decoder_hidden_size + encoder_hidden_size)</span></span><br><span class="line">            x = torch.cat((hidden.repeat(self.T - <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>),</span><br><span class="line">                           cell.repeat(self.T - <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), input_encoded), dim=<span class="number">2</span>)</span><br><span class="line">            x = F.softmax(self.attn_layer(x.view(<span class="number">-1</span>, <span class="number">2</span> * self.decoder_hidden_size + self.encoder_hidden_size</span><br><span class="line">                                                 )).view(<span class="number">-1</span>, self.T - <span class="number">1</span>))  <span class="comment"># batch_size * T - 1, row sum up to 1</span></span><br><span class="line">            <span class="comment"># Eqn. 14: compute context vector</span></span><br><span class="line">            context = torch.bmm(x.unsqueeze(<span class="number">1</span>), input_encoded)[:, <span class="number">0</span>, :]  <span class="comment"># batch_size * encoder_hidden_size</span></span><br><span class="line">            <span class="keyword">if</span> t &lt; self.T - <span class="number">1</span>:</span><br><span class="line">                <span class="comment"># Eqn. 15</span></span><br><span class="line">                y_tilde = self.fc(torch.cat((context, y_history[:, t].unsqueeze(<span class="number">1</span>)), dim=<span class="number">1</span>))  <span class="comment"># batch_size * 1</span></span><br><span class="line">                <span class="comment"># Eqn. 16: LSTM</span></span><br><span class="line">                self.lstm_layer.flatten_parameters()</span><br><span class="line">                _, lstm_output = self.lstm_layer(y_tilde.unsqueeze(<span class="number">0</span>), (hidden, cell))</span><br><span class="line">                hidden = lstm_output[<span class="number">0</span>]  <span class="comment"># 1 * batch_size * decoder_hidden_size</span></span><br><span class="line">                cell = lstm_output[<span class="number">1</span>]  <span class="comment"># 1 * batch_size * decoder_hidden_size</span></span><br><span class="line">        <span class="comment"># Eqn. 22: final output</span></span><br><span class="line">        y_pred = self.fc_final(torch.cat((hidden[<span class="number">0</span>], context), dim=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># self.logger.info("hidden %s context %s y_pred: %s", hidden[0][0][:10], context[0][:10], y_pred[:10])</span></span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> Variable(x.data.new(<span class="number">1</span>, x.size(<span class="number">0</span>), self.decoder_hidden_size).zero_())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_data</span><span class="params">(dat, col_names)</span>:</span></span><br><span class="line">    scale = StandardScaler().fit(dat)</span><br><span class="line">    proc_dat = scale.transform(dat)</span><br><span class="line"></span><br><span class="line">    mask = np.ones(proc_dat.shape[<span class="number">1</span>], dtype=bool)</span><br><span class="line">    dat_cols = list(dat.columns)</span><br><span class="line">    <span class="keyword">for</span> col_name <span class="keyword">in</span> col_names:</span><br><span class="line">        mask[dat_cols.index(col_name)] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    feats = proc_dat[:, mask]</span><br><span class="line">    targs = proc_dat[:, ~mask]</span><br><span class="line">    <span class="keyword">return</span> feats, targs, scale</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">da_rnn</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, file_data, encoder_hidden_size=<span class="number">64</span>, decoder_hidden_size=<span class="number">64</span>, T=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 learning_rate=<span class="number">0.01</span>, batch_size=<span class="number">128</span>, parallel=True, debug=False)</span>:</span></span><br><span class="line">        self.T = T</span><br><span class="line">        dat = pd.read_csv(file_data, nrows=<span class="number">100</span> <span class="keyword">if</span> debug <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">        <span class="comment"># read first 100 rows</span></span><br><span class="line">        <span class="comment"># self.logger.info("Shape of data: %s.\nMissing in data: %s.", dat.shape, dat.isnull().sum().sum())</span></span><br><span class="line">        <span class="comment"># scale = StandardScaler().fit(dat.values)</span></span><br><span class="line">        <span class="comment"># dat = pd.DataFrame(scale.transform(dat.values))</span></span><br><span class="line">        <span class="comment"># self.X = dat.loc[:, [x for x in dat.columns.tolist() if x != 'NDX']].as_matrix()</span></span><br><span class="line">        self.X, self.y, self.scaler = preprocess_data(dat, (<span class="string">"NDX"</span>,))</span><br><span class="line">        <span class="comment"># select matrix without NDX</span></span><br><span class="line">        <span class="comment"># (ndarray:(40560,81))</span></span><br><span class="line">        self.y = (self.y).reshape((self.y).shape[<span class="number">0</span>],)</span><br><span class="line">        <span class="comment"># self.y = np.array(dat.NDX)</span></span><br><span class="line">        <span class="comment"># (ndarray:(40560,))</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        <span class="comment"># 128</span></span><br><span class="line">        self.encoder = EncoderAtt(input_size=self.X.shape[<span class="number">1</span>], hidden_size=encoder_hidden_size, T=T).to(device)</span><br><span class="line">        self.decoder = DecoderAtt(encoder_hidden_size=encoder_hidden_size, decoder_hidden_size=decoder_hidden_size, T=T).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> parallel:</span><br><span class="line">            self.encoder = nn.DataParallel(self.encoder)</span><br><span class="line">            self.decoder = nn.DataParallel(self.decoder)</span><br><span class="line">        <span class="comment">#  multiple GPU training</span></span><br><span class="line"></span><br><span class="line">        self.encoder_optimizer = optim.Adam(params=filter(<span class="keyword">lambda</span> p: p.requires_grad, self.encoder.parameters()),</span><br><span class="line">                                           lr=learning_rate)</span><br><span class="line">        self.decoder_optimizer = optim.Adam(params=filter(<span class="keyword">lambda</span> p: p.requires_grad, self.decoder.parameters()),</span><br><span class="line">                                           lr=learning_rate)</span><br><span class="line">        <span class="comment"># self.learning_rate = learning_rate</span></span><br><span class="line"></span><br><span class="line">        self.train_size = int(self.X.shape[<span class="number">0</span>] * <span class="number">0.7</span>)</span><br><span class="line">        <span class="comment"># &#123;int&#125; 28392</span></span><br><span class="line">        <span class="comment"># self.y = self.y - np.mean(self.y[:self.train_size])</span></span><br><span class="line">        <span class="comment"># self.y = (self.y - np.mean(self.y[:self.train_size])) / np.std(self.y[:self.train_size])</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Question: why Adam requires data to be normalized?</span></span><br><span class="line">        <span class="comment"># self.logger.info("Training size: %d.", self.train_size)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, n_epochs=<span class="number">10</span>)</span>:</span></span><br><span class="line">        iter_per_epoch = int(np.ceil(self.train_size * <span class="number">1.</span> / self.batch_size))</span><br><span class="line">        print(<span class="string">"Iterations per epoch: %3.3f ~ %d."</span>, self.train_size * <span class="number">1.</span> / self.batch_size, iter_per_epoch)</span><br><span class="line">        self.iter_losses = np.zeros(n_epochs * iter_per_epoch)</span><br><span class="line">        self.epoch_losses = np.zeros(n_epochs)</span><br><span class="line"></span><br><span class="line">        self.loss_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">        n_iter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        learning_rate = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">            perm_idx = np.random.permutation(self.train_size - self.T)</span><br><span class="line">            j = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> j &lt; self.train_size:</span><br><span class="line">                batch_idx = perm_idx[j:(j + self.batch_size)]</span><br><span class="line">                X = np.zeros((len(batch_idx), self.T - <span class="number">1</span>, self.X.shape[<span class="number">1</span>]))</span><br><span class="line">                y_history = np.zeros((len(batch_idx), self.T - <span class="number">1</span>))</span><br><span class="line">                y_target = self.y[batch_idx + self.T]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(len(batch_idx)):</span><br><span class="line">                    X[k, :, :] = self.X[batch_idx[k] : (batch_idx[k] + self.T - <span class="number">1</span>), :]</span><br><span class="line">                    y_history[k, :] = self.y[batch_idx[k]: (batch_idx[k] + self.T - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">                loss = self.train_iteration(X, y_history, y_target)</span><br><span class="line">                self.iter_losses[int(i * iter_per_epoch + j / self.batch_size)] = loss</span><br><span class="line">                <span class="comment">#if (j / self.batch_size) % 50 == 0:</span></span><br><span class="line">                <span class="comment">#    self.logger.info("Epoch %d, Batch %d: loss = %3.3f.", i, j / self.batch_size, loss)</span></span><br><span class="line">                j += self.batch_size</span><br><span class="line">                n_iter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> n_iter % <span class="number">10000</span> == <span class="number">0</span> <span class="keyword">and</span> n_iter &gt; <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">for</span> param_group <span class="keyword">in</span> self.encoder_optimizer.param_groups:</span><br><span class="line">                        param_group[<span class="string">'lr'</span>] = param_group[<span class="string">'lr'</span>] * <span class="number">0.9</span></span><br><span class="line">                    <span class="keyword">for</span> param_group <span class="keyword">in</span> self.decoder_optimizer.param_groups:</span><br><span class="line">                        param_group[<span class="string">'lr'</span>] = param_group[<span class="string">'lr'</span>] * <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line">            self.epoch_losses[i] = np.mean(self.iter_losses[range(i * iter_per_epoch, (i + <span class="number">1</span>) * iter_per_epoch)])</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Epoch %d, loss: %3.3f."</span> % (i, self.epoch_losses[i]))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                y_train_pred = self.predict(on_train=<span class="literal">True</span>)  <span class="comment"># 28383</span></span><br><span class="line">                y_test_pred = self.predict(on_train=<span class="literal">False</span>)  <span class="comment"># 12168</span></span><br><span class="line">                y_pred = np.concatenate((y_train_pred, y_test_pred))    <span class="comment"># 40551</span></span><br><span class="line">                <span class="comment"># X (40560,)</span></span><br><span class="line">                <span class="comment"># y (40560,)</span></span><br><span class="line">                print(y_train_pred.shape, y_test_pred.shape, y_pred.shape)</span><br><span class="line">                print((self.y).shape,(self.X).shape)</span><br><span class="line">                <span class="comment"># (40560,) (40560, 81)</span></span><br><span class="line">                true = concatenate(((self.y).reshape(self.y.shape[<span class="number">0</span>], <span class="number">1</span>), self.X), axis=<span class="number">1</span>)</span><br><span class="line">                true = self.scaler.inverse_transform(true)</span><br><span class="line">                self.y = true[:, <span class="number">0</span>]</span><br><span class="line">                <span class="comment"># true [1,40560] len = 40560</span></span><br><span class="line">                print(self.T, len(y_train_pred) + self.T)</span><br><span class="line">                <span class="comment"># 10 28393</span></span><br><span class="line">                print(self.T + len(y_train_pred), len(self.y) + <span class="number">1</span>)</span><br><span class="line">                <span class="comment"># 28393 40561</span></span><br><span class="line">                <span class="comment"># y_train_pred = concatenate((y_train_pred.reshape(y_train_pred.shape[0], 1), self.X[self.T-1: len(y_train_pred) + self.T-1]), axis=1)</span></span><br><span class="line">                y_train_pred = concatenate((y_train_pred.reshape(y_train_pred.shape[<span class="number">0</span>], <span class="number">1</span>),</span><br><span class="line">                                            self.X[: len(y_train_pred)]), axis=<span class="number">1</span>)</span><br><span class="line">                y_train_pred = self.scaler.inverse_transform(y_train_pred)</span><br><span class="line">                y_train_pred = y_train_pred[:, <span class="number">0</span>]</span><br><span class="line">                <span class="comment"># y_train_pred [10,28392] len = 28383</span></span><br><span class="line">                <span class="comment"># y_test_pred = concatenate((y_test_pred.reshape(y_test_pred.shape[0], 1), self.X[self.T + len(y_train_pred)-1:]), axis=1)</span></span><br><span class="line">                y_test_pred = concatenate(</span><br><span class="line">                    (y_test_pred.reshape(y_test_pred.shape[<span class="number">0</span>], <span class="number">1</span>), self.X[len(y_train_pred):len(y_train_pred)+len(y_test_pred)]), axis=<span class="number">1</span>)</span><br><span class="line">                y_test_pred = self.scaler.inverse_transform(y_test_pred)</span><br><span class="line">                y_test_pred = y_test_pred[:, <span class="number">0</span>]</span><br><span class="line">                <span class="comment"># y_test_pred [28393,40560] len = 12168</span></span><br><span class="line">                plt.figure()</span><br><span class="line">                plt.plot(range(<span class="number">1</span>, <span class="number">1</span> + len(self.y)), self.y, label=<span class="string">"True"</span>)</span><br><span class="line">                plt.plot(range(self.T, len(y_train_pred) + self.T), y_train_pred, label = <span class="string">'Predicted - Train'</span>)</span><br><span class="line">                plt.plot(range(self.T + len(y_train_pred), len(self.y) + <span class="number">1</span>), y_test_pred, label = <span class="string">'Predicted - Test'</span>)</span><br><span class="line">                plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">                plt.savefig(<span class="string">'./resultpic/epoch_%d.jpg'</span> % i)</span><br><span class="line">                plt.show()</span><br><span class="line"></span><br><span class="line">        y_train_pred = self.predict(on_train=<span class="literal">True</span>)</span><br><span class="line">        y_test_pred = self.predict(on_train=<span class="literal">False</span>)</span><br><span class="line">        y_pred = np.concatenate((y_train_pred, y_test_pred))</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.plot(range(<span class="number">1</span>, <span class="number">1</span> + len(self.y)), self.y, label=<span class="string">"True"</span>)</span><br><span class="line">        plt.plot(range(self.T, len(y_train_pred) + self.T), y_train_pred, label=<span class="string">'Predicted - Train'</span>)</span><br><span class="line">        plt.plot(range(self.T + len(y_train_pred), len(self.y) + <span class="number">1</span>), y_test_pred, label=<span class="string">'Predicted - Test'</span>)</span><br><span class="line">        plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">        plt.savefig(<span class="string">'./resultpic/final.jpg'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_iteration</span><span class="params">(self, X, y_history, y_target)</span>:</span></span><br><span class="line">        self.encoder_optimizer.zero_grad()</span><br><span class="line">        self.decoder_optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        input_weighted, input_encoded = self.encoder(Variable(torch.from_numpy(X).type(torch.FloatTensor).to(device)))</span><br><span class="line">        y_pred = self.decoder(input_encoded, Variable(torch.from_numpy(y_history).type(torch.FloatTensor).to(device)))</span><br><span class="line">        y_pred = y_pred.view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># print('y_pred', y_pred.shape)</span></span><br><span class="line">        y_true = Variable(torch.from_numpy(y_target).type(torch.FloatTensor).to(device))</span><br><span class="line">        <span class="comment"># print('y_true', y_true.shape)</span></span><br><span class="line">        loss = self.loss_func(y_pred, y_true)</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        self.encoder_optimizer.step()</span><br><span class="line">        self.decoder_optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, on_train = False)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> on_train:</span><br><span class="line">            y_pred = np.zeros(self.train_size - self.T + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_pred = np.zeros(self.X.shape[<span class="number">0</span>] - self.train_size)</span><br><span class="line"></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; len(y_pred):</span><br><span class="line">            batch_idx = np.array(range(len(y_pred)))[i : (i + self.batch_size)]</span><br><span class="line">            X = np.zeros((len(batch_idx), self.T - <span class="number">1</span>, self.X.shape[<span class="number">1</span>]))</span><br><span class="line">            y_history = np.zeros((len(batch_idx), self.T - <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(batch_idx)):</span><br><span class="line">                <span class="keyword">if</span> on_train:</span><br><span class="line">                    X[j, :, :] = self.X[range(batch_idx[j], batch_idx[j] + self.T - <span class="number">1</span>), :]</span><br><span class="line">                    y_history[j, :] = self.y[range(batch_idx[j],  batch_idx[j]+ self.T - <span class="number">1</span>)]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    X[j, :, :] = self.X[range(batch_idx[j] + self.train_size - self.T, batch_idx[j] + self.train_size - <span class="number">1</span>), :]</span><br><span class="line">                    y_history[j, :] = self.y[range(batch_idx[j] + self.train_size - self.T,  batch_idx[j]+ self.train_size - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">            y_history = Variable(torch.from_numpy(y_history).type(torch.FloatTensor).to(device))</span><br><span class="line">            _, input_encoded = self.encoder(Variable(torch.from_numpy(X).type(torch.FloatTensor).to(device)))</span><br><span class="line">            y_pred[i:(i + self.batch_size)] = self.decoder(input_encoded, y_history).cpu().data.numpy()[:, <span class="number">0</span>]</span><br><span class="line">            i += self.batch_size</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">io_dir = <span class="string">'nasdaq100_padding.csv'</span></span><br><span class="line"></span><br><span class="line">model = da_rnn(file_data=<span class="string">'&#123;&#125;'</span>.format(io_dir), parallel=<span class="literal">False</span>, learning_rate=<span class="number">.001</span>)</span><br><span class="line"></span><br><span class="line">model.train(n_epochs=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">y_pred = model.predict()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.semilogy(range(len(model.iter_losses)), model.iter_losses)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.semilogy(range(len(model.epoch_losses)), model.epoch_losses)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(y_pred, label = <span class="string">'Predicted'</span>)</span><br><span class="line">plt.plot(model.y[model.train_size:], label = <span class="string">"True"</span>)</span><br><span class="line">plt.legend(loc = <span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Bugs fix for&lt;br&gt;&lt;a href=&quot;https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py&quot;&gt;https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Learn-Pytorch-Variable</title>
    <link href="http://arithmeticjia.github.io/2019/12/10/Learn-Pytorch-Variable/"/>
    <id>http://arithmeticjia.github.io/2019/12/10/Learn-Pytorch-Variable/</id>
    <published>2019-12-10T11:01:51.000Z</published>
    <updated>2019-12-10T11:14:08.493Z</updated>
    
    <content type="html"><![CDATA[<p>Tensoræ˜¯Pytorchçš„ä¸€ä¸ªå®Œç¾ç»„ä»¶(å¯ä»¥ç”Ÿæˆé«˜ç»´æ•°ç»„)ï¼Œä½†æ˜¯è¦æ„å»ºç¥ç»ç½‘ç»œè¿˜æ˜¯è¿œè¿œä¸å¤Ÿçš„ï¼Œæˆ‘ä»¬éœ€è¦èƒ½å¤Ÿè®¡ç®—å›¾çš„Tensorï¼Œé‚£å°±æ˜¯Variableã€‚Variableæ˜¯å¯¹Tensorçš„ä¸€ä¸ªå°è£…ï¼Œæ“ä½œå’ŒTensoræ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯æ¯ä¸ªVariableéƒ½æœ‰ä¸‰ä¸ªå±æ€§ï¼ŒVaribaleçš„Tensoræœ¬èº«çš„.dataï¼Œå¯¹åº”Tensorçš„æ¢¯åº¦.gradï¼Œä»¥åŠè¿™ä¸ªVariableæ˜¯é€šè¿‡ä»€ä¹ˆæ–¹å¼å¾—åˆ°çš„.grad_fn<br><a id="more"></a></p><h4 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h4><p>autograd.Variable æ˜¯åŒ…çš„æ ¸å¿ƒç±». å®ƒåŒ…è£…äº†å¼ é‡, å¹¶ä¸”æ”¯æŒå‡ ä¹æ‰€æœ‰çš„æ“ä½œ. ä¸€æ—¦ä½ å®Œæˆäº†ä½ çš„è®¡ç®—, ä½ å°±å¯ä»¥è°ƒç”¨ .backward() æ–¹æ³•, ç„¶åæ‰€æœ‰çš„æ¢¯åº¦è®¡ç®—ä¼šè‡ªåŠ¨è¿›è¡Œ.ä½ è¿˜å¯ä»¥é€šè¿‡ .data å±æ€§æ¥è®¿é—®åŸå§‹çš„å¼ é‡, è€Œå…³äºè¯¥ variableï¼ˆå˜é‡ï¼‰çš„æ¢¯åº¦ä¼šè¢«ç´¯è®¡åˆ° .gradä¸Šå».è¿˜æœ‰ä¸€ä¸ªé’ˆå¯¹è‡ªåŠ¨æ±‚å¯¼å®ç°æ¥è¯´éå¸¸é‡è¦çš„ç±» - Function.Variable å’Œ Function æ˜¯ç›¸äº’è”ç³»çš„, å¹¶ä¸”å®ƒä»¬æ„å»ºäº†ä¸€ä¸ªéå¾ªç¯çš„å›¾, ç¼–ç äº†ä¸€ä¸ªå®Œæ•´çš„è®¡ç®—å†å²ä¿¡æ¯. æ¯ä¸€ä¸ª variableï¼ˆå˜é‡ï¼‰éƒ½æœ‰ä¸€ä¸ª .grad_fn å±æ€§, å®ƒå¼•ç”¨äº†ä¸€ä¸ªå·²ç»åˆ›å»ºäº† Variable çš„ Function ï¼ˆé™¤äº†ç”¨æˆ·åˆ›å»ºçš„ Variable <code>ä¹‹å¤– - å®ƒä»¬çš„</code>grad_fn is None ï¼‰.å¦‚æœä½ æƒ³è®¡ç®—å¯¼æ•°, ä½ å¯ä»¥åœ¨ Variable ä¸Šè°ƒç”¨ .backward() æ–¹æ³•. å¦‚æœ Variable æ˜¯æ ‡é‡çš„å½¢å¼ï¼ˆä¾‹å¦‚, å®ƒåŒ…å«ä¸€ä¸ªå…ƒç´ æ•°æ®ï¼‰, ä½ ä¸å¿…æŒ‡å®šä»»ä½•å‚æ•°ç»™ backward(), ä½†æ˜¯, å¦‚æœå®ƒæœ‰æ›´å¤šçš„å…ƒç´ . ä½ éœ€è¦å»æŒ‡å®šä¸€ä¸ª grad_output å‚æ•°, è¯¥å‚æ•°æ˜¯ä¸€ä¸ªåŒ¹é… shapeï¼ˆå½¢çŠ¶ï¼‰çš„å¼ é‡.</p><h4 id="åˆ›å»ºä¸€ä¸ª2Ã—2çš„å˜é‡"><a href="#åˆ›å»ºä¸€ä¸ª2Ã—2çš„å˜é‡" class="headerlink" title="åˆ›å»ºä¸€ä¸ª2Ã—2çš„å˜é‡"></a>åˆ›å»ºä¸€ä¸ª2Ã—2çš„å˜é‡</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">View more, visit my tutorial page: https://arithmeticjia.github.io</span></span><br><span class="line"><span class="string">My Blog: https://www.guanacossj.com</span></span><br><span class="line"><span class="string">Dependencies:</span></span><br><span class="line"><span class="string">torch: 1.3.0</span></span><br><span class="line"><span class="string">matplotlib</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># Variable in torch is to build a computational graph,</span></span><br><span class="line"><span class="comment"># but this graph is dynamic compared with a static graph in Tensorflow or Theano.</span></span><br><span class="line"><span class="comment"># So torch does not have placeholder, torch can just pass variable to the computational graph.</span></span><br><span class="line"></span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])            <span class="comment"># build a tensor</span></span><br><span class="line">variable = Variable(tensor, requires_grad=<span class="literal">True</span>)      <span class="comment"># build a variable, usually for compute gradients</span></span><br><span class="line"></span><br><span class="line">print(tensor)       <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br><span class="line">print(variable)     <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]])</span><br><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]], requires_grad=True)</span><br></pre></td></tr></table></figure><h4 id="è®¡ç®—å˜é‡çš„ç‚¹ä¹˜ç§¯ã€æ¢¯åº¦"><a href="#è®¡ç®—å˜é‡çš„ç‚¹ä¹˜ç§¯ã€æ¢¯åº¦" class="headerlink" title="è®¡ç®—å˜é‡çš„ç‚¹ä¹˜ç§¯ã€æ¢¯åº¦"></a>è®¡ç®—å˜é‡çš„ç‚¹ä¹˜ç§¯ã€æ¢¯åº¦</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">View more, visit my tutorial page: https://arithmeticjia.github.io</span></span><br><span class="line"><span class="string">My Blog: https://www.guanacossj.com</span></span><br><span class="line"><span class="string">Dependencies:</span></span><br><span class="line"><span class="string">torch: 1.3.0</span></span><br><span class="line"><span class="string">matplotlib</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># Variable in torch is to build a computational graph,</span></span><br><span class="line"><span class="comment"># but this graph is dynamic compared with a static graph in Tensorflow or Theano.</span></span><br><span class="line"><span class="comment"># So torch does not have placeholder, torch can just pass variable to the computational graph.</span></span><br><span class="line"></span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])            <span class="comment"># build a tensor</span></span><br><span class="line">variable = Variable(tensor, requires_grad=<span class="literal">True</span>)      <span class="comment"># build a variable, usually for compute gradients</span></span><br><span class="line"></span><br><span class="line">print(tensor)       <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br><span class="line">print(variable)     <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># till now the tensor and variable seem the same.</span></span><br><span class="line"><span class="comment"># However, the variable is a part of the graph, it's a part of the auto-gradient.</span></span><br><span class="line"></span><br><span class="line">t_out = torch.mean(tensor*tensor)       <span class="comment"># x^2</span></span><br><span class="line">v_out = torch.mean(variable*variable)   <span class="comment"># x^2</span></span><br><span class="line">print(t_out)</span><br><span class="line">print(v_out)                            <span class="comment"># 7.5</span></span><br><span class="line"></span><br><span class="line">print(variable*variable)</span><br><span class="line"><span class="comment"># ç‚¹ä¹˜æ“ä½œ</span></span><br><span class="line">print(torch.mm(variable,variable))</span><br><span class="line"><span class="comment"># çŸ©é˜µç›¸ä¹˜</span></span><br><span class="line">v_out.backward()    <span class="comment"># backpropagation from v_out</span></span><br><span class="line"><span class="comment"># v_out = 1/4 * sum(variable*variable)</span></span><br><span class="line"><span class="comment"># the gradients w.r.t the variable, d(v_out)/d(variable) = 1/4*2*variable = variable/2</span></span><br><span class="line">print(variable.grad)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"> 0.5000  1.0000</span></span><br><span class="line"><span class="string"> 1.5000  2.0000</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]])</span><br><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]], requires_grad=True)</span><br><span class="line">tensor(7.5000)</span><br><span class="line">tensor(7.5000, grad_fn=&lt;MeanBackward0&gt;)</span><br><span class="line">tensor([[ 1.,  4.],</span><br><span class="line">        [ 9., 16.]], grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line">tensor([[ 7., 10.],</span><br><span class="line">        [15., 22.]], grad_fn=&lt;MmBackward&gt;)</span><br><span class="line">tensor([[0.5000, 1.0000],</span><br><span class="line">        [1.5000, 2.0000]])</span><br></pre></td></tr></table></figure><p>æ³¨æ„è¿™é‡Œçš„å˜é‡çš„ç‚¹ä¹˜å’Œç›¸ä¹˜çš„åŒºåˆ«</p><h4 id="æŸ¥çœ‹å˜é‡çš„æ•°æ®"><a href="#æŸ¥çœ‹å˜é‡çš„æ•°æ®" class="headerlink" title="æŸ¥çœ‹å˜é‡çš„æ•°æ®"></a>æŸ¥çœ‹å˜é‡çš„æ•°æ®</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">print(variable)     <span class="comment"># this is data in variable format</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">print(variable.data)    <span class="comment"># this is data in tensor format</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]], requires_grad=True)</span><br><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]])</span><br></pre></td></tr></table></figure><h4 id="Variableè½¬numpy"><a href="#Variableè½¬numpy" class="headerlink" title="Variableè½¬numpy"></a>Variableè½¬numpy</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(variable.data.numpy())    <span class="comment"># numpy format</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[ 1.  2.]</span></span><br><span class="line"><span class="string"> [ 3.  4.]]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[1. 2.]</span><br><span class="line"> [3. 4.]]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Tensoræ˜¯Pytorchçš„ä¸€ä¸ªå®Œç¾ç»„ä»¶(å¯ä»¥ç”Ÿæˆé«˜ç»´æ•°ç»„)ï¼Œä½†æ˜¯è¦æ„å»ºç¥ç»ç½‘ç»œè¿˜æ˜¯è¿œè¿œä¸å¤Ÿçš„ï¼Œæˆ‘ä»¬éœ€è¦èƒ½å¤Ÿè®¡ç®—å›¾çš„Tensorï¼Œé‚£å°±æ˜¯Variableã€‚Variableæ˜¯å¯¹Tensorçš„ä¸€ä¸ªå°è£…ï¼Œæ“ä½œå’ŒTensoræ˜¯ä¸€æ ·çš„ï¼Œä½†æ˜¯æ¯ä¸ªVariableéƒ½æœ‰ä¸‰ä¸ªå±æ€§ï¼ŒVaribaleçš„Tensoræœ¬èº«çš„.dataï¼Œå¯¹åº”Tensorçš„æ¢¯åº¦.gradï¼Œä»¥åŠè¿™ä¸ªVariableæ˜¯é€šè¿‡ä»€ä¹ˆæ–¹å¼å¾—åˆ°çš„.grad_fn&lt;br&gt;
    
    </summary>
    
    
      <category term="Pytorch" scheme="http://arithmeticjia.github.io/categories/Pytorch/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://arithmeticjia.github.io/tags/pytorch/"/>
    
  </entry>
  
</feed>
