<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>请叫我算术嘉的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://arithmeticjia.github.io/"/>
  <updated>2020-05-12T01:31:42.940Z</updated>
  <id>http://arithmeticjia.github.io/</id>
  
  <author>
    <name>请叫我算术嘉</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CSRF-跨站点请求伪造攻击</title>
    <link href="http://arithmeticjia.github.io/2020/05/12/CSRF-%E8%B7%A8%E7%AB%99%E7%82%B9%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0%E6%94%BB%E5%87%BB/"/>
    <id>http://arithmeticjia.github.io/2020/05/12/CSRF-%E8%B7%A8%E7%AB%99%E7%82%B9%E8%AF%B7%E6%B1%82%E4%BC%AA%E9%80%A0%E6%94%BB%E5%87%BB/</id>
    <published>2020-05-12T01:13:00.000Z</published>
    <updated>2020-05-12T01:31:42.940Z</updated>
    
    <content type="html"><![CDATA[<meta name="referrer" content="no-referrer" /><p>什么是CSRF跨站点请求伪造(Cross—Site Request Forgery)<br><a id="more"></a></p><h3 id="CSRF攻击原理"><a href="#CSRF攻击原理" class="headerlink" title="CSRF攻击原理"></a>CSRF攻击原理</h3><p>1、用户浏览并登录信任网站A<br>2、A验证通过，在用户处产生A的cookie<br>3、用户在没有登出A网站的情况下，访问危险网站B<br>4、这时候B(危险的)要求访问A(安全的)，发出一个请求<br>5、这时候，浏览器会携带(2)的cookie访问A<br>6、虽然A不知道这个请求是来自B发出的还是用户在浏览器发出的，但是，由于请求携带了cookie，所以A认为这是一个正常的请求</p><h3 id="CSRF防御策略"><a href="#CSRF防御策略" class="headerlink" title="CSRF防御策略"></a>CSRF防御策略</h3><p>方法一、Token 验证：（用的最多）<br>（1）服务器发送给客户端一个token(注意，这个token是不存在cookie中的)；<br>（2）客户端提交的表单中带着这个token。<br>（3）如果这个 token 不合法，那么服务器拒绝这个请求。<br>在Django中，服务器在页面返回\{\% csrf_token \%\}，每次表单提交都必须携带</p><p>方法二、隐藏令牌：<br>把 token 隐藏在 http 的 head头中。<br>方法二和方法一有点像，本质上没有太大区别，只是使用方式上有区别。</p><p>方法三、Referer 验证：<br>Referer 指的是页面请求来源。意思是，只接受本站的请求，服务器才做响应；如果不是，就拦截。</p><p>根据 HTTP 协议，在 HTTP 头中有一个字段叫 Referer，它记录了该 HTTP 请求的来源地址。在通常情况下，访问一个安全受限页面的请求来自于同一个网站，比如需要访问 <a href="http://bank.example/withdraw?account=bob&amp;amount=1000000&amp;for=Mallory，用户必须先登陆" target="_blank" rel="noopener">http://bank.example/withdraw?account=bob&amp;amount=1000000&amp;for=Mallory，用户必须先登陆</a> bank.example，然后通过点击页面上的按钮来触发转账事件。这时，该转帐请求的 Referer 值就会是转账按钮所在的页面的 URL，通常是以 bank.example 域名开头的地址。而如果黑客要对银行网站实施 CSRF 攻击，他只能在他自己的网站构造请求，当用户通过黑客的网站发送请求到银行时，该请求的 Referer 是指向黑客自己的网站。因此，要防御 CSRF 攻击，银行网站只需要对于每一个转账请求验证其 Referer 值，如果是以 bank.example 开头的域名，则说明该请求是来自银行网站自己的请求，是合法的。如果 Referer 是其他网站的话，则有可能是黑客的 CSRF 攻击，拒绝该请求。</p><p>这种方法的显而易见的好处就是简单易行，网站的普通开发人员不需要操心 CSRF 的漏洞，只需要在最后给所有安全敏感的请求统一增加一个拦截器来检查 Referer 的值就可以。特别是对于当前现有的系统，不需要改变当前系统的任何已有代码和逻辑，没有风险，非常便捷。</p><p>然而，这种方法并非万无一失。Referer 的值是由浏览器提供的，虽然 HTTP 协议上有明确的要求，但是每个浏览器对于 Referer 的具体实现可能有差别，并不能保证浏览器自身没有安全漏洞。使用验证 Referer 值的方法，就是把安全性都依赖于第三方（即浏览器）来保障，从理论上来讲，这样并不安全。事实上，对于某些浏览器，比如 IE6 或 FF2，目前已经有一些方法可以篡改 Referer 值。如果 bank.example 网站支持 IE6 浏览器，黑客完全可以把用户浏览器的 Referer 值设为以 bank.example 域名开头的地址，这样就可以通过验证，从而进行 CSRF 攻击。</p><p>即便是使用最新的浏览器，黑客无法篡改 Referer 值，这种方法仍然有问题。因为 Referer 值会记录下用户的访问来源，有些用户认为这样会侵犯到他们自己的隐私权，特别是有些组织担心 Referer 值会把组织内网中的某些信息泄露到外网中。因此，用户自己可以设置浏览器使其在发送请求时不再提供 Referer。当他们正常访问银行网站时，网站会因为请求没有 Referer 值而认为是 CSRF 攻击，拒绝合法用户的访问。</p>]]></content>
    
    <summary type="html">
    
      &lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot; /&gt;

&lt;p&gt;什么是CSRF跨站点请求伪造(Cross—Site Request Forgery)&lt;br&gt;
    
    </summary>
    
    
      <category term="WEB安全" scheme="http://arithmeticjia.github.io/categories/WEB%E5%AE%89%E5%85%A8/"/>
    
    
      <category term="csrf" scheme="http://arithmeticjia.github.io/tags/csrf/"/>
    
  </entry>
  
  <entry>
    <title>Gibbs-Sampling</title>
    <link href="http://arithmeticjia.github.io/2020/05/11/Gibbs-Sampling/"/>
    <id>http://arithmeticjia.github.io/2020/05/11/Gibbs-Sampling/</id>
    <published>2020-05-11T13:18:37.000Z</published>
    <updated>2020-05-11T15:08:24.470Z</updated>
    
    <content type="html"><![CDATA[<p><meta name="referrer" content="no-referrer"/><br>Gibbs Sampling<br><a id="more"></a></p><p>假设二维场景下，状态(x, y)转移到(x’, y’)，可以分为三种场景</p><ul><li>平行于y轴转移，如上图中从状态A转移到状态B</li><li>平行于x轴转移，如上图中从状态A转移到状态C</li><li>其他情况转移，如上图从状态A转移到状态D</li></ul><p><img src="https://img2018.cnblogs.com/blog/452535/201810/452535-20181005001150218-101842018.png" alt=""></p><p>A-&gt;B:</p><script type="math/tex; mode=display">p(x_{1},y_{1})p(y_{2}|x_{1}) = p(x_{1})p(y_{1}|x_{1})p(y_{2}|x_{1})</script><p>B-&gt;A:</p><script type="math/tex; mode=display">p(x_{1},y_{2})p(y_{1}|x_{1}) = p(x_{1})p(y_{2}|x_{1})p(y_{1}|x_{1})</script><p>即:</p><script type="math/tex; mode=display">P(A)p(y_{2}|x_{1}) = P(B)p(y_{1}|x_{1})</script><p>同理，对于场景二:</p><script type="math/tex; mode=display">P(A)p(x_{2}|y_{1}) = P(C)p(x_{1}|y_{1})</script><p>对于场景三，规定不允许转移</p><script type="math/tex; mode=display">P(A) * 0 = P(D) * 0</script><p>实际上，从状态A转移到状态D可以通过一次场景一转移和一次场景二转移得到。所以即使规定A到D的转移概率为0，也满足A到D可以经过有限次转移达到。<br>于是，我们可以构造二维平面上任意两点之间的转移概率矩阵Q:</p><script type="math/tex; mode=display">Q(A->B) = p(y_{B}|x_{1})---x_{A} = x_{B} = x_{1}</script><script type="math/tex; mode=display">Q(A->C) = p(x_{C}|y_{1})---y_{A} = y_{C} = y_{1}</script><script type="math/tex; mode=display">Q(A->D) = 0---其他</script><p>这里的$y_{B}$就是$y_{2}$,$x_{C}$就是$x_{2}$,由此可得:</p><script type="math/tex; mode=display">p(A)Q(A->B) = p(B)Q(B->A)</script><script type="math/tex; mode=display">p(A)Q(A->C) = p(C)Q(C->A)</script><p>对于这样的概率转移Q，很容易验证对于平面上任意两点X和Y，满足细致平稳条件:</p><script type="math/tex; mode=display">p(X)P(X->Y) = p(Y)P(Y->X)</script><p><strong><em>什么是细致平稳条件？</em></strong></p><p>假设向量v:</p><script type="math/tex; mode=display">v = [0.6,0.4]</script><p>和一个概率转移矩阵P:</p><script type="math/tex; mode=display">P = \begin{bmatrix} 0.7&0.3 \\  0,8& 0.2\end{bmatrix}</script><p>当v与n个P相乘，n趋于无穷大时，发现最后得到的向量会收敛到一个稳定值:</p><script type="math/tex; mode=display">\lim_{n->\infty }vp^{n} = [0.73,0.27]</script><p>代码验证:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">v = np.array([<span class="number">0.6</span>, <span class="number">0.4</span>])</span><br><span class="line"></span><br><span class="line">P = np.array([[<span class="number">0.7</span>, <span class="number">0.3</span>],[<span class="number">0.8</span>, <span class="number">0.2</span>]])</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    v = np.dot(v, P)</span><br><span class="line">    <span class="keyword">print</span> v</span><br><span class="line"></span><br><span class="line">Out:</span><br><span class="line">...</span><br><span class="line">[<span class="number">0.72727273</span> <span class="number">0.27272727</span>]</span><br><span class="line">[<span class="number">0.72727273</span> <span class="number">0.27272727</span>]</span><br></pre></td></tr></table></figure><br>细致平衡条件（Detailed Balance Condition）:给定一个马尔科夫链，分布π和概率转移矩阵P，如果下面等式成立</p><script type="math/tex; mode=display">π_{i}P_{ij} = π_{j}P_{ji}</script><p>则此马尔科夫链具有一个平稳分布（Stationary Distribution）π</p><p>回到上面，直观理解就是<strong>从X点走到Y点，需要沿着坐标轴轮换着走若干步，其路径就是一条折线</strong><br>最后这个二维空间的马氏链会收敛到平稳分布p(x,y)</p><p>最后，Gibbs Sampling算法的大概流程</p><ul><li>随机初始化$X_{0}=x_{0}$,$Y_{0}=y_{0}$</li><li>对t=0,1,2,…,循环采样<br>  1、$y_{t+1}$ ~ p(y|x_{t})<br>  2、$x_{t+1}$ ~ p(x|y_{t+1})</li></ul><p>马氏链的转换就是轮换着沿着x轴、y轴不断走折线，得到样本(x_{0},y_{0}),(x_{0},y_{1}),(x_{1},y_{1}),(x_{1},y_{2}),…,直到马氏链收敛，也就是平稳。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;/&gt;&lt;br&gt;Gibbs Sampling&lt;br&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="http://arithmeticjia.github.io/categories/NLP/"/>
    
    
      <category term="nlp" scheme="http://arithmeticjia.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>Topic-Compositional-Neural-Language-Model</title>
    <link href="http://arithmeticjia.github.io/2020/05/11/Topic-Compositional-Neural-Language-Model/"/>
    <id>http://arithmeticjia.github.io/2020/05/11/Topic-Compositional-Neural-Language-Model/</id>
    <published>2020-05-11T12:20:26.000Z</published>
    <updated>2020-05-12T15:10:36.810Z</updated>
    
    <content type="html"><![CDATA[<p>Topic Compositional Neural Language Model<br><a id="more"></a></p><h3 id="Preliminaries"><a href="#Preliminaries" class="headerlink" title="Preliminaries"></a>Preliminaries</h3><ul><li>RNN-based language model<script type="math/tex; mode=display">V(vocabulary set): \{y_{1},...,y_{M}\}</script><script type="math/tex; mode=display">p(y_{1},...,y_{M}) = p(y_{1})\prod_{m=2}^{M}p(y_{m}|y_{1:m-1})</script></li></ul><p>把$y_{1:m}$个单词作为隐藏态</p><script type="math/tex; mode=display">p(y_{m}|y_{1:m-1}) = p(y_{m}|h_{m})</script><p>根据RNN，可得</p><script type="math/tex; mode=display">h_{m} = f(h_{m-1},x_{m})</script><ul><li>Topic Model</li></ul><p>LDA model</p><h3 id="Topic-Compositional-Neural-Language-Model"><a href="#Topic-Compositional-Neural-Language-Model" class="headerlink" title="Topic Compositional Neural Language Model"></a>Topic Compositional Neural Language Model</h3><h4 id="neural-topic-model-神经主题模型"><a href="#neural-topic-model-神经主题模型" class="headerlink" title="neural topic model(神经主题模型)"></a>neural topic model(神经主题模型)</h4><p>capture the long-range semantic meanings across the document(捕获文档中的远距离语义)</p><ul><li>d:bag-of-words</li><li>D:vocabulary size</li></ul><p>pass a Gaussian random vector through a softmax function to parameterize the multinomial document topic distributions(通过一个softmax函数传递高斯随机向量来参数化多项式文档主题分布)</p><h4 id="neural-language-model-神经语言模型"><a href="#neural-language-model-神经语言模型" class="headerlink" title="neural language model(神经语言模型)"></a>neural language model(神经语言模型)</h4><p>learn the local semantic and syntactic relationships between words(学习单词之间的本地语义和句法关系)</p><ul><li>Mixture-of-Experts (MoE) language model</li><li>expert networks($E_{1}$,$E_{2}$,…,$E_{T}$) - RNN</li></ul><script type="math/tex; mode=display">p(y_{m}) = \sum_{k=1}^{T} t_{k} * softmax(Vh_{m}^{(k)})</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Topic Compositional Neural Language Model&lt;br&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="http://arithmeticjia.github.io/categories/NLP/"/>
    
    
      <category term="nlp" scheme="http://arithmeticjia.github.io/tags/nlp/"/>
    
  </entry>
  
  <entry>
    <title>TopicRNN</title>
    <link href="http://arithmeticjia.github.io/2020/04/21/TopicRNN/"/>
    <id>http://arithmeticjia.github.io/2020/04/21/TopicRNN/</id>
    <published>2020-04-21T11:22:42.000Z</published>
    <updated>2020-04-22T06:17:35.670Z</updated>
    
    <content type="html"><![CDATA[<p>A RECURRENT NEURAL NETWORK WITH LONG-RANGE SEMANTIC DEPENDENCY<br><a id="more"></a></p><h3 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a>INTRODUCTION</h3><ul><li>提出了TopicRNN语言模型，通过潜在的主题捕获长距离的语义信息。</li><li>这些topic提供context信息给RNN，以这样的方式考虑句子（文章）的上下文信息。</li></ul><h3 id="TopicRNN-Model"><a href="#TopicRNN-Model" class="headerlink" title="TopicRNN-Model"></a>TopicRNN-Model</h3><ul><li>首先得到主题向量$\theta ~ N(0,I$(高斯分布)</li><li>给定$y_{1:t-1}$，对于文档中的第$t$个词</li><li>计算隐藏态$h_{t}=f_{W}(x_{t},h_{t-1})$</li></ul><p>自从Neural Language Model（NLM）流行以来，期望能够把NLM和话题模型（Topic Model）进行结合的想法就屡见不鲜。这篇论文也是这个方向的一次尝试。NLM的主要优势是在句子以下的结构上对字句进行建模，而话题模型则往往能够在真个文档甚至更高的层次上对文本的语义进行建模。把这两者结合起来就是想利用这两方面的优势。在这篇文章里，话题模型通过Variational Autoencoder的框架来捕捉到文档的话题（Topic）隐变量。之后，这个变量成为了对不同的语言模型进行加权的权重，而语言文字的产生则利用了Mixture-of-Experts的框架来对不同的RNN语言模型进行整合。需要注意的是，在这篇文章提出的方法里，话题模型对文字的整体数据和语言模型对单独的字句都进行了建模，也就是说，一个文档分别有两个产生过程，一个针对全局文字，一个针对有顺序的字句。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A RECURRENT NEURAL NETWORK WITH LONG-RANGE SEMANTIC DEPENDENCY&lt;br&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="http://arithmeticjia.github.io/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>VAE-变分自动编码器</title>
    <link href="http://arithmeticjia.github.io/2020/04/20/VAE-%E5%8F%98%E5%88%86%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8/"/>
    <id>http://arithmeticjia.github.io/2020/04/20/VAE-%E5%8F%98%E5%88%86%E8%87%AA%E5%8A%A8%E7%BC%96%E7%A0%81%E5%99%A8/</id>
    <published>2020-04-20T07:20:14.000Z</published>
    <updated>2020-05-11T15:14:14.919Z</updated>
    
    <content type="html"><![CDATA[<p><meta name="referrer" content="no-referrer" /><br>Variational Auto-Encoder<br><a id="more"></a></p><h3 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h3><h4 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h4><script type="math/tex; mode=display">I(x) = -logp(x)</script><p>p(x) 为事件x发生的概率，当log底数为e时，信息量的单位为nat（奈特），当log底数为2时，信息量的单位为bit（比特）。</p><h4 id="信息熵"><a href="#信息熵" class="headerlink" title="信息熵"></a>信息熵</h4><p>表示随机变量x在离散和连续情况下的信息熵H:</p><script type="math/tex; mode=display">H = \sum -\log p(x) * p(x)</script><script type="math/tex; mode=display">H = \int -\log p(x) * p(x)d(x)</script><h3 id="K-L散度（Kullback-Leibler-divergence）"><a href="#K-L散度（Kullback-Leibler-divergence）" class="headerlink" title="K-L散度（Kullback-Leibler divergence）"></a>K-L散度（Kullback-Leibler divergence）</h3><p>K-L散度又被称为相对熵（relative entropy），是对两个概率分布间差异的非对称性度量。</p><h3 id="贝叶斯公式（Bayes-Rule）"><a href="#贝叶斯公式（Bayes-Rule）" class="headerlink" title="贝叶斯公式（Bayes Rule）"></a>贝叶斯公式（Bayes Rule）</h3><script type="math/tex; mode=display">p(z|x) = \frac{p(z,x)}{p(x)} = \frac{p(x|z)p(z)}{p(x)}</script><h3 id="AE-自编码器"><a href="#AE-自编码器" class="headerlink" title="AE-自编码器"></a>AE-自编码器</h3><p>encoder-decoder结构</p><p><img src="https://pic3.zhimg.com/80/v2-8ae0e598375aeeed45488edd064e1cfa_1440w.jpg" alt=""></p><h3 id="VAE-变分自编码器"><a href="#VAE-变分自编码器" class="headerlink" title="VAE-变分自编码器"></a>VAE-变分自编码器</h3><p>假设给定样本x，希望在给定x的条件下推出z的分布，即p(z|x)</p><p>根据贝叶斯公式:</p><script type="math/tex; mode=display">p(z|x) = \frac{p(z,x)}{p(x)} = \frac{p(x|z)p(z)}{p(x)}</script><p>因为无法得知p(x),使用q(z|x)去近似p(z|x),满足</p><script type="math/tex; mode=display">minKL(q(z|x)||p(z|x))</script><script type="math/tex; mode=display">KL(q(z|x)||p(z|x)) = \int q(z|x)log\frac{q(z|x)}{p(z|x)}d(z) = \int q(z|x)log\frac{q(z|x)}{\frac{p(x|z)p(z)}{p(x)}}d(z)</script><script type="math/tex; mode=display">= \int q(z|x)logq(z|x)d(z) + \int q(z|x)logp(x)d(z) - \int q(z|x)logp(x|z)p(z)d(z)</script><p>显然</p><script type="math/tex; mode=display">\int q(z|x)d(z) = 1</script><script type="math/tex; mode=display">= logp(x) + \int q(z|x)logq(z|x)d(z) - \int q(z|x)logp(x|z)p(z)d(z)</script><p>p(x)为定值，因此</p><script type="math/tex; mode=display">minKL(q(z|x)||p(z|x)) = > min(L) = min(\int q(z|x)logq(z|x)d(z) - \int q(z|x)logp(x|z)p(z)d(z))</script><script type="math/tex; mode=display">= \int q(z|x)logq(z|x)d(z) - \int q(z|x)logp(x|z)d(z) - \int q(z|x)logp(z)d(z)</script><script type="math/tex; mode=display">= \int q(z|x)log\frac{q(z|x)}{p(z)}d(z) - \int q(z|x)logp(x|z)d(z)</script><script type="math/tex; mode=display">= KL(q(z|x)||p(z)) - E_{z服从q(z|x)}[logp(x|z)]</script><ul><li>给定真实样本$X_{k}$,假设存在专属$X_{k}$的分布$p(Z|X_{k})$,目的是训练生成器$X=g(Z)$，把从分布$p(Z|X_{k})$中采样的$Z_{k}$还原为$X_{k}$</li><li>给每个$X_{k}$配上专属的正态分布，使用神经网络训练均值和方差$\mu_{k}$和$\sigma^{2}_{k}$,此时我知道了$X_{k}$的均值和方差,然后从专属分布$p(Z|X_{k})$中采样出$Z_{k}$,然后经过生成器得到$\hat{X} = g(Z_{k})$,最小化$D(\hat{X},X_{k})$,同时每个$P(Z|X)$趋向$N~(0,1)$正态分布</li><li>对于两个损失的比例问题，原论文直接计算了各分量独立的正态分布和标准正态分布的KL散度<script type="math/tex; mode=display">KL(N(\mu,\sigma^{2})||N(0,1))</script></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot; /&gt;&lt;br&gt;Variational Auto-Encoder&lt;br&gt;
    
    </summary>
    
    
      <category term="NLP" scheme="http://arithmeticjia.github.io/categories/NLP/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-04-17周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/04/17/2020-04-17%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/04/17/2020-04-17%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-04-17T10:50:20.000Z</published>
    <updated>2020-04-17T10:51:48.719Z</updated>
    
    <content type="html"><![CDATA[<p>不知道说啥<br><a id="more"></a></p><p>1、堆叠LSTM</p><p><img src="https://upload-images.jianshu.io/upload_images/7311123-f4f81fb3930f84a0.png?imageMogr2/auto-orient/strip|imageView2/2/w/640/format/webp.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不知道说啥&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-04-03周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/04/02/2020-04-03%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/04/02/2020-04-03%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-04-02T09:08:46.000Z</published>
    <updated>2020-04-03T11:50:56.026Z</updated>
    
    <content type="html"><![CDATA[<p>A Memory-Network Based Solution for Multivariate Time-Series Forecasting<br><a id="more"></a></p><h3 id="By-the-way"><a href="#By-the-way" class="headerlink" title="By the way"></a>By the way</h3><p>Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks(SIGIR2018)</p><ul><li>LSTNet</li><li>利用数据中的周期模式，GRU中计算$t$时刻的隐向量时，不是以$t-1$时刻的隐向量为输入，而是以$t-p$时刻的隐向量为输入($p$为周期)</li></ul><p><img src="https://pic2.zhimg.com/80/v2-2b3ff23e31f2a6fbc85058f5754b2c09_1440w.jpg" alt=""></p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><ul><li>一种基于深度学习的时间序列预测模型MTNet</li><li>MTNet由一个大的内存组件、三个独立的编码器和一个联合训练的自回归组件组成</li><li>可解释性</li></ul><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><ul><li><p>DA-RNN不考虑外源数据不同成分间的空间相关性。更重要的是，在第二阶段DA-RNN进行的点式注意可能不合适捕捉连续的周期模式</p></li><li><p>认为LSTNet网络为了考虑周期信息，需要引入超参$p$，在周期长度会发生变化的环境中，这个超参p是未知的</p></li></ul><h3 id="Framework"><a href="#Framework" class="headerlink" title="Framework"></a>Framework</h3><h4 id="Problem-Formulation"><a href="#Problem-Formulation" class="headerlink" title="Problem Formulation"></a>Problem Formulation</h4><script type="math/tex; mode=display">Y = {y_{1},y_{2},...,y_{T}},y_{t} \in R^{D}</script><p>D表示变量的维度</p><h4 id="Memory-Time-series-Network"><a href="#Memory-Time-series-Network" class="headerlink" title="Memory Time-series Network"></a>Memory Time-series Network</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/1585821126570.jpg" alt=""></p><p>long-term time series historical data</p><p>{$X_{i}$} = <script type="math/tex">X_{1},...,X_{n}</script></p><p>that are to be store in the memory</p><p>$Q$<br>a short-term historical time series data</p><p><strong>这里的{$X_{i}$}和Q不重叠</strong></p><p>Encoder Architecture</p><p>使用非池化的卷积层来提取时间维度上的短期模式和变量之间的局部依赖关系</p><script type="math/tex; mode=display">X \in T × D</script><p>卷积层由多个内核组成，尺寸都是w × D，所以最后的卷积结果就是(T-w+1) × 1</p><p>一共有$d_{c}$个过滤器（卷积核），得到了</p><script type="math/tex; mode=display">d_{c} × T_{c}</script><p>矩阵</p><p>加上attention层，再使用RNN(GRU)</p><p>首先，一条时序数据被分为长期的历史数据{$X_{i}$}和最近的历史数据$Q$ ，{$X_{i}$}和$Q$没有重合部分。<br>每一个X_{i}通过$Encoder_{m}$网络得到表示$m_{i}$ ；$Q$通过$Encoder_{in}$网络得到表示$u$。<br>其中$Encoder_{m}$和$Encoder_{in}$的结构都为上节所述的 Encoder 网络。</p><p>将$u$和每一个$m_{i}$做内积，在通过Softmax函数归一化，得到一系列的权重$p_{i}$。</p><script type="math/tex; mode=display">p_{i} = Softmax(u^{T}m_{i})</script><p><strong>本文的亮点是attention的设计，即权重$p_{i}$的得到。权重$p_{i}$越大表示Q和X_{i}位置之前的一段序列的相似程度大，则该段序列对于当前的预测更重要</strong></p><p>每一个$X_{i}$再通过$Encoder_{c}$网络得到表示$c_{i}$。每一个$c_{i}$再和$p_{i}$相乘得到$o_{i}$。</p><script type="math/tex; mode=display">o_{i} = p_{i} * c_{i}</script><p>将所有的$o_{i}$和$u$拼接后通过W矩阵转换，得到最终的预测输出：</p><script type="math/tex; mode=display">y_{t}^{D} = W^{D}[u;o_{1};o_{2};...;o_{T}] + b</script><p>Autoregressive Component（自回归模型）</p><script type="math/tex; mode=display">y_{t,i}^{L} = \sum_{k=0}^{s^{ar}-1}w_{k}^{ar}q_{t-k,i}+b^{ar}</script><p>The final prediction of MTNet</p><script type="math/tex; mode=display">y_{t} = y_{t}^{D} + y_{t}^{L}</script><p>将非线性（MTNet 中的神经网络部分）和线性（MTNet 中的自回归部分）模型进行ensemble。得到最终的预测输出。</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h4 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h4><p>2 univariate datasets:  Beijing PM2.51, GEFCom(2014) Electricity Price (Hong et al. 2016),</p><p>4 multivariate datasets: Traffic, Solar-Energy, Electricity,Exchange-Rate. </p><h4 id="Methods-for-comparison"><a href="#Methods-for-comparison" class="headerlink" title="Methods for comparison"></a>Methods for comparison</h4><h4 id="Metrics"><a href="#Metrics" class="headerlink" title="Metrics"></a>Metrics</h4><p>univariate: Root Mean Squared Error (RMSE)/Mean Absolute Error (MAE)</p><p>multivariate: Root Relative Squared Error (RRSE)/Empirical Correlation Coefficient (CORR)</p><h4 id="Interpretability-of-MTNet"><a href="#Interpretability-of-MTNet" class="headerlink" title="Interpretability of MTNet"></a>Interpretability of MTNet</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/1585836468820.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;A Memory-Network Based Solution for Multivariate Time-Series Forecasting&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-3-27周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/03/26/2020-3-27%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/03/26/2020-3-27%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-03-26T09:05:17.000Z</published>
    <updated>2020-03-27T12:17:38.751Z</updated>
    
    <content type="html"><![CDATA[<p>PTMs-Pre-trained Models,PTMs(预训练模型)<br><a id="more"></a></p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>在图像领域，预训练过程是一个比较常规的做法，对于图像来说一般是CNN的多层叠加网络结构。一般步骤如下：</p><ul><li>先现在一个大数据集A上预训练模型，模型参数保存下来，设为Model_A</li><li>对于target任务B，在Model_A的基础上进行再训练Model_B，这时候一般策略是使用Model_A的底层网络参数初始化Model_B，上层参数随机初始化并训练</li></ul><p>对于底层参数，一般有两种策略：</p><ul><li>一种是Frozen，即训练Model_B过程中底层网络参数设为不可训练的，直接使用Model_A训练好的参数</li><li>另一种是Fine-tuning，即训练Model_B过程中底层网络参数设为可训练的，在Model_A训练好的参数基础上微调</li></ul><p>在NLP领域，使用word embedding</p><p>Word2vec—-&gt;ELMO(Embedding from Language Models)—-&gt;GPT(Generative Pre-Training)—-&gt;BERT(Bidirectional Encoder Representations from Transformers)</p><h3 id="word-embedding（词嵌入）"><a href="#word-embedding（词嵌入）" class="headerlink" title="word embedding（词嵌入）"></a>word embedding（词嵌入）</h3><p>高维词向量嵌入到一个低维空间</p><p>Embedding是数学领域的有名词，是指某个对象 X 被嵌入到另外一个对象 Y 中，映射 f : X → Y </p><p>Word Embedding 是NLP中一组语言模型和特征学习技术的总称，把词汇表中的单词或者短语映射成由实数构成的向量上(映射)</p><h4 id="One-Hot-Representation"><a href="#One-Hot-Representation" class="headerlink" title="One-Hot-Representation"></a>One-Hot-Representation</h4><p><img src="https://pic2.zhimg.com/80/v2-09e1bda72c4b903e25db203ab4aa6dc6_1440w.jpg" alt=""></p><p>在one hot representation编码的每个单词都是一个维度，彼此independent</p><p>语料库<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">John likes to watch movies.Marry likes too.</span><br><span class="line">John also likes to watch football games.</span><br></pre></td></tr></table></figure><br>词典<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "John": 1，</span><br><span class="line">    "likes": 2,</span><br><span class="line">    "to": 3,</span><br><span class="line">    "watch": 4,</span><br><span class="line">    "movies": 5,</span><br><span class="line">    "also": 6,</span><br><span class="line">    "football": 7, </span><br><span class="line">    "games": 8,</span><br><span class="line">    "Marry": 9,</span><br><span class="line">    "too": 10</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>one-hot表示<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">John:[1,0,0,0,0,0,0,0,0,0]</span><br><span class="line">likes:[0,1,0,0,0,0,0,0,0,0]</span><br><span class="line">...</span><br><span class="line">too:[0,0,0,0,0,0,0,0,0,1]</span><br></pre></td></tr></table></figure><br>再举个例子</p><p><img src="https://img-blog.csdnimg.cn/20190807181106401.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI2ODExMzc3,size_16,color_FFFFFF,t_70" alt=""><br>一共有四种状态，1，2，3，4<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1 -&gt; 0001</span><br><span class="line">2 -&gt; 0010</span><br><span class="line">3 -&gt; 0100</span><br><span class="line">4 -&gt; 1000</span><br></pre></td></tr></table></figure><br><img src="https://img-blog.csdnimg.cn/20190808002943747.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NpbmF0XzI2ODExMzc3,size_16,color_FFFFFF,t_70" alt=""></p><ul><li>无法捕捉两个word之间的关系，也就是没有办法捕捉语义信息</li><li>词向量可能非常长</li></ul><h4 id="Distributed-Representation"><a href="#Distributed-Representation" class="headerlink" title="Distributed-Representation"></a>Distributed-Representation</h4><h4 id="N-Gram"><a href="#N-Gram" class="headerlink" title="N-Gram"></a>N-Gram</h4><p>N-Gram是基于一个假设：第n个词出现与前n-1个词相关，而与其他任何词不相关</p><script type="math/tex; mode=display">S = (w_{1},w_{2},w_{3},...,w_{n})</script><p>假设每一个单词$w_{i}$都要依赖于第一个单词到$w_{1}$到他之前的一个单词$w_{i-1}$的影响</p><script type="math/tex; mode=display">p(S) = p(w_{1},w_{2},w_{3},...,w_{n})=p(w_{1})p(w_{2}|w_{1})...p(w_{n}|w_{n-1}w_{n-2}...w_{1})</script><p>不妨利用马尔科夫假设<br>即当前这个词仅仅跟前面几个有限的词相关，因此也就不必追溯到最开始的那个词，这样便可以大幅缩减上述算式的长度</p><script type="math/tex; mode=display">p(S) = p(w_{1},w_{2},w_{3},...,w_{n})=\prod p(w_{i}|w_{i-1}...w_{1})≈\prod p(w_{i}|w_{i-1}...w_{i-N+1})</script><p>当N=2时，称为Bi-Gram<br>当N=3时，称为Tri-Gram<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">I am John</span><br><span class="line">John I am</span><br><span class="line">I like learning</span><br></pre></td></tr></table></figure><br>此时Tri-Gram下</p><script type="math/tex; mode=display">p(am|I) = 2/3</script><p>两个重要应用场景</p><ul><li>评估句子之间差异性</li><li>评估一个句子是否合理<br>N-Gram距离<script type="math/tex; mode=display">s = "ABCD"</script><script type="math/tex; mode=display">t = "ABC"</script>当N=2时，第一个字符串可以拆成<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(AB,BC,CD)</span><br></pre></td></tr></table></figure>第二个字符串可以拆成<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(AB,BC)</span><br></pre></td></tr></table></figure>距离公式<script type="math/tex; mode=display">|G_{N}(s)| + |G_{N}(t)| - 2 \times |G_{N}(s) \cap G_{N}(t)|</script>d = 4 + 3 - 2 * 1 = 1</li></ul><h3 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h3><p>预备知识回顾</p><h4 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h4><script type="math/tex; mode=display">\sigma (x) = \frac{1}{1+e^{-x}}</script><p><img src="https://www.guanacossj.com/media/articlebodypics/sigmoid.jpg" alt=""><br>求导</p><script type="math/tex; mode=display">\sigma^{'} (x) = \sigma (x)[1-\sigma (x)]</script><p>易得</p><script type="math/tex; mode=display">[log\sigma (x)]^{'} = 1 - \sigma (x)</script><script type="math/tex; mode=display">[log(1-\sigma (x))]^{'} = - \sigma (x)</script><h4 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h4><script type="math/tex; mode=display">P(A|B) = \frac {P(A,B)}{P(B)}</script><script type="math/tex; mode=display">P(B|A) = \frac {P(A,B)}{P(A)}</script><script type="math/tex; mode=display">P(A|B) = P(A) \frac {P(B|A)}{P(A)}</script><h4 id="逻辑回归二分类器"><a href="#逻辑回归二分类器" class="headerlink" title="逻辑回归二分类器"></a>逻辑回归二分类器</h4><p>设</p><script type="math/tex; mode=display">\{\{x_{i},y_{i}\}\}_{i=1}^{m}</script><p>二分类函数长这样</p><script type="math/tex; mode=display">h_{\theta }(x) = \sigma (\theta _{0}+\theta _{1}x_{1}+\theta _{1}x_{1}+...+\theta _{n}x_{n})</script><p>令</p><script type="math/tex; mode=display">\theta =(\theta_{0},\theta_{1},\theta_{2},...,\theta_{n})^{T}</script><p>其中θ为待定参数</p><p>简化二分类函数</p><script type="math/tex; mode=display">h_{\theta }(x) = \sigma (\theta ^{T}x)=\frac{1}{1+e^{-\theta ^{T}x}}</script><p>取阈值T=0.5</p><h4 id="Huffman树-编码"><a href="#Huffman树-编码" class="headerlink" title="Huffman树-编码"></a>Huffman树-编码</h4><p>最优二叉树—-带权路径长度最短的二叉树</p><p>“我”，”喜欢”，”观看”，”巴西”，”足球”，”世界杯”<br> 15     8      6      5      3       1</p><p>选根节点最小的树合并</p><p><img src="https://www.guanacossj.com/media/articlebodypics/1585241496316.jpg" alt=""></p><p>词频越大的词离根节点越近<br>显然词频越小，权重越小</p><p><img src="https://www.guanacossj.com/media/articlebodypics/1585241408762.jpg" alt=""></p><h4 id="Skip-gram"><a href="#Skip-gram" class="headerlink" title="Skip-gram"></a>Skip-gram</h4><p>使用一个词预测上下文</p><h4 id="CBOW-Continues-Bag-of-Words-Model"><a href="#CBOW-Continues-Bag-of-Words-Model" class="headerlink" title="CBOW-Continues Bag-of-Words Model"></a>CBOW-Continues Bag-of-Words Model</h4><p>使用一个词语的上下文作为输入，来预测这个词语本身</p><p><img src="https://www.guanacossj.com/media/articlebodypics/1585297454343.jpg" alt=""></p><p><strong>输入层到隐藏层</strong></p><p>输入层是四个词的one-hot向量表示，分别是$x_{t-2}$,$x_{t-1}$,$x_{t+1}$,$x_{t+2}$，维度都是V×1，V是模型的训练本文中所有词的个数</p><p>输入层到隐藏层的权重矩阵为W，维度为V×d，d是认为给定的词向量维度，隐藏层的向量为h，维度是d×1</p><script type="math/tex; mode=display">h = \frac{W*x_{t-2}+W*x_{t-1}+W*x_{t+1}+W*x_{t+2}}{4}</script><p><strong>隐藏层到输出层</strong></p><p>记隐藏层到输出层的权重矩阵为U，维度为d×V，输出向量为y，维度为V×1，那么</p><script type="math/tex; mode=display">y = softmax(U^{T}*h)</script><p>此时输出层的向量y和输入层的向量x，虽然维度一样，但是y并不是one-hot向量，假设训练样本是</p><p>“I like to eat apple”，此时用”I”,”like”,”eat”,”apple”预测”to”，输出的y向量大概是这样</p><p><img src="https://pic4.zhimg.com/80/v2-918b97c077fe15b4a67e0afddb62bfa3_1440w.jpg" alt=""></p><p>目的是构造最大化函数L</p><script type="math/tex; mode=display">L = \prod_{t=1}^{V}p(w_{t}|w_{t-k},w_{t-k+1},...,w_{t-1},w_{t+1},...,w_{t+k-1},w_{t+k})</script><h4 id="层次softmax和负采样"><a href="#层次softmax和负采样" class="headerlink" title="层次softmax和负采样"></a>层次softmax和负采样</h4><p>层次softmax是一棵huffman树，树的叶子节点是训练文本中所有的词，非叶子节点都是一个逻辑回归二分类器，每个逻辑回归分类器的参数都不同，分别用$θ_{*}$表示。</p><p><img src="https://www.guanacossj.com/media/articlebodypics/1585301521653.jpg" alt=""></p><p>分类器的输入是向量h(隐藏层向量)</p><p>采样到 I 的概率$p(I|context) = (1-\sigma(\theta_{1}h)) * (1-\sigma(\theta_{3}h))$</p><p>采样到 eat 的概率$p(eat|context) = (1-\sigma(\theta_{1}h)) * \sigma(\theta_{3}h)$</p><p>采样到 to 的概率$p(to|context) = \sigma(\theta_{1}h) * (1-\sigma(\theta_{2}h))$</p><p>正样本</p><script type="math/tex; mode=display">p(w|context(w)) = \sigma(\theta^{w}h^{context(w})</script><p>负样本</p><script type="math/tex; mode=display">p(w|NEG(w)) = \sigma(\theta^{w}h^{NEG(w})</script><script type="math/tex; mode=display">L_{CBOW} = \prod_{t=1}^{V} p(w^{t}|context(w^{t}))p(w^{t}|NEG(w^{t}))</script><p>最大化—-&gt;梯度上升法</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PTMs-Pre-trained Models,PTMs(预训练模型)&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-3-20周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/03/20/2020-3-20%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/03/20/2020-3-20%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-03-20T09:08:59.000Z</published>
    <updated>2020-03-20T09:40:19.918Z</updated>
    
    <content type="html"><![CDATA[<p>searching and mining trillions of time series subsquences under dynamic time warping<br><a id="more"></a></p><h3 id="Assumptions"><a href="#Assumptions" class="headerlink" title="Assumptions"></a>Assumptions</h3><ul><li>Time Series Subsequences must be Normalized （时间序列子序列必须经过归一化处理）</li><li>Dynamic Time Warping is the Best Measure</li><li>Arbitrary Query Lengths cannot be Indexed</li><li>There Exists Data Mining Problems that we are Willing to Wait Some Hours to Answer</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;searching and mining trillions of time series subsquences under dynamic time warping&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-3-13周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/03/10/2020-3-13%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/03/10/2020-3-13%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-03-10T13:13:01.000Z</published>
    <updated>2020-03-13T05:36:21.778Z</updated>
    
    <content type="html"><![CDATA[<p>DTW(Dynamic Time Warping)<br>动态时间规整<br><a id="more"></a></p><h3 id="DTW-Dynamic-Time-Warping"><a href="#DTW-Dynamic-Time-Warping" class="headerlink" title="DTW(Dynamic Time Warping)"></a>DTW(Dynamic Time Warping)</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/dtw01.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/dtw02.jpg" alt=""></p><p>假设有两个序列：</p><p>a = [2, 0, 1, 1, 2, 4, 2, 1, 2, 0]</p><p>b = [1, 1, 2, 4, 2, 1, 2, 0]</p><p>用欧式距离计算出每序列的每两点之间的距离</p><figure class="highlight shell"><figcaption><span>script</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[[1. 1. 0. 2. 0. 1. 0. 2.]</span><br><span class="line"> [1. 1. 2. 4. 2. 1. 2. 0.]</span><br><span class="line"> [0. 0. 1. 3. 1. 0. 1. 1.]</span><br><span class="line"> [0. 0. 1. 3. 1. 0. 1. 1.]</span><br><span class="line"> [1. 1. 0. 2. 0. 1. 0. 2.]</span><br><span class="line"> [3. 3. 2. 0. 2. 3. 2. 4.]</span><br><span class="line"> [1. 1. 0. 2. 0. 1. 0. 2.]</span><br><span class="line"> [0. 0. 1. 3. 1. 0. 1. 1.]</span><br><span class="line"> [1. 1. 0. 2. 0. 1. 0. 2.]</span><br><span class="line"> [1. 1. 2. 4. 2. 1. 2. 0.]]</span><br></pre></td></tr></table></figure><p>waring path W</p><p>$W = w_{1},w_{2},w_{3},…,w_{k}$ </p><p>$max(m,n) &lt;= k &lt; m+n-1$</p><p>$w_{k} = (i,j)_{k}$</p><script type="math/tex; mode=display">DTW(Q,C) = \min (\frac{\sum_{k=1}^{K}w_{k}}{K})</script><p>分母中的K主要是用来对不同的长度的规整路径做补偿</p><p>采用动态规划算法。假设我们要求到位置(𝑖,𝑗)的最小累计距离𝐷(𝑖,𝑗)，那么它只能由𝐷(𝑖−1,𝑗)，𝐷(𝑖,𝑗−1)和𝐷(𝑖−1,𝑗−1)这三个位置的最小累计距离中寻找，也就是</p><script type="math/tex; mode=display">𝐷(i,j)=d_{i,j}+𝑚𝑖𝑛[𝐷(i−1,j),𝐷(i,j−1),𝐷(i−1,j−1)]</script><p><img src="https://www.guanacossj.com/media/articlebodypics/dtw.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>DTW方法是欧氏距离方法的改进，只改进了其不能处理local time shifting的问题。没有引入任何阈值参数，因此对时间上的偏移（噪声和离群点）的抑制并不好，且对时间上的偏移的适应性也不好。</p><p>优点：使用动态规划的思想，实现了对某些点的重复使用，确保重复使用的点达成的路径最优的，从而较为高效地解决了数据不对齐的问题。</p><p>缺点：还是无法处理离群点、异常点，对于噪声的抑制没有进行处理。虽然能够处理local time shifting，但是对时间上的偏移做的也不好。算法也不是metric类型的。</p><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DTW(Dynamic Time Warping)&lt;br&gt;动态时间规整&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-3.6周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/03/06/2020-3-6%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/03/06/2020-3-6%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-03-06T04:34:50.000Z</published>
    <updated>2020-03-11T08:49:17.306Z</updated>
    
    <content type="html"><![CDATA[<p>Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models<br><a id="more"></a></p><h3 id="Shape-and-Time-Distortion-Loss-for-Training-DeepTime-Series-Forecasting-Models"><a href="#Shape-and-Time-Distortion-Loss-for-Training-DeepTime-Series-Forecasting-Models" class="headerlink" title="Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models"></a>Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models</h3><p>NeurIPS 2019</p><p>训练深度时间序列预测模型的形状和时间失真损失</p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>This paper addresses the problem of time series forecasting for non-stationary signals and multiple future steps prediction. </p><p>DILATE (DIstortion Loss including shApe and TimE) 形状和时间失真损失</p><p>DILATE aims at accurately predicting sudden changes, and explicitly incorporates two terms supporting precise shape and temporal change detection.</p><p><img src="https://img-blog.csdn.net/20180824212209631?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMxODIxNjc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><p><img src="https://img-blog.csdn.net/20180824212233242?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMxODIxNjc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><p><img src="https://img-blog.csdn.net/2018082421225311?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzMxODIxNjc1/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>Time series forecasting [6] consists in analyzing the dynamics and correlations between historical data for predicting future behavior</p><p>In one-step prediction problems [39, 30], future prediction reduces to a single scalar value. This is in sharp contrast with multi-step time series prediction [49, 2, 48], which consists in predicting a complete trajectory[trəˈdʒektəri] of future data at a rather long temporal extent. Multi-step forecasting thus requires to accurately describe time series evolution.</p><p><img src="https://pic1.zhimg.com/v2-b872cd50b4a341901005bf4246493fa0_r.jpg" alt=""></p><p>(a) Non informative prediction 非信息性预测<br>(b) Correct shape, time delay<br>(c) Correct time, inaccurate shape</p><p>In contrast, the DILATE loss proposed in this work, which disentangles shape and temporal decay terms,<br>supports predictions (b) and (c) over prediction (a) that does not capture the sharp change of regime.</p><h4 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h4><p>Time series forecasting Traditional methods for time series forecasting include linear autoregressive models, such as the ARIMA model [6], and Exponential[ˌekspəˈnenʃl] Smoothing [27], which both fall into the broad category of linear State Space Models (SSMs) [17].</p><h4 id="Training-Deep-Neural-Networks-with-DILATE-DIstortion-Loss-including-shApe-and-TimE"><a href="#Training-Deep-Neural-Networks-with-DILATE-DIstortion-Loss-including-shApe-and-TimE" class="headerlink" title="Training Deep Neural Networks with DILATE((DIstortion Loss including shApe and TimE))"></a>Training Deep Neural Networks with DILATE((DIstortion Loss including shApe and TimE))</h4><p><img src="https://pic3.zhimg.com/v2-fc51e16266d817369cbd3bcbd6624552_b.jpg" alt=""></p><p>a set of N input time series:</p><script type="math/tex; mode=display">A=\{ X_{i} \}_{i\in \{1:N\}}</script><p>对于</p><script type="math/tex; mode=display">x_{i} = (x_{i}^{1},...,x_{i}^{n})</script><p>predicts the future <strong>k-step</strong> ahead trajectory </p><script type="math/tex; mode=display">\hat{y}_{i} = (\hat{y}_{i}^{1},...,\hat{y}_{i}^{k})</script><p>actual ground truth future trajectory</p><script type="math/tex; mode=display">\dot{y}_{i} = (\dot{y}_{i}^{1},...,\dot{y}_{i}^{k})</script><p><img src="https://www.guanacossj.com/media/articlebodypics/1583471554346.jpg" alt=""></p><p><script type="math/tex">\alpha \in [0,1]</script>  hyper parameter [ˈhaɪpə(r) pəˈræmɪtə(r)] </p><p>Notations and definitions(符号和定义):</p><ul><li>A: a warping path as a binary matrix(二值矩阵)</li></ul><h4 id="Shape-and-temporal-terms"><a href="#Shape-and-temporal-terms" class="headerlink" title="Shape and temporal terms"></a>Shape and temporal terms</h4><h5 id="Shape-term"><a href="#Shape-term" class="headerlink" title="Shape term"></a>Shape term</h5><p>Shape term Our shape loss function is based on the Dynamic Time Warping (DTW)</p><p>The DTW loss focuses on the structural shape dissimilarity between signals</p><p>Temporal term Our second term Ltemporal in Eq (1) aims at penalizing temporal distortions between $\hat{y}_{i}$ $\dot{y}_{i}$</p><h4 id="DILATE-Efficient-Forward-and-Backward-Implementation"><a href="#DILATE-Efficient-Forward-and-Backward-Implementation" class="headerlink" title="DILATE Efficient Forward and Backward Implementation"></a>DILATE Efficient Forward and Backward Implementation</h4><h4 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h4><p>To illustrate the relevance of DILATE, we carry out experiments on 3 non-stationary time series datasets from different domains </p><p><img src="https://pic3.zhimg.com/v2-b22b26e263592f889b52486b8c1f85ce_b.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-2-28周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/02/28/2020-2-28%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/02/28/2020-2-28%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-02-28T04:22:34.000Z</published>
    <updated>2020-02-28T06:32:20.573Z</updated>
    
    <content type="html"><![CDATA[<p>Memory In Memory（学习高阶非平稳特征信息）<br><a id="more"></a><br>Memory In Memory: A Predictive Neural Network for Learning Higher-Order Non-Stationarity from Spatiotemporal Dynamics<br>一种用于高阶学习的预测神经网络时空动力学中的非平稳性</p><p>cvpr2019</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>any non-stationary process can be decomposed into deterministic, time-variant polynomials[ˌpɒlɪˈnəʊmiəlz] , plus a zero-mean stochastic term. </p><p>任何一个非平稳过程都可以分解为：确定项+时间变量多项式+零均值随机项</p><p>By applying differencing operations appropriately, we may turn time-variant polynomials into a constant, making the deterministic[dɪˌtɜːmɪˈnɪstɪk] component predictable.</p><p>通过差分的操作，我们可以把时间变量多项式转换成一个常量，使确定性的组成部分可预测</p><p>We propose the Memory In Memory (MIM) networks and corresponding recurrent blocks for this purpose. The MIM blocks exploit the differential signals between adjacent recurrent states to model the non-stationary and approximately stationary properties in spatiotemporal dynamics with two cascaded, self-renewed memory modules.</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>We attempt to resolve this problem by proposing a generic RNNs architecture that is more effective in non-stationarity modeling. </p><p>In particular, the forget gates in the recent PredRNN model [32] does not work appropriately on precipitation forecasting: about 80% of them are saturated over all timestamps, implying almost timeinvariant memory state transitions. </p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h4 id="ARIMA-Autoregressive-Integrated-Moving-Average-Model"><a href="#ARIMA-Autoregressive-Integrated-Moving-Average-Model" class="headerlink" title="ARIMA(Autoregressive Integrated Moving Average Model)"></a>ARIMA(Autoregressive Integrated Moving Average Model)</h4><p>A time-series random variable whose power spectrum remains constant over time can be viewed as a combination of signal and noise. </p><p>功率谱是功率谱密度函数（PSD）的简称，它定义为单位频带内的信号功率</p><p><img src="https://www.guanacossj.com/media/articlebodypics/w.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/w_f.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/p.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/f.jpg" alt=""></p><script type="math/tex; mode=display">|S(f)|^2</script><script type="math/tex; mode=display">\lim_{T->\propto }\frac{1}{T}|S(f)|^2</script><h4 id="Deterministic-Spatiotemporal-Prediction"><a href="#Deterministic-Spatiotemporal-Prediction" class="headerlink" title="Deterministic Spatiotemporal Prediction"></a>Deterministic Spatiotemporal Prediction</h4><h4 id="Stochastic-Spatiotemporal-Prediction"><a href="#Stochastic-Spatiotemporal-Prediction" class="headerlink" title="Stochastic Spatiotemporal Prediction"></a>Stochastic Spatiotemporal Prediction</h4><h3 id="Memory-In-Memory"><a href="#Memory-In-Memory" class="headerlink" title="Memory In Memory"></a>Memory In Memory</h3><p><img src="https://pic1.zhimg.com/v2-dc4a2024ce0201315661daf3b43c6ab8_r.jpg" alt=""></p><p>左边是ST-LSTM结构，右边是更改的</p><p>ST-LSTM中的忘记门基本是饱和的，所以它基本上只获取了平稳的信息，而整个直接联系就是C状态值，再加上下面的输入为差分，而差分的转换其实就是非平稳的信息</p><p><img src="https://pic1.zhimg.com/v2-69b9793882f1a8739d6b1d37336cc808_r.jpg" alt=""></p><p><img src="https://pic3.zhimg.com/v2-f1eac55dddaa00ab46da1b4ea116072e_r.jpg" alt=""></p><p><img src="https://pic3.zhimg.com/80/v2-6ec88fa03dd40e82a037c13ff6b64bd2_1440w.jpg" alt=""></p><h3 id="Memory-In-Memory-Networks"><a href="#Memory-In-Memory-Networks" class="headerlink" title="Memory In Memory Networks"></a>Memory In Memory Networks</h3><p><img src="https://pic3.zhimg.com/v2-06741d37a4c01fce37ef43901f5b311e_r.jpg" alt=""></p><p>红色箭头：用于微分建模的H的对角状态转移路径</p><p>蓝色箭头：存储单元C，N和S的水平转换路径</p><p>黑色箭头：之字形状态</p><h3 id="experience"><a href="#experience" class="headerlink" title="experience"></a>experience</h3><p>模型参数：一共四层，第一层是ST-LSTM，其余三层为MIM，MIM的feature channel为64，利用l2损失，ADAM optimizer，lr为0.001，利用了两个trick，为layer nomalization和scheduled sampling</p><p><img src="https://pic3.zhimg.com/v2-fec039cb2962195deb890b6680c5395a_r.jpg" alt=""></p><h3 id="GluonTS-AWS"><a href="#GluonTS-AWS" class="headerlink" title="GluonTS(AWS)"></a>GluonTS(AWS)</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/gluonts_all.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/gluonts67.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Memory In Memory（学习高阶非平稳特征信息）&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-2-21周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/02/20/2020-2-21%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/02/20/2020-2-21%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-02-20T12:53:19.000Z</published>
    <updated>2020-02-21T07:20:17.207Z</updated>
    
    <content type="html"><![CDATA[<p>PredRNN++…<br><a id="more"></a></p><h3 id="PredRNN"><a href="#PredRNN" class="headerlink" title="PredRNN++:"></a>PredRNN++:</h3><p>Towards A Resolution of the Deep-in-Time Dilemma in Spatiotemporal Predictive Learning</p><p>旨在解决时空预测的深层次时间困境</p><p>ICML2018 Tsinghua</p><h4 id="PredRNN-1"><a href="#PredRNN-1" class="headerlink" title="PredRNN"></a>PredRNN</h4><p>nips2017 Tsinghua</p><p>PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs</p><p>用ST-LSTM的预测学习循环神经网络(spatial + temporal)</p><p>PredRNN利用了一种双重记忆机制，通过简单的门控级联，将水平更新的时间记忆C与垂直转换的空间记忆M结合起来</p><p>先来回忆一下LSTM</p><p><img src="https://pic2.zhimg.com/v2-810f2d553fa6e6f43854efdc881be8a1_r.jpg" alt=""></p><ol><li>$h_{t-1}$与$X_{t}$做concat操作，之后经过sigmoid形成[0, 1]的忘记门，输入门，输出门</li><li>Ct-1通过忘记门 </li><li>ht-1与Xt做concat操作通过tanh激活函数，通过输入门（这里在通过输入门之前相当于生成了此时的输入生成状态） </li><li>以上通过遗忘门和输入门的两个向量相加就是最后的Ct，也就是此时的cell state</li><li>最后，这个cell state通过再一次的非线性变化tanh 最终通过输出门输出得到最后的ht</li></ol><p>Spatiotemporal memory flow</p><p><img src="https://pic4.zhimg.com/80/v2-7c898aed50f1e1d9aee647e4c273ad33_hd.jpg" alt=""></p><p><img src="https://pic1.zhimg.com/80/v2-f5c836f08237baea9393aefce80d0fd8_hd.jpg" alt=""></p><p>缺点:</p><ol><li>去掉水平方向的时间流，会牺牲时间上的一致性，因为在同一层的不同时间没有时间流了。 </li><li>记忆需要在遥远的状态之间流动更长的路径，更容易造成梯度消失。 所以引入了一个新的building blocks为ST-LSTM。</li></ol><p>Spatiotemporal LSTM</p><p><img src="https://pic4.zhimg.com/v2-f3cd76086384ede22b29e6c8a5f7f45b_r.jpg" alt=""></p><p><img src="https://pic2.zhimg.com/80/v2-fdb1465c439cd9f3eea4ee52bf2b4125_hd.jpg" alt=""></p><p>震惊！！！</p><ul><li>上半部分就是LSTM(Standard Temporal Memory)</li><li>下半部分相当于把c和h一起更改为M，M即时空记忆状态(Spatiotemporal Memory)</li></ul><p><img src="https://pic2.zhimg.com/80/v2-bbe7560a50ff9d511746fb94562ebd39_hd.jpg" alt=""></p><h4 id="PredRNN-2"><a href="#PredRNN-2" class="headerlink" title="PredRNN++"></a>PredRNN++</h4><ul><li>Stacked ConvLSTMs(nips2015)</li><li>Deep Transition ConvLSTMs</li><li>Pred RNN 红线表示空间记忆的深度过渡路径，水平的黑色箭头表示时间记忆的更新方向<br><img src="https://img-blog.csdnimg.cn/20191222210421460.png?#pic_center" alt=""></li></ul><p>Causal LSTM(因果长短期记忆)<br>通过这种方式，将获得更强大的建模能力，以实现更强的空间相关性和短期动态</p><p><img src="https://img-blog.csdnimg.cn/20191222205916540.png?#pic_center" alt=""></p><ul><li>每个门不是由X和H决定，而是由X和H以及C决定，通过输入门之前的状态也是由三者决定的</li><li>两个memory结构，即C和M，C为temporal state，M为spatial state，因为输入C为上一个时刻的C，M是上一层的M，所以这里C与时间维度有关，M与空间维度有关</li><li>M作为第二部分的state输入，并且通过忘记门之前做了一个非线性操作tanh</li></ul><p>对比ST-LSTM来说，Causal LSTM对于M和H定义更加清晰，并且不是简单的concat，而是采用了一个递归深度更深的一个级联结构最终输出H</p><p>Gradient Highway(高速梯度)</p><p>通过Recurrent Highway Networks的思想能够证明高速网络能够有效的在非常深的网络中传递梯度，继而防止长时导致的梯度消失</p><p><img src="https://pic3.zhimg.com/80/v2-3b541bda171d5f4c9299c77326e13702_hd.jpg" alt=""></p><p><img src="https://pic1.zhimg.com/80/v2-cf06c70d336a89700435195a0574b39c_hd.jpg" alt=""></p><p>总体架构</p><p><img src="https://pic3.zhimg.com/80/v2-61d4c59ff38010cb9613574bf0290c9a_hd.jpg" alt=""></p><p>GHU连接了当前时刻以及前一个时刻的输入，引导的结果就是梯度不再是一股线传播了，而是可以直接在第一层与第二层之间有个高速的传播，换句话讲就是传播的距离缩短了，也就变得没有之前的那么’深‘了，可以有效的解决梯度消失的问题</p><h3 id="Block-Hankel-Tensor-ARIMA-for-Multiple-Short-Time-Series-Forecasting"><a href="#Block-Hankel-Tensor-ARIMA-for-Multiple-Short-Time-Series-Forecasting" class="headerlink" title="Block Hankel Tensor ARIMA for Multiple Short Time Series Forecasting"></a>Block Hankel Tensor ARIMA for Multiple Short Time Series Forecasting</h3><h4 id="Hankel-汉克尔矩阵（每一条逆对角线上的元素都相等的矩阵）"><a href="#Hankel-汉克尔矩阵（每一条逆对角线上的元素都相等的矩阵）" class="headerlink" title="Hankel:汉克尔矩阵（每一条逆对角线上的元素都相等的矩阵）"></a>Hankel:汉克尔矩阵（每一条逆对角线上的元素都相等的矩阵）</h4><script type="math/tex; mode=display">\begin{bmatrix} 1&  2&  3&  4&  5&  6& 7\\  2&  3&  4&  5&  6&  7& 8\\  3&  4&  5&  6&  7&  8& 9\end{bmatrix}</script><h4 id="Tucker分解"><a href="#Tucker分解" class="headerlink" title="Tucker分解"></a>Tucker分解</h4><p>这是一个三阶张量</p><p><img src="/Users/Arithmetic/Pictures/tensor.png" alt=""></p><p>秩一张量：如果一个K阶张量能够表示成K个向量的外积，那么该张量称为秩一张量<br>[[3 4],[6 8]]这个二阶张量可以表示为[1 2]○[3 4]的外积，那么这就是一个二阶秩一张量<br><img src="https://www.guanacossj.com/media/articlebodypics/1582207111495.jpg" alt=""></p><p>CP分解<br>CP分解其实就是多个rank-one tensors的和</p><p><img src="/Users/Arithmetic/Pictures/cp.png" alt=""></p><p>公式表示如下：</p><p><img src="/Users/Arithmetic/Pictures/cp_f.png" alt=""></p><p>tucker分解</p><p><img src="http://www.xiongfuli.com/assets/img/201606/tucker.png" alt=""></p><p><img src="/Users/Arithmetic/Pictures/tucker_f.png" alt=""></p><p>这里A$\in$R$^{I\times P}$,B$\in$R$^{J\times Q}$,C$\in$R$^{K\times R}$是因子矩阵（通常是正交的），可以当做是每一维上的主要成分。核张量表示每一维成分之间的联系<br>因此，对于一个三阶张量，可以通过tucker分解为三个二阶因子矩阵和一个三阶核向量</p><h4 id="Step1-Block-Hankel-Tensor-via-MDT"><a href="#Step1-Block-Hankel-Tensor-via-MDT" class="headerlink" title="Step1: Block Hankel Tensor via MDT"></a>Step1: Block Hankel Tensor via MDT</h4><p>MDT:multi-way delay embedding transform(多路延迟变换)<br>目的是利用MDT将多个TS转换成一个高阶的块Hankel张量<br>假设有1000条时间序列，每条序列的长度为40，即 I = 1000，T=40，设置参数t = 5<br>经MDT沿着时间维度变换后，得到一个1000✖️5*✖️（40-5+1）=1000✖️5✖️36的三维张量</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PredRNN++…&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-2.14周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/02/13/2020-2-14%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/02/13/2020-2-14%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-02-13T13:27:22.000Z</published>
    <updated>2020-02-14T07:33:34.201Z</updated>
    
    <content type="html"><![CDATA[<p>DA-RNN + Transformer + CW-RNN<br><a id="more"></a></p><h3 id="彩蛋"><a href="#彩蛋" class="headerlink" title="彩蛋"></a>彩蛋</h3><ul><li>写了一篇关于使用百度Echarts绘制新型冠状病毒全国分布图的博客</li><li>收到来自皖南医学院弋矶山医院教育处金来润的邮件希望合作</li><li>其实就是他们团队收集了一点疫情数据打算发一篇文章，希望我按照他们的要求给他们画个图</li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/QQ20200214-0.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/QQ20200214-1.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/echarts.png" alt=""></p><h3 id="DA-RNN"><a href="#DA-RNN" class="headerlink" title="DA-RNN"></a>DA-RNN</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-67-1.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-67-2.jpg" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.256</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.348</span><br></pre></td></tr></table></figure><h3 id="CW-RNN-Clock-Work-RNN"><a href="#CW-RNN-Clock-Work-RNN" class="headerlink" title="CW-RNN(Clock Work RNN)"></a>CW-RNN(Clock Work RNN)</h3><p>ICML2014</p><p>本文介绍了对标准RNN体系结构的一个简单而强大的改进，即时钟工作RNN（CW-RNN），它将隐藏层划分为不同的模块，每个处理以自己的时间粒度输入，仅以指定的时钟速率进行计算。<br>CW-RNN没有使标准RNN模型更加复杂，而是减少了RNN参数的数量，显著提高了测试任务的性能，加快了网络评估的速度</p><p>Input = ($x_{1}$,$x_{2}$,…,$x_{t}$,…)</p><p>Output = ($y_{1}$,$y_{2}$,…,$y_{t}$,…)</p><p><img src="https://www.guanacossj.com/media/articlebodypics/cwrnn.jpg" alt=""></p><ul><li>把隐含层节点分成了若干个模块（在图中分成了3个模块，是为了方便说明，实际中的模块个数可以自定义），而且每个模块都分配了一个时钟周期（Ti），便于独立管理</li><li>隐含层之间的连接，在一个模块内部是全连接，但是模块之间是有方向的。模块之间的连接是从高时钟频率的模块指向低时钟频率的模块</li><li>标准RNN<script type="math/tex; mode=display">y_{H}^{(t)} = f_{H}(W_{H}*y^{(t-1)}+W_{I}*x^{(t)})</script><script type="math/tex; mode=display">y_{O}^{(t)} = f_{O}(W_{O}*y_{H}^{(t)})</script></li><li>只有当$t$ MOD $T_{i}$ = 0时才会被执行</li><li>{$T_{1}$,…,$T_{g}$}的设置是任意的，在论文中使用$T_{i}=2^{i-1}$</li></ul><p>$T_{1}=1$,$T_{2}=2$,$T_{3}=4$,$T_{4}=8$,$T_{5}=16$,$T_{6}=32$,$T_{7}=64$</p><ul><li>分块<script type="math/tex; mode=display">W_{H} = \begin{pmatrix}\\ W_{H_{1}}\\ .\\ .\\ W_{H_{g}}\end{pmatrix}</script><script type="math/tex; mode=display">W_{I} = \begin{pmatrix}\\ W_{I_{1}}\\ .\\ .\\ W_{I_{g}}\end{pmatrix}</script></li><li>不参与运算的部分置零<script type="math/tex; mode=display">\left\{\begin{matrix}\\ W_{H_{i}},t mod T_{i} = 0\\ 0,otherwise\end{matrix}\right.</script></li><li>将$W_{h}$强制转成上三角<script type="math/tex; mode=display">W_{H_{i}}={0_{1},...,0_{i-1},W_{H_{i,i}},...,W_{H_{i,g}}}</script></li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/cwrnn-2.jpg" alt=""></p><p>我们要处理序列中第6（t=6）个元素的时候，通过t与每个模块的时钟周期进行MOD（求余数）计算后可以得到只有前两个模块会参与运算。所以$W_{h}$和$W_{x}$矩阵除了上面两行之外，其他元素的值都是0。经过计算之后，得到的$h_{t}$也只有前两个模块有值。因此，我们也可以把CW-RNN过程看成是通过一些人工的干预，选择不同的隐含层节点进行工作</p><p><img src="http://ir.dlut.edu.cn/Uploads/ue/image/20151201/6358457411082925904194442.jpg" alt=""></p><ul><li>低时钟速率模块处理、保留和输出从输入中获得的长期信息序列，</li><li>高时钟速率模块则侧重于本地的高频信息</li></ul><p>作者对比了传统RNN、LSTM、CW-RNN，在取局部图的时候可以观察到，LSTM的回归效果相对平滑，而CW-RNN并没有这种缺陷<br><img src="https://www.guanacossj.com/media/articlebodypics/comparecwrnn.jpg" alt=""></p><p>我复现了下股票数据集上的效果，实验还没完全完成<br><img src="https://www.guanacossj.com/media/articlebodypics/cwrnn-stock.jpg" alt=""></p><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/multi_head_net.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/transf-67-1.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/transf-67-2.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DA-RNN + Transformer + CW-RNN&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-1-17周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/01/17/2020-1-17%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/01/17/2020-1-17%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-01-17T08:38:22.000Z</published>
    <updated>2020-01-17T09:58:07.962Z</updated>
    
    <content type="html"><![CDATA[<p>Django+uwsgi+Nginx<br><a id="more"></a></p><h3 id="WSGI"><a href="#WSGI" class="headerlink" title="WSGI"></a>WSGI</h3><p>WSGI Web Server Gateway Interface</p><p><img src="https://pic1.zhimg.com/80/v2-6c4572c783816364f2569af961814430_hd.jpg" alt=""></p><p>WSGI是一种通信协议，WSGI 不是框架，也不是一个模块，而是介于 Web应用程序（Web框架）与 Web Server 之间交互的一种规范。</p><h3 id="uwsgi"><a href="#uwsgi" class="headerlink" title="uwsgi"></a>uwsgi</h3><ul><li>二进制协议，可以携带任何类型的数据。一个uwsgi分组的头4个字节描述了这个分组包含的数据类型。</li><li>uwsgi是一种线路协议而不是通信协议，在此常用于在uWSGI服务器与其他网络服务器的数据通信。</li></ul><h3 id="uWSGI"><a href="#uWSGI" class="headerlink" title="uWSGI"></a>uWSGI</h3><p>uWSGI是实现了uwsgi和WSGI两种协议的Web服务器，使用c语言开发。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> uwsgi</span><br></pre></td></tr></table></figure><ul><li>两级结构 在这种结构里，uWSGI作为服务器，它用到了HTTP协议以及wsgi协议，flask应用作为application，实现了wsgi协议。当有客户端发来请求，uWSGI接受请求，调用flask app得到相应，之后相应给客户端。 这里说一点，通常来说，Flask等web框架会自己附带一个wsgi服务器(这就是flask应用可以直接启动的原因)，但是这只是在开发阶段用到的，在生产环境是不够用的，所以用到了uwsgi这个性能高的wsgi服务器。</li><li>三级结构 在这种结构里，uWSGI作为中间件，它用到了uwsgi协议(与nginx通信)，wsgi协议(调用Flask app)。</li><li>提高web server性能(uWSGI处理静态资源不如nginx；nginx会在收到一个完整的http请求后再转发给wWSGI)。</li><li>nginx可以做负载均衡(前提是有多个服务器)，保护了实际的web服务器(客户端是和nginx交互而不是uWSGI)。</li></ul><h3 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a>Nginx</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install nginx</span><br></pre></td></tr></table></figure><p>Nginx是一款轻量级的Web服务器、反向代理服务器，由于它的内存占用少，启动极快，高并发能力强，在互联网项目中广泛应用。</p><p><img src="https://pic2.zhimg.com/80/v2-4787a512240b238ebf928cd0651e1d99_hd.jpg" alt=""></p><h3 id="Django-uWSGI-Nginx"><a href="#Django-uWSGI-Nginx" class="headerlink" title="Django + uWSGI + Nginx"></a>Django + uWSGI + Nginx</h3><p><img src="https://img-blog.csdnimg.cn/20181216174304355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d5bWFpc3ls,size_16,color_FFFFFF,t_70" alt=""></p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">user</span> www-data;</span><br><span class="line"><span class="attribute">worker_processes</span> auto;</span><br><span class="line"><span class="attribute">pid</span> /run/nginx.pid;</span><br><span class="line"><span class="attribute">include</span> /etc/nginx/modules-enabled/<span class="regexp">*.conf</span>;</span><br><span class="line"></span><br><span class="line"><span class="section">events</span> &#123;</span><br><span class="line"><span class="attribute">worker_connections</span> <span class="number">768</span>;</span><br><span class="line"><span class="comment"># multi_accept on;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">http</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Basic Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="section">server</span> &#123;   <span class="comment"># 这个server标识我要配置了</span></span><br><span class="line"><span class="attribute">listen</span> <span class="number">80</span>;  <span class="comment"># 我要监听那个端口</span></span><br><span class="line"><span class="attribute">server_name</span> <span class="number">118.25.79.249</span> ;  <span class="comment"># 你访问的路径前面的url名称</span></span><br><span class="line"><span class="attribute">charset</span>  utf-<span class="number">8</span>; <span class="comment"># Nginx编码</span></span><br><span class="line"><span class="attribute">gzip</span> <span class="literal">on</span>;  <span class="comment"># 启用压缩,这个的作用就是给用户一个网页,比如3M压缩后1M这样传输速度就会提高很多</span></span><br><span class="line"><span class="attribute">gzip_types</span> text/plain application/x-javascript text/css text/javascript application/x-httpd-php application/json text/json image/jpeg image/gif image/png application/octet-stream;  <span class="comment"># 支持压缩的类型</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">error_page</span>  <span class="number">404</span>           /<span class="number">404</span>.html;  <span class="comment"># 错误页面</span></span><br><span class="line"><span class="attribute">error_page</span>   <span class="number">500</span> <span class="number">502</span> <span class="number">503</span> <span class="number">504</span>  /50x.html;  <span class="comment"># 错误页面</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定项目路径uwsgi</span></span><br><span class="line"><span class="attribute">location</span> / &#123;        <span class="comment"># 这个location就和咱们Django的url(r'^admin/', admin.site.urls),</span></span><br><span class="line"><span class="attribute">include</span> uwsgi_params;  <span class="comment"># 导入一个Nginx模块他是用来和uWSGI进行通讯的</span></span><br><span class="line"><span class="attribute">uwsgi_connect_timeout</span> <span class="number">30</span>;  <span class="comment"># 设置连接uWSGI超时时间</span></span><br><span class="line"><span class="attribute">uwsgi_pass</span>  <span class="number">127.0.0.1:8000</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定静态文件路径</span></span><br><span class="line"><span class="attribute">location</span> /static/ &#123;</span><br><span class="line"><span class="attribute">alias</span>  /home/mysite/static/;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="attribute">sendfile</span> <span class="literal">on</span>;</span><br><span class="line"><span class="attribute">tcp_nopush</span> <span class="literal">on</span>;</span><br><span class="line"><span class="attribute">tcp_nodelay</span> <span class="literal">on</span>;</span><br><span class="line"><span class="attribute">keepalive_timeout</span> <span class="number">65</span>;</span><br><span class="line"><span class="attribute">types_hash_max_size</span> <span class="number">2048</span>;</span><br><span class="line"><span class="comment"># server_tokens off;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># server_names_hash_bucket_size 64;</span></span><br><span class="line"><span class="comment"># server_name_in_redirect off;</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">include</span> /etc/nginx/mime.types;</span><br><span class="line"><span class="attribute">default_type</span> application/octet-stream;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># SSL Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">ssl_protocols</span> TLSv1 TLSv1.<span class="number">1</span> TLSv1.<span class="number">2</span>; <span class="comment"># Dropping SSLv3, ref: POODLE</span></span><br><span class="line"><span class="attribute">ssl_prefer_server_ciphers</span> <span class="literal">on</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Logging Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">access_log</span> /var/log/nginx/access.log;</span><br><span class="line"><span class="attribute">error_log</span> /var/log/nginx/error.log;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Gzip Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">gzip</span> <span class="literal">on</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># gzip_vary on;</span></span><br><span class="line"><span class="comment"># gzip_proxied any;</span></span><br><span class="line"><span class="comment"># gzip_comp_level 6;</span></span><br><span class="line"><span class="comment"># gzip_buffers 16 8k;</span></span><br><span class="line"><span class="comment"># gzip_http_version 1.1;</span></span><br><span class="line"><span class="comment"># gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Virtual Host Configs</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">include</span> /etc/nginx/conf.d/<span class="regexp">*.conf</span>;</span><br><span class="line"><span class="attribute">include</span> /etc/nginx/sites-enabled/*;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#mail &#123;</span></span><br><span class="line"><span class="comment">## See sample authentication script at:</span></span><br><span class="line"><span class="comment">## http://wiki.nginx.org/ImapAuthenticateWithApachePhpScript</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">## auth_http localhost/auth.php;</span></span><br><span class="line"><span class="comment">## pop3_capabilities "TOP" "USER";</span></span><br><span class="line"><span class="comment">## imap_capabilities "IMAP4rev1" "UIDPLUS";</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#server &#123;</span></span><br><span class="line"><span class="comment">#listen     localhost:110;</span></span><br><span class="line"><span class="comment">#protocol   pop3;</span></span><br><span class="line"><span class="comment">#proxy      on;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#server &#123;</span></span><br><span class="line"><span class="comment">#listen     localhost:143;</span></span><br><span class="line"><span class="comment">#protocol   imap;</span></span><br><span class="line"><span class="comment">#proxy      on;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[uwsgi] </span><br><span class="line">chdir = /home/mysite</span><br><span class="line">module = mysite.wsgi:application</span><br><span class="line">socket = 127.0.0.1:8000</span><br><span class="line">master = true </span><br><span class="line">processes = 1</span><br><span class="line">threads = 2</span><br><span class="line">max-requests = 6000</span><br><span class="line">chmod-socket = 666</span><br><span class="line">buffer-size = 65535</span><br><span class="line">logto = /var/log/mysite.log</span><br><span class="line">async</span><br><span class="line">ugreen =''</span><br><span class="line">http-timeout = 300</span><br><span class="line"><span class="comment">#plugins=python</span></span><br></pre></td></tr></table></figure><h3 id="Activemq"><a href="#Activemq" class="headerlink" title="Activemq"></a>Activemq</h3><p>ActiveMQ 是 Apache 出品，最流行的，能力强劲的开源消息总线。ActiveMQ 是一个完全支持 JMS1.1 和 J2EE 1.4 规范的 JMS Provider 实现。</p><h4 id="queue"><a href="#queue" class="headerlink" title="queue"></a>queue</h4><p><img src="https://pic4.zhimg.com/80/v2-b7edcfa850af9627bed67ef9e89f8d3f_hd.jpg" alt=""></p><h4 id="topic"><a href="#topic" class="headerlink" title="topic"></a>topic</h4><p><img src="https://pic2.zhimg.com/80/v2-b1874d392a6119fb4e497425dcc58609_hd.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Django+uwsgi+Nginx&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-1-10周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/01/07/2020-1-10%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/01/07/2020-1-10%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-01-07T06:07:24.000Z</published>
    <updated>2020-01-10T10:47:51.445Z</updated>
    
    <content type="html"><![CDATA[<p>“All you need is attention”<br><a id="more"></a></p><h3 id="LSTM-Attention"><a href="#LSTM-Attention" class="headerlink" title="LSTM + Attention"></a>LSTM + Attention</h3><p>FEED-FORWARD NETWORKS WITH ATTENTION CAN SOLVE SOME LONG-TERM MEMORY PROBLEMS</p><h4 id="FEED-FORWARD-ATTENTION"><a href="#FEED-FORWARD-ATTENTION" class="headerlink" title="FEED-FORWARD ATTENTION"></a>FEED-FORWARD ATTENTION</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/FEED-FORWARD-ATTENTION.jpg" alt=""></p><script type="math/tex; mode=display">e_{t} = a(h_{t})</script><script type="math/tex; mode=display">\alpha_{t} = \frac{exp(e_{t})}{\sum_{k=1}^{T}exp(e_{k})}</script><script type="math/tex; mode=display">c = \sum_{t=1}^{T}\alpha_{t}h_{t}</script><h4 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/all-lstmattention.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/test-lstmattention.png" alt=""></p><h3 id="Seq2Seq-Attention"><a href="#Seq2Seq-Attention" class="headerlink" title="Seq2Seq + Attention"></a>Seq2Seq + Attention</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/all-seq2seqattention.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/test-seq2seqattention.png" alt=""></p><h3 id="DA-RNN"><a href="#DA-RNN" class="headerlink" title="DA-RNN"></a>DA-RNN</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-gru.jpg" alt=""></p><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h4><p>顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制</p><p>给出信息输入：用X = [x1, · · · , xN ]表示N 个输入信息；通过线性变换得到为查询向量序列，键向量序列和值向量序列，其中$W^{Q}$,$W^{K}$,$W^{V}$是我们模型训练过程学习到的合适的参数</p><script type="math/tex; mode=display">Q = W^{Q}X</script><script type="math/tex; mode=display">K = W^{K}X</script><script type="math/tex; mode=display">V = W^{V}X</script><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix}*[v^{T}_{1},v^{T}_{2},...,v^{T}_{n}])*\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix} = softmax(QK^{T})V</script><p><img src="https://pic2.zhimg.com/v2-07c4c02a9bdecb23d9664992f142eaa5_r.jpg" alt=""></p><p>Source中的构成元素想象成是由一系列的<Key,Value>数据对构成<br>Target中的某个元素Query<br>(在Seq2Se2中，Q是Decoder的隐藏态，K和V都是Encoder的隐藏态)</p><ul><li>1、根据Query和Key计算权重系数，常用的相似度函数有点积，拼接，感知机等</li><li>2、使用softmax函数对这些权重进行归一化</li><li>3、根据权重系数对Value进行加权求和得到attention</li></ul><h4 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h4><p>防止Q和K点乘积结果过大，会除以一个尺度标度 </p><script type="math/tex; mode=display">Attention(Q,K,V) = sofrmax(\frac{QK^{T}}{\sqrt{d_{k}}})V</script><h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><ul><li>$Q$，$K$，$V$首先进过一个线性变换，然后输入到放缩点积attention</li><li>每次$Q$，$K$，$V$进行线性变换的参数$W$是不一样的</li><li>通过$h$个不同的线性变换对$Q$，$K$，$V$进行投影，最后将不同的attention结果拼接起来</li></ul><script type="math/tex; mode=display">Multihead(Q,K,V) = Concat(head_{1},...,head_{h})W^{O}</script><script type="math/tex; mode=display">head_{i} = Attention(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})</script><h4 id="Experiment-1"><a href="#Experiment-1" class="headerlink" title="Experiment"></a>Experiment</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/all-transformer.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/test-transformer.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;“All you need is attention”&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>说说LSTM</title>
    <link href="http://arithmeticjia.github.io/2019/12/29/%E8%AF%B4%E8%AF%B4LSTM/"/>
    <id>http://arithmeticjia.github.io/2019/12/29/%E8%AF%B4%E8%AF%B4LSTM/</id>
    <published>2019-12-29T15:22:35.000Z</published>
    <updated>2019-12-29T15:34:02.931Z</updated>
    
    <content type="html"><![CDATA[<p>Long Short Term Memory<br><a id="more"></a></p><h4 id="从RNN开始"><a href="#从RNN开始" class="headerlink" title="从RNN开始"></a>从RNN开始</h4><p>RNN(Recurrent Neural Network)是一类用于处理序列数据的神经网络，擅长对序列数据进行建模处理。LSTM(Long Short-Term Memory) 在传统的 RNN 的基础上增加了状态$c$，称为记忆单元态 (cell state)，用以取代传统的隐含神经元节点。它负责把记忆信息从序列的初始位置，传递到序列的末端。</p><h4 id="LSTM的组成"><a href="#LSTM的组成" class="headerlink" title="LSTM的组成"></a>LSTM的组成</h4><p>在$t$时刻，当前神经元的输入有三个：当前时刻输入值$x_{t}$、前一时刻输出值$s_{t-1}$,和前一时刻的记忆单元状态$c_{t-1}$, 输出有两个，当前时刻LSTM的输出值$s_{t}$和当前时刻的记忆单元状态$c_{t}$。<br>LSTM通过三个门控开关传递记忆状态。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Long Short Term Memory&lt;br&gt;
    
    </summary>
    
    
      <category term="LSTM" scheme="http://arithmeticjia.github.io/categories/LSTM/"/>
    
    
      <category term="lstm" scheme="http://arithmeticjia.github.io/tags/lstm/"/>
    
      <category term="deeplearning" scheme="http://arithmeticjia.github.io/tags/deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode78Pascal-Triangle-2-Java</title>
    <link href="http://arithmeticjia.github.io/2019/12/29/Leetcode78Pascal-Triangle-2-Java/"/>
    <id>http://arithmeticjia.github.io/2019/12/29/Leetcode78Pascal-Triangle-2-Java/</id>
    <published>2019-12-29T03:54:18.000Z</published>
    <updated>2019-12-29T03:58:39.143Z</updated>
    
    <content type="html"><![CDATA[<p>Java 找规律法<br><a id="more"></a><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">getRow</span><span class="params">(<span class="keyword">int</span> rowIndex)</span> </span>&#123;</span><br><span class="line">        List&lt;Integer&gt; res = <span class="keyword">new</span> ArrayList&lt;Integer&gt;();</span><br><span class="line">        <span class="keyword">long</span> k = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(rowIndex &gt;= <span class="number">0</span>)</span><br><span class="line">            res.add(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= rowIndex + <span class="number">1</span>; i++) &#123;</span><br><span class="line">            k = k * (rowIndex + <span class="number">2</span> - i) / (i-<span class="number">1</span>);</span><br><span class="line">            res.add((<span class="keyword">int</span>)k);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>这里用到了杨辉三角的规律，第n行m个数等于</p><p>譬如第三行第二个数</p><script type="math/tex; mode=display">C_{3-1}^{2-1} = C_{2}^{1} = 2</script><p>譬如第四行第三个数</p><script type="math/tex; mode=display">C_{4-1}^{3-1} = C_{3}^{2} = 3</script><p>那这个对我们的算法有啥帮助呢？</p><p>举个栗子，看第四行</p><p>应该是1 3 3 1</p><p>在本题中是1 4 6 4 1</p><p>$C_{5-1}^{1-1} = C_{4}^{0} = 1$，$C_{5-1}^{2-1} = C_{4}^{1} = 4$，$C_{5-1}^{3-1} = C_{4}^{2} = 6$，$C_{5-1}^{4-1} = C_{4}^{3} = 4$，$C_{5-1}^{5-1} = C_{4}^{4} = 1$</p><p>找规律如下：</p><p>第一个数：<script type="math/tex">C_{5-1}^{1-1} = C_{4}^{0} = 1</script></p><p>第二个数：<script type="math/tex">C_{5-1}^{2-1} = C_{4}^{1} = C_{5-1}^{1-1} * \frac{(rowIndex-2+2)}{2-1}</script></p><p>第n行m个数：第m-1个数 × $ \frac{(rowIndex-m+2)}{m-1} $，第n行第一个数永远是1</p><p>晚安~~~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Java 找规律法&lt;br&gt;
    
    </summary>
    
    
      <category term="Leetcode" scheme="http://arithmeticjia.github.io/categories/Leetcode/"/>
    
    
      <category term="leetcode" scheme="http://arithmeticjia.github.io/tags/leetcode/"/>
    
      <category term="java" scheme="http://arithmeticjia.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode[78]Pascal&#39;s Triangle II</title>
    <link href="http://arithmeticjia.github.io/2019/12/28/Leetcode78Pascal-Triangle-2/"/>
    <id>http://arithmeticjia.github.io/2019/12/28/Leetcode78Pascal-Triangle-2/</id>
    <published>2019-12-28T14:05:49.000Z</published>
    <updated>2019-12-28T14:07:35.863Z</updated>
    
    <content type="html"><![CDATA[<p>python3 最优雅解法<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getRow</span><span class="params">(self, rowIndex)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type rowIndex: int</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        res = [<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, rowIndex + <span class="number">1</span>):</span><br><span class="line">            res.insert(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">            <span class="comment"># j循环每次算出r[0]...r[j-1]，再加上最后一个永远存在的1，正好是rowIndex+1个数</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">                res[j] = res[j] + res[j + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;python3 最优雅解法&lt;br&gt;
    
    </summary>
    
    
      <category term="Leetcode" scheme="http://arithmeticjia.github.io/categories/Leetcode/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="leetcode" scheme="http://arithmeticjia.github.io/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>2019-12-27周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2019/12/27/2019-12-27%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2019/12/27/2019-12-27%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2019-12-27T07:40:46.000Z</published>
    <updated>2020-01-10T09:53:38.616Z</updated>
    
    <content type="html"><![CDATA[<p>RNN -&gt; LSTM -&gt; GRU -&gt; Seq2Seq -&gt; Attention -&gt; Transformer<br><a id="more"></a></p><h3 id="Encoder-Decoder-Seq2Seq"><a href="#Encoder-Decoder-Seq2Seq" class="headerlink" title="Encoder-Decoder(Seq2Seq)"></a>Encoder-Decoder(Seq2Seq)</h3><p><img src="https://pic4.zhimg.com/80/v2-77e8a977fc3d43bec8b05633dc52ff9f_hd.jpg" alt=""></p><ul><li>Encoder-Decoder结构先将输入数据编码成一个上下文向量$c$</li><li>把Encoder的最后一个隐状态赋值给$c$,还可以对最后的隐状态做一个变换得到$c$，也可以对所有的隐状态做变换</li><li>拿到c之后，就用另一个RNN网络对其进行解码(Decoder),将c当做之前的初始状态$h_{0}$输入到Decoder中</li><li>还有一种做法是将$c$当做每一步的输入</li></ul><p><img src="https://pic4.zhimg.com/80/v2-e0fbb46d897400a384873fc100c442db_hd.jpg" alt=""></p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><ul><li>在Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征$c$再解码，因此，$c$中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈</li><li>Attention机制通过在每个时间输入不同的$c$来解决这个问题</li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/v2-8da16d429d33b0f2705e47af98e66579_hd_gaitubao_525x551_gaitubao_345x362.jpg" alt=""></p><ul><li>每一个$c$会自动去选取与当前所要输出的$y$最合适的上下文信息。具体来说，我们用$\alpha_{ij}$衡量Encoder中第$j$阶段的$h_{j}$和解码时第$i$阶段的相关性，最终Decoder中第$i$阶段的输入的上下文信息$c_{i}$就来自于所有$h_{j}$对$\alpha_{ij}$的加权和。</li><li>$\alpha_{ij}$和Decoder的第$i$阶段的隐藏状态、Encoder第$j$个阶段的隐藏状态有关</li><li>在Encoder的过程中保留每个RNN单元的隐藏状态(hidden state)得到($h_{1}$…$h_{N}$)，取$h_{j}$，表示Encoder层的隐层第$j$时刻的输出</li><li>在Decoder的过程中根据$x_{i}$和$h’_{i-1}$(这里和Encoder的$h_{i}$区分一下)得到$h’_{i}$，设为$s_{i}$</li><li>注：最开始的论文在Encoder-Decoder里面的当前Decoder的attention得分用的是$s_{i-1}$和$h_{j}$来算，但斯坦福教材上图上确实是画的$s_{i}$和$h_{j}$来算，而且后续论文大多是用的这种方式，即当前步的attention score用的当前步的隐藏状态$s_{i}$和前面的$h_{j}$去算的</li><li>通过Decoder的hidden states加上Encoder的hidden states来计算一个分数，用于计算权重<script type="math/tex; mode=display">e_{ij} = score(s_{i},h_{j})</script></li><li>注：这里有很多计算方式<script type="math/tex; mode=display">score(s_{i},h_{j}) = \left\{\begin{matrix}s^{T}_{i}h_{j}\\ s^{T}_{i}W_{a}h_{j}\\ v^{T}_{a}tanh(W_{a}[s^{T}_{i};h_{j}])\end{matrix}\right.</script></li><li>softmax权重归一化<script type="math/tex; mode=display">\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}</script></li><li>计算$c$<script type="math/tex; mode=display">c_{i} = \sum_{j=1}^{T_{x}}\alpha_{ij}h_{j}</script></li></ul><p><img src="https://pic4.zhimg.com/80/v2-8ddf993a95ee6e525fe2cd5ccd49bba7_hd.jpg" alt=""></p><p>(1)$h_{t} = RNN_{enc}(x_{t},h_{t-1})$, Encoder方面接受的是每一个单词word embedding，和上一个时间点的hidden state。输出的是这个时间点的hidden state。</p><p>(2)$s_{t} = RNN_{dnc}(y_{t},s_{t-1})$, Decoder方面接受的是目标句子里单词的word embedding，和上一个时间点的hidden state。</p><p>(3)$c_{i} = \sum_{j=1}^{T_{x}}\alpha _{ij}h_{j}$, context vector是一个对于encoder输出的hidden states的一个加权平均。</p><p>(4)$\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}$, 每一个encoder的hidden states对应的权重。</p><p>(5)$e_{ij} = score(s_{i},h_{j})$, 通过decoder的hidden states加上encoder的hidden states来计算一个分数，用于计算权重(4)</p><p>(6)$\hat{s}_{t}=tanh(W_{c}[c_{t};s_{t}])$, 将context vector 和 decoder的hidden states 串起来。</p><p>(7)$p(y_{t}|y_{&lt;t},x) = softmax(W_{s}\hat{s}_{t})$, 计算最后的输出概率。</p><h3 id="Transformer—-Attention-Is-All-You-Need"><a href="#Transformer—-Attention-Is-All-You-Need" class="headerlink" title="Transformer—-Attention Is All You Need"></a>Transformer—-Attention Is All You Need</h3><p><img src="https://pic1.zhimg.com/80/v2-4b53b731a961ee467928619d14a5fd44_hd.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/v2-4b53b731a961ee467928619d14a5fd44_r.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/4155986-208004e73fb93c97.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/4155986-e7fd5fcf3acc00a3.png" alt=""></p><ul><li>Transformer 的 Encoder 由 6 个编码器叠加组成，Decoder 也由 6 个解码器组成，在结构上都是相同的，但它们不共享权重。</li><li>Encoder的每一层有两个操作，分别是Self-Attention和Feed Forward；</li><li>Decoder的每一层有三个操作，分别是Self-Attention、Encoder-Decoder Attention以及Feed Forward操作。</li><li>这里的Self-Attention和Encoder-Decoder Attention都是用的是Multi-Head Attention机制</li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/v2-df2ca1b7a60d829245b7b7c37f80a3aa_r.jpg" alt=""></p><h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h4><ul><li>RNN的循环特性导致其不利于并行计算，模型训练时间较长</li><li>在传统的seq2seq中，我们通过RNN获取hidden state去做attention，那么当我们完全抛弃RNN的时候，怎么去做attention呢？</li><li>对每个input做embedding，代替hidden state，embedding通过三个不同的线性层生成$Q$，$K$，$V$。</li><li>Q: query;K: key; V: value</li><li>K = V = Q</li></ul><script type="math/tex; mode=display">Q = W_{Q}X</script><script type="math/tex; mode=display">K = W_{K}X</script><script type="math/tex; mode=display">V = W_{V}X</script><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix}*[v^{T}_{1},v^{T}_{2},...,v^{T}_{n}])*\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix} = softmax(QK^{T})V</script><p>举个栗子</p><p><img src="https://pic1.zhimg.com/80/v2-087b831f622f83e4529c1bbf646530f0_hd.jpg" alt=""></p><ul><li>假如我们要翻译一个词组Thinking Machines，其中Thinking的输入的embedding vector用$x_{1}$表示，Machines的embedding vector用$x_{2}$表示</li><li>$W^{Q}$，$W^{K}$，$W^{V}$是我们模型训练过程学习到的合适的参数</li><li>$x$与$W^{Q}$，$W^{K}$，$W^{V}$相乘获得$q$，$k$，$v$</li><li>如上图中所示我们分别得到了$q_{1}$与$k_{1}$，$k_{2}$的点乘积，然后我们进行尺度缩放与softmax归一化</li></ul><h4 id="Scaled-Dot-Product-Attention-缩放了的点乘注意力"><a href="#Scaled-Dot-Product-Attention-缩放了的点乘注意力" class="headerlink" title="Scaled Dot-Product Attention(缩放了的点乘注意力)"></a>Scaled Dot-Product Attention(缩放了的点乘注意力)</h4><script type="math/tex; mode=display">Attention(Q,K,V) = sofrmax(\frac{QK^{T}}{\sqrt{d_{k}}})V</script><ul><li>输入包含$d_{k}$维的query和key，以及$d_{v}$维的value。通过计算query和各个key的点积，除以$\sqrt{d_{k}}$归一化，然后经过softmax激活变成权重，最后再乘value。点积注意力机制的优点是速度快、占用空间小。</li></ul><h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><ul><li>$Q$，$K$，$V$首先进过一个线性变换，然后输入到放缩点积attention</li><li>每次$Q$，$K$，$V$进行线性变换的参数$W$是不一样的</li><li>通过$h$个不同的线性变换对$Q$，$K$，$V$进行投影，最后将不同的attention结果拼接起来</li></ul><script type="math/tex; mode=display">Multihead(Q,K,V) = Concat(head_{1},...,head_{h})W^{O}</script><script type="math/tex; mode=display">head_{i} = Attention(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})</script><h4 id="Position-wise-feed-forward-networks-位置全链接前馈网络"><a href="#Position-wise-feed-forward-networks-位置全链接前馈网络" class="headerlink" title="Position-wise feed-forward networks(位置全链接前馈网络)"></a>Position-wise feed-forward networks(位置全链接前馈网络)</h4><ul><li>由两个线性变换（Wx+b）和一个ReLU（relu的数学表达式就是f(x)=max(0,x)）<script type="math/tex; mode=display">FFN(x) = max(0,xW_{1} + b_{1})W_{2} + b_{2}</script></li></ul><h4 id="Positional-Encoding-位置编码"><a href="#Positional-Encoding-位置编码" class="headerlink" title="Positional Encoding(位置编码)"></a>Positional Encoding(位置编码)</h4><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1115<span class="string">-1120</span> after data smoothing</span><br><span class="line">T = 10</span><br><span class="line">features = 70</span><br><span class="line">train = all * 0.7</span><br><span class="line"><span class="keyword">test </span>= all * 0.3</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-1115-1120-all.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-1115-1120-test-m.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-1115-1120-test.jpg" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 3.955</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.289</span><br></pre></td></tr></table></figure><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nasdaq100_padding</span><br><span class="line">T = 10</span><br><span class="line">features = 81</span><br><span class="line">train = all * 0.7</span><br><span class="line"><span class="keyword">test </span>= all * 0.3</span><br></pre></td></tr></table></figure><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">Encoder:</span> LSTM</span><br><span class="line"><span class="symbol">Decoder:</span> LSTM</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-lstm.jpg" alt=""><br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.579</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.105</span><br></pre></td></tr></table></figure></p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">Encoder:</span> BiLSTM</span><br><span class="line"><span class="symbol">Decoder:</span> LSTM</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-bi-lstm.jpg" alt=""><br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.384</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.069</span><br></pre></td></tr></table></figure></p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">Encoder:</span> GRU</span><br><span class="line"><span class="symbol">Decoder:</span> LSTM</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-gru.jpg" alt=""><br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.252</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.046</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RNN -&amp;gt; LSTM -&amp;gt; GRU -&amp;gt; Seq2Seq -&amp;gt; Attention -&amp;gt; Transformer&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
</feed>
