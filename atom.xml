<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>请叫我算术嘉的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://arithmeticjia.github.io/"/>
  <updated>2019-12-22T13:56:10.844Z</updated>
  <id>http://arithmeticjia.github.io/</id>
  
  <author>
    <name>请叫我算术嘉</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Seq2seq模型及注意力机制模型</title>
    <link href="http://arithmeticjia.github.io/2019/12/22/Seq2seq%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A8%A1%E5%9E%8B/"/>
    <id>http://arithmeticjia.github.io/2019/12/22/Seq2seq%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A8%A1%E5%9E%8B/</id>
    <published>2019-12-22T07:21:43.000Z</published>
    <updated>2019-12-22T13:56:10.844Z</updated>
    
    <content type="html"><![CDATA[<p>对弈处理输出序列为不定长情况的问题呢，例如机器翻译，例如英文到法语的句子翻译，输入和输出均为不定长。前人提出了seq2seq模型，basic idea是设计一个encoder与decoder，其中encoder将输入序列编码为一个包含输入序列所有信息的context vector $ c $，decoder通过对$ c $的解码获得输入序列的信息，从而得到输出序列。encoder及decoder都通常为RNN循环神经网络<br><a id="more"></a></p><h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><ul><li>forget gate:<script type="math/tex; mode=display">f_{t} = \sigma (W_{f}[h_{t-1};x_{t}]+b_{f})</script>$W_{f}$是遗忘门的权重矩阵，$[h_{t-1};x_{t}]$表示把两个向量连接成一个更长的向量，$b_{f}$是遗忘门的偏置项，$\sigma$是sigmoid函数<br>如果输入的维度是$d_{x}$，隐藏层的维度是$d_{h}$，单元状态的维度是$d_{c}$（通常$d_{c} = d_{h}$），则遗忘门的权重矩阵$W_{f}$的维度是$d_{c}×(d_{h}+d_{x})$</li><li><p>input gate</p><script type="math/tex; mode=display">i_{t} = \sigma (W_{i}[h_{t-1};x_{t}]+b_{i})</script></li><li><p>output gate</p><script type="math/tex; mode=display">o_{t} = \sigma (W_{o}[h_{t-1};x_{t}]+b_{o})</script></li><li><p>final out</p><script type="math/tex; mode=display">\tilde{c}_{t}= tanh(W_{c}[h_{t-1};x_{t}]+b_{c})</script><script type="math/tex; mode=display">c_{t} = f_{t} * c_{t-1} + i_{t} * \tilde{c}_{t}</script><script type="math/tex; mode=display">h_{t} = o_{t} * tanh(c_{t})</script></li><li><p>前向计算每个神经元的输出值，对于LSTM来说就是$f_{t},i_{t},c_{t},o_{t},h_{t}$5个向量的值</p></li><li>反向计算每个神经元的误差项$\delta$，包括两个方向，一是沿时间的反向传播，即从当前t时刻开始，计算每个时刻的误差项；另一个是将误差项向上一层传播</li><li>根据相应的误差项，计算每个权重的梯度</li></ul><h4 id="seq2seq模型"><a href="#seq2seq模型" class="headerlink" title="seq2seq模型"></a>seq2seq模型</h4><h5 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h5><p>编码器的作用是把一个不定长的输入序列$ x_{1},x_{2},…,x_{T} $转化成一个定长的context vector $c$. 该context vector编码了输入序列$ x_{1},x_{2},…,x_{T} $的序列。回忆一下循环神经网络，假设该循环神经网络单元为$f$（可以为vanilla RNN, LSTM, GRU)，那么hidden state为</p><script type="math/tex; mode=display">h_{t} = f(x_{t},h_{t-1})</script><p>编码器的context vector是所有时刻hidden state的函数，即：</p><script type="math/tex; mode=display">c=q(h_{1},...,h_{T})</script><p>简单地，我们可以把最终时刻的hidden state[公式]作为context vecter。当然我们也可以取各个时刻hidden states的平均，以及其他方法。</p><h5 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h5><p>编码器最终输出一个context vector $c$，该context vector编码了输入序列$ x_{1},x_{2},…,x_{T} $的信息。</p><p>假设训练数据中的输出序列为$y_{1}y_{2},…,y_{T}^{‘}$,我们希望每个[公式]时刻的输出即取决于之前的输出也取决于context vector，即估计[公式]，从而得到输出序列的联合概率分布：</p><p>[公式]</p><p>并定义该序列的损失函数loss function</p><p>[公式]</p><p>通过最小化损失函数来训练seq2seq模型。</p><p>那么如何估计[公式]？</p><p>我们使用另一个循环神经网络作为解码器。解码器使用函数[公式]来表示[公式]时刻输出[公式]的概率</p><p>[公式]</p><p>为了区分编码器中的hidden state[公式]，其中[公式]为[公式]时刻解码器的hidden state。区别于编码器，解码器中的循环神经网络的输入除了前一个时刻的输出序列[公式]，和前一个时刻的hidden state[公式]以外，还包含了context vector[公式]。即：</p><p>[公式]</p><p>其中函数g为解码器的循环神经网络单元。</p><h4 id="DA-RNN"><a href="#DA-RNN" class="headerlink" title="DA-RNN"></a>DA-RNN</h4><h5 id="第一阶段，使用注意力机制自适应地提取每个时刻的相关feature"><a href="#第一阶段，使用注意力机制自适应地提取每个时刻的相关feature" class="headerlink" title="第一阶段，使用注意力机制自适应地提取每个时刻的相关feature"></a>第一阶段，使用注意力机制自适应地提取每个时刻的相关feature</h5><script type="math/tex; mode=display">e_{t}^{k}=v_{e}^{T}tanh(W_{e}[h_{t-1};c_{t-1}]+U_{e}x^{k})</script><ul><li>用softmax函数将其归一化<script type="math/tex; mode=display">\alpha _{t}^{k}=\frac{exp(e_{t}^{k})}{\sum_{i-1}^{n}exp(e_{t}^{i})}</script></li><li>得到更新后的x<script type="math/tex; mode=display">\tilde{x} = (\alpha _{t}^{1}x_{t}^{1}, \alpha _{t}^{2}x_{t}^{2},...,\alpha _{t}^{n}x_{t}^{n})</script></li><li>选取LSTM作为编码器<script type="math/tex">f_{1}</script><script type="math/tex; mode=display">h_{t} = f_{1}(h_{t-1},  \tilde{x})</script></li></ul><h5 id="第二阶段，使用另一个注意力机制选取与之相关的encoder-hidden-states"><a href="#第二阶段，使用另一个注意力机制选取与之相关的encoder-hidden-states" class="headerlink" title="第二阶段，使用另一个注意力机制选取与之相关的encoder hidden states"></a>第二阶段，使用另一个注意力机制选取与之相关的encoder hidden states</h5><ul><li>对所有时刻的$h_{t’}$取加权平均，即：</li></ul><script type="math/tex; mode=display">c_{t}^{'} = \sum_{t-1}^{T}\beta _{t^{'}}^{t}h_{t}</script><ul><li><script type="math/tex">\beta _{t^{'}}^{t}</script>的设计类似于Bahanau的工作，基于前一个时刻解码器的hidden state $ d_{t’-1} $和cell state$s_{t’-1}^{‘}$计算得到：</li></ul><script type="math/tex; mode=display">l_{t'}^{t}=v_{d}^{T}tanh(W_{d}[d_{t'-1};s_{t'-1}^{'}]+U_{d}h_{t})</script><script type="math/tex; mode=display">\beta _{t'}^{t}=\frac{exp(l_{t'}^{t})}{\sum_{j=1}^{T}exp(l_{t'}^{j})}</script><ul><li>解码器的输入是上一个时刻的目标序列$y_{t’-1}$和hidden state$d_{t’-1}$以及context vector $c_{t’-1}$，即<script type="math/tex; mode=display">d_{t'}=f_{2}(y_{t'-1},c_{t'-1},d_{t'-1}</script></li><li>这里设计了$\tilde{y}_{t’-1}$来combie$y_{t’-1}$与$c_{t’-1}$的信息，即<script type="math/tex; mode=display">\tilde{y}_{t'-1} = \tilde{\omega }^{T}[y_{t'-1};c_{t'-1}]+\tilde{b}</script></li><li>然后<script type="math/tex; mode=display">d_{t'}=f_{2}(d_{t'-1},\tilde{y}_{t'-1})</script></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对弈处理输出序列为不定长情况的问题呢，例如机器翻译，例如英文到法语的句子翻译，输入和输出均为不定长。前人提出了seq2seq模型，basic idea是设计一个encoder与decoder，其中encoder将输入序列编码为一个包含输入序列所有信息的context vector $ c $，decoder通过对$ c $的解码获得输入序列的信息，从而得到输出序列。encoder及decoder都通常为RNN循环神经网络&lt;br&gt;
    
    </summary>
    
    
    
      <category term="seq2seq" scheme="http://arithmeticjia.github.io/tags/seq2seq/"/>
    
      <category term="attention" scheme="http://arithmeticjia.github.io/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>da-rnn-bug-fix</title>
    <link href="http://arithmeticjia.github.io/2019/12/21/da-rnn-bug-fix/"/>
    <id>http://arithmeticjia.github.io/2019/12/21/da-rnn-bug-fix/</id>
    <published>2019-12-21T14:19:27.000Z</published>
    <updated>2019-12-22T06:42:37.279Z</updated>
    
    <content type="html"><![CDATA[<p>Bugs fix for<br><a href="https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py" target="_blank" rel="noopener" title="https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py">https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py</a><br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> concatenate</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">filename = <span class="string">'nasdaq100_padding.csv'</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(filename)</span><br><span class="line"><span class="comment"># print(dataset.values)</span></span><br><span class="line"></span><br><span class="line">features = dataset.values.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 82</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderAtt</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, T)</span>:</span></span><br><span class="line">        <span class="comment"># input size: number of underlying factors (81)</span></span><br><span class="line">        <span class="comment"># T: number of time steps (10)</span></span><br><span class="line">        <span class="comment"># hidden_size: dimension of the hidden state</span></span><br><span class="line">        super(EncoderAtt, self).__init__()</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.T = T</span><br><span class="line"></span><br><span class="line">        self.lstm_layer = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=<span class="number">1</span>)</span><br><span class="line">        self.attn_linear = nn.Linear(in_features=<span class="number">2</span> * hidden_size + T - <span class="number">1</span>, out_features=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_data)</span>:</span></span><br><span class="line">        <span class="comment"># input_data: batch_size * T - 1 * input_size</span></span><br><span class="line">        input_weighted = Variable(input_data.data.new(input_data.size(<span class="number">0</span>), self.T - <span class="number">1</span>, self.input_size).zero_())</span><br><span class="line">        input_encoded = Variable(input_data.data.new(input_data.size(<span class="number">0</span>), self.T - <span class="number">1</span>, self.hidden_size).zero_())</span><br><span class="line">        <span class="comment"># hidden, cell: initial states with dimention hidden_size</span></span><br><span class="line">        hidden = self.init_hidden(input_data) <span class="comment"># 1 * batch_size * hidden_size</span></span><br><span class="line">        cell = self.init_hidden(input_data)</span><br><span class="line">        <span class="comment"># hidden.requires_grad = False</span></span><br><span class="line">        <span class="comment"># cell.requires_grad = False</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(self.T - <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># Eqn. 8: concatenate the hidden states with each predictor</span></span><br><span class="line">            x = torch.cat((hidden.repeat(self.input_size, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>),</span><br><span class="line">                           cell.repeat(self.input_size, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>),</span><br><span class="line">                           input_data.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)), dim = <span class="number">2</span>) <span class="comment"># batch_size * input_size * (2*hidden_size + T - 1)</span></span><br><span class="line">            <span class="comment"># Eqn. 9: Get attention weights</span></span><br><span class="line">            x = self.attn_linear(x.view(<span class="number">-1</span>, self.hidden_size * <span class="number">2</span> + self.T - <span class="number">1</span>)) <span class="comment"># (batch_size * input_size) * 1</span></span><br><span class="line">            attn_weights = F.softmax(x.view(<span class="number">-1</span>, self.input_size)) <span class="comment"># batch_size * input_size, attn weights with values sum up to 1.</span></span><br><span class="line">            <span class="comment"># Eqn. 10: LSTM</span></span><br><span class="line">            weighted_input = torch.mul(attn_weights, input_data[:, t, :]) <span class="comment"># batch_size * input_size</span></span><br><span class="line">            <span class="comment"># Fix the warning about non-contiguous memory</span></span><br><span class="line">            <span class="comment"># see https://discuss.pytorch.org/t/dataparallel-issue-with-flatten-parameter/8282</span></span><br><span class="line">            self.lstm_layer.flatten_parameters()</span><br><span class="line">            _, lstm_states = self.lstm_layer(weighted_input.unsqueeze(<span class="number">0</span>), (hidden, cell))</span><br><span class="line">            hidden = lstm_states[<span class="number">0</span>]</span><br><span class="line">            cell = lstm_states[<span class="number">1</span>]</span><br><span class="line">            <span class="comment"># Save output</span></span><br><span class="line">            input_weighted[:, t, :] = weighted_input</span><br><span class="line">            input_encoded[:, t, :] = hidden</span><br><span class="line">        <span class="keyword">return</span> input_weighted, input_encoded</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># No matter whether CUDA is used, the returned variable will have the same type as x.</span></span><br><span class="line">        <span class="keyword">return</span> Variable(x.data.new(<span class="number">1</span>, x.size(<span class="number">0</span>), self.hidden_size).zero_()) <span class="comment"># dimension 0 is the batch dimension</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderAtt</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder_hidden_size, decoder_hidden_size, T)</span>:</span></span><br><span class="line">        super(DecoderAtt, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.T = T</span><br><span class="line">        self.encoder_hidden_size = encoder_hidden_size</span><br><span class="line">        self.decoder_hidden_size = decoder_hidden_size</span><br><span class="line"></span><br><span class="line">        self.attn_layer = nn.Sequential(nn.Linear(<span class="number">2</span> * decoder_hidden_size + encoder_hidden_size, encoder_hidden_size),</span><br><span class="line">                                        nn.Tanh(), nn.Linear(encoder_hidden_size, <span class="number">1</span>))</span><br><span class="line">        self.lstm_layer = nn.LSTM(input_size=<span class="number">1</span>, hidden_size=decoder_hidden_size)</span><br><span class="line">        self.fc = nn.Linear(encoder_hidden_size + <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc_final = nn.Linear(decoder_hidden_size + encoder_hidden_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.fc.weight.data.normal_()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_encoded, y_history)</span>:</span></span><br><span class="line">        <span class="comment"># input_encoded: batch_size * T - 1 * encoder_hidden_size</span></span><br><span class="line">        <span class="comment"># y_history: batch_size * (T-1)</span></span><br><span class="line">        <span class="comment"># Initialize hidden and cell, 1 * batch_size * decoder_hidden_size</span></span><br><span class="line">        hidden = self.init_hidden(input_encoded)</span><br><span class="line">        cell = self.init_hidden(input_encoded)</span><br><span class="line">        <span class="comment"># hidden.requires_grad = False</span></span><br><span class="line">        <span class="comment"># cell.requires_grad = False</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(self.T - <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># Eqn. 12-13: compute attention weights</span></span><br><span class="line">            <span class="comment">## batch_size * T * (2*decoder_hidden_size + encoder_hidden_size)</span></span><br><span class="line">            x = torch.cat((hidden.repeat(self.T - <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>),</span><br><span class="line">                           cell.repeat(self.T - <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), input_encoded), dim=<span class="number">2</span>)</span><br><span class="line">            x = F.softmax(self.attn_layer(x.view(<span class="number">-1</span>, <span class="number">2</span> * self.decoder_hidden_size + self.encoder_hidden_size</span><br><span class="line">                                                 )).view(<span class="number">-1</span>, self.T - <span class="number">1</span>))  <span class="comment"># batch_size * T - 1, row sum up to 1</span></span><br><span class="line">            <span class="comment"># Eqn. 14: compute context vector</span></span><br><span class="line">            context = torch.bmm(x.unsqueeze(<span class="number">1</span>), input_encoded)[:, <span class="number">0</span>, :]  <span class="comment"># batch_size * encoder_hidden_size</span></span><br><span class="line">            <span class="keyword">if</span> t &lt; self.T - <span class="number">1</span>:</span><br><span class="line">                <span class="comment"># Eqn. 15</span></span><br><span class="line">                y_tilde = self.fc(torch.cat((context, y_history[:, t].unsqueeze(<span class="number">1</span>)), dim=<span class="number">1</span>))  <span class="comment"># batch_size * 1</span></span><br><span class="line">                <span class="comment"># Eqn. 16: LSTM</span></span><br><span class="line">                self.lstm_layer.flatten_parameters()</span><br><span class="line">                _, lstm_output = self.lstm_layer(y_tilde.unsqueeze(<span class="number">0</span>), (hidden, cell))</span><br><span class="line">                hidden = lstm_output[<span class="number">0</span>]  <span class="comment"># 1 * batch_size * decoder_hidden_size</span></span><br><span class="line">                cell = lstm_output[<span class="number">1</span>]  <span class="comment"># 1 * batch_size * decoder_hidden_size</span></span><br><span class="line">        <span class="comment"># Eqn. 22: final output</span></span><br><span class="line">        y_pred = self.fc_final(torch.cat((hidden[<span class="number">0</span>], context), dim=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># self.logger.info("hidden %s context %s y_pred: %s", hidden[0][0][:10], context[0][:10], y_pred[:10])</span></span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> Variable(x.data.new(<span class="number">1</span>, x.size(<span class="number">0</span>), self.decoder_hidden_size).zero_())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_data</span><span class="params">(dat, col_names)</span>:</span></span><br><span class="line">    scale = StandardScaler().fit(dat)</span><br><span class="line">    proc_dat = scale.transform(dat)</span><br><span class="line"></span><br><span class="line">    mask = np.ones(proc_dat.shape[<span class="number">1</span>], dtype=bool)</span><br><span class="line">    dat_cols = list(dat.columns)</span><br><span class="line">    <span class="keyword">for</span> col_name <span class="keyword">in</span> col_names:</span><br><span class="line">        mask[dat_cols.index(col_name)] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    feats = proc_dat[:, mask]</span><br><span class="line">    targs = proc_dat[:, ~mask]</span><br><span class="line">    <span class="keyword">return</span> feats, targs, scale</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">da_rnn</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, file_data, encoder_hidden_size=<span class="number">64</span>, decoder_hidden_size=<span class="number">64</span>, T=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 learning_rate=<span class="number">0.01</span>, batch_size=<span class="number">128</span>, parallel=True, debug=False)</span>:</span></span><br><span class="line">        self.T = T</span><br><span class="line">        dat = pd.read_csv(file_data, nrows=<span class="number">100</span> <span class="keyword">if</span> debug <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">        <span class="comment"># read first 100 rows</span></span><br><span class="line">        <span class="comment"># self.logger.info("Shape of data: %s.\nMissing in data: %s.", dat.shape, dat.isnull().sum().sum())</span></span><br><span class="line">        <span class="comment"># scale = StandardScaler().fit(dat.values)</span></span><br><span class="line">        <span class="comment"># dat = pd.DataFrame(scale.transform(dat.values))</span></span><br><span class="line">        <span class="comment"># self.X = dat.loc[:, [x for x in dat.columns.tolist() if x != 'NDX']].as_matrix()</span></span><br><span class="line">        self.X, self.y, self.scaler = preprocess_data(dat, (<span class="string">"NDX"</span>,))</span><br><span class="line">        <span class="comment"># select matrix without NDX</span></span><br><span class="line">        <span class="comment"># (ndarray:(40560,81))</span></span><br><span class="line">        self.y = (self.y).reshape((self.y).shape[<span class="number">0</span>],)</span><br><span class="line">        <span class="comment"># self.y = np.array(dat.NDX)</span></span><br><span class="line">        <span class="comment"># (ndarray:(40560,))</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        <span class="comment"># 128</span></span><br><span class="line">        self.encoder = EncoderAtt(input_size=self.X.shape[<span class="number">1</span>], hidden_size=encoder_hidden_size, T=T).to(device)</span><br><span class="line">        self.decoder = DecoderAtt(encoder_hidden_size=encoder_hidden_size, decoder_hidden_size=decoder_hidden_size, T=T).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> parallel:</span><br><span class="line">            self.encoder = nn.DataParallel(self.encoder)</span><br><span class="line">            self.decoder = nn.DataParallel(self.decoder)</span><br><span class="line">        <span class="comment">#  multiple GPU training</span></span><br><span class="line"></span><br><span class="line">        self.encoder_optimizer = optim.Adam(params=filter(<span class="keyword">lambda</span> p: p.requires_grad, self.encoder.parameters()),</span><br><span class="line">                                           lr=learning_rate)</span><br><span class="line">        self.decoder_optimizer = optim.Adam(params=filter(<span class="keyword">lambda</span> p: p.requires_grad, self.decoder.parameters()),</span><br><span class="line">                                           lr=learning_rate)</span><br><span class="line">        <span class="comment"># self.learning_rate = learning_rate</span></span><br><span class="line"></span><br><span class="line">        self.train_size = int(self.X.shape[<span class="number">0</span>] * <span class="number">0.7</span>)</span><br><span class="line">        <span class="comment"># &#123;int&#125; 28392</span></span><br><span class="line">        <span class="comment"># self.y = self.y - np.mean(self.y[:self.train_size])</span></span><br><span class="line">        <span class="comment"># self.y = (self.y - np.mean(self.y[:self.train_size])) / np.std(self.y[:self.train_size])</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Question: why Adam requires data to be normalized?</span></span><br><span class="line">        <span class="comment"># self.logger.info("Training size: %d.", self.train_size)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, n_epochs=<span class="number">10</span>)</span>:</span></span><br><span class="line">        iter_per_epoch = int(np.ceil(self.train_size * <span class="number">1.</span> / self.batch_size))</span><br><span class="line">        print(<span class="string">"Iterations per epoch: %3.3f ~ %d."</span>, self.train_size * <span class="number">1.</span> / self.batch_size, iter_per_epoch)</span><br><span class="line">        self.iter_losses = np.zeros(n_epochs * iter_per_epoch)</span><br><span class="line">        self.epoch_losses = np.zeros(n_epochs)</span><br><span class="line"></span><br><span class="line">        self.loss_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">        n_iter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        learning_rate = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">            perm_idx = np.random.permutation(self.train_size - self.T)</span><br><span class="line">            j = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> j &lt; self.train_size:</span><br><span class="line">                batch_idx = perm_idx[j:(j + self.batch_size)]</span><br><span class="line">                X = np.zeros((len(batch_idx), self.T - <span class="number">1</span>, self.X.shape[<span class="number">1</span>]))</span><br><span class="line">                y_history = np.zeros((len(batch_idx), self.T - <span class="number">1</span>))</span><br><span class="line">                y_target = self.y[batch_idx + self.T]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(len(batch_idx)):</span><br><span class="line">                    X[k, :, :] = self.X[batch_idx[k] : (batch_idx[k] + self.T - <span class="number">1</span>), :]</span><br><span class="line">                    y_history[k, :] = self.y[batch_idx[k]: (batch_idx[k] + self.T - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">                loss = self.train_iteration(X, y_history, y_target)</span><br><span class="line">                self.iter_losses[int(i * iter_per_epoch + j / self.batch_size)] = loss</span><br><span class="line">                <span class="comment">#if (j / self.batch_size) % 50 == 0:</span></span><br><span class="line">                <span class="comment">#    self.logger.info("Epoch %d, Batch %d: loss = %3.3f.", i, j / self.batch_size, loss)</span></span><br><span class="line">                j += self.batch_size</span><br><span class="line">                n_iter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> n_iter % <span class="number">10000</span> == <span class="number">0</span> <span class="keyword">and</span> n_iter &gt; <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">for</span> param_group <span class="keyword">in</span> self.encoder_optimizer.param_groups:</span><br><span class="line">                        param_group[<span class="string">'lr'</span>] = param_group[<span class="string">'lr'</span>] * <span class="number">0.9</span></span><br><span class="line">                    <span class="keyword">for</span> param_group <span class="keyword">in</span> self.decoder_optimizer.param_groups:</span><br><span class="line">                        param_group[<span class="string">'lr'</span>] = param_group[<span class="string">'lr'</span>] * <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line">            self.epoch_losses[i] = np.mean(self.iter_losses[range(i * iter_per_epoch, (i + <span class="number">1</span>) * iter_per_epoch)])</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Epoch %d, loss: %3.3f."</span> % (i, self.epoch_losses[i]))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                y_train_pred = self.predict(on_train=<span class="literal">True</span>)  <span class="comment"># 28383</span></span><br><span class="line">                y_test_pred = self.predict(on_train=<span class="literal">False</span>)  <span class="comment"># 12168</span></span><br><span class="line">                y_pred = np.concatenate((y_train_pred, y_test_pred))    <span class="comment"># 40551</span></span><br><span class="line">                <span class="comment"># X (40560,)</span></span><br><span class="line">                <span class="comment"># y (40560,)</span></span><br><span class="line">                print(y_train_pred.shape, y_test_pred.shape, y_pred.shape)</span><br><span class="line">                print((self.y).shape,(self.X).shape)</span><br><span class="line">                <span class="comment"># (40560,) (40560, 81)</span></span><br><span class="line">                true = concatenate(((self.y).reshape(self.y.shape[<span class="number">0</span>], <span class="number">1</span>), self.X), axis=<span class="number">1</span>)</span><br><span class="line">                true = self.scaler.inverse_transform(true)</span><br><span class="line">                self.y = true[:, <span class="number">0</span>]</span><br><span class="line">                <span class="comment"># true [1,40560] len = 40560</span></span><br><span class="line">                print(self.T, len(y_train_pred) + self.T)</span><br><span class="line">                <span class="comment"># 10 28393</span></span><br><span class="line">                print(self.T + len(y_train_pred), len(self.y) + <span class="number">1</span>)</span><br><span class="line">                <span class="comment"># 28393 40561</span></span><br><span class="line">                y_train_pred = concatenate((y_train_pred.reshape(y_train_pred.shape[<span class="number">0</span>], <span class="number">1</span>), self.X[self.T<span class="number">-1</span>: len(y_train_pred) + self.T<span class="number">-1</span>]), axis=<span class="number">1</span>)</span><br><span class="line">                y_train_pred = self.scaler.inverse_transform(y_train_pred)</span><br><span class="line">                y_train_pred = y_train_pred[:, <span class="number">0</span>]</span><br><span class="line">                <span class="comment"># y_train_pred [10,28392] len = 28383</span></span><br><span class="line">                y_test_pred = concatenate((y_test_pred.reshape(y_test_pred.shape[<span class="number">0</span>], <span class="number">1</span>), self.X[self.T + len(y_train_pred)<span class="number">-1</span>:]), axis=<span class="number">1</span>)</span><br><span class="line">                y_test_pred = self.scaler.inverse_transform(y_test_pred)</span><br><span class="line">                y_test_pred = y_test_pred[:, <span class="number">0</span>]</span><br><span class="line">                <span class="comment"># y_test_pred [28393,40560] len = 12168</span></span><br><span class="line">                plt.figure()</span><br><span class="line">                plt.plot(range(<span class="number">1</span>, <span class="number">1</span> + len(self.y)), self.y, label=<span class="string">"True"</span>)</span><br><span class="line">                plt.plot(range(self.T, len(y_train_pred) + self.T), y_train_pred, label = <span class="string">'Predicted - Train'</span>)</span><br><span class="line">                plt.plot(range(self.T + len(y_train_pred), len(self.y) + <span class="number">1</span>), y_test_pred, label = <span class="string">'Predicted - Test'</span>)</span><br><span class="line">                plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">                plt.savefig(<span class="string">'./resultpic/epoch_%d.jpg'</span> % i)</span><br><span class="line">                plt.show()</span><br><span class="line"></span><br><span class="line">        y_train_pred = self.predict(on_train=<span class="literal">True</span>)</span><br><span class="line">        y_test_pred = self.predict(on_train=<span class="literal">False</span>)</span><br><span class="line">        y_pred = np.concatenate((y_train_pred, y_test_pred))</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.plot(range(<span class="number">1</span>, <span class="number">1</span> + len(self.y)), self.y, label=<span class="string">"True"</span>)</span><br><span class="line">        plt.plot(range(self.T, len(y_train_pred) + self.T), y_train_pred, label=<span class="string">'Predicted - Train'</span>)</span><br><span class="line">        plt.plot(range(self.T + len(y_train_pred), len(self.y) + <span class="number">1</span>), y_test_pred, label=<span class="string">'Predicted - Test'</span>)</span><br><span class="line">        plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">        plt.savefig(<span class="string">'./resultpic/final.jpg'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_iteration</span><span class="params">(self, X, y_history, y_target)</span>:</span></span><br><span class="line">        self.encoder_optimizer.zero_grad()</span><br><span class="line">        self.decoder_optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        input_weighted, input_encoded = self.encoder(Variable(torch.from_numpy(X).type(torch.FloatTensor).to(device)))</span><br><span class="line">        y_pred = self.decoder(input_encoded, Variable(torch.from_numpy(y_history).type(torch.FloatTensor).to(device)))</span><br><span class="line">        y_pred = y_pred.view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># print('y_pred', y_pred.shape)</span></span><br><span class="line">        y_true = Variable(torch.from_numpy(y_target).type(torch.FloatTensor).to(device))</span><br><span class="line">        <span class="comment"># print('y_true', y_true.shape)</span></span><br><span class="line">        loss = self.loss_func(y_pred, y_true)</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        self.encoder_optimizer.step()</span><br><span class="line">        self.decoder_optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, on_train = False)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> on_train:</span><br><span class="line">            y_pred = np.zeros(self.train_size - self.T + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_pred = np.zeros(self.X.shape[<span class="number">0</span>] - self.train_size)</span><br><span class="line"></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; len(y_pred):</span><br><span class="line">            batch_idx = np.array(range(len(y_pred)))[i : (i + self.batch_size)]</span><br><span class="line">            X = np.zeros((len(batch_idx), self.T - <span class="number">1</span>, self.X.shape[<span class="number">1</span>]))</span><br><span class="line">            y_history = np.zeros((len(batch_idx), self.T - <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(batch_idx)):</span><br><span class="line">                <span class="keyword">if</span> on_train:</span><br><span class="line">                    X[j, :, :] = self.X[range(batch_idx[j], batch_idx[j] + self.T - <span class="number">1</span>), :]</span><br><span class="line">                    y_history[j, :] = self.y[range(batch_idx[j],  batch_idx[j]+ self.T - <span class="number">1</span>)]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    X[j, :, :] = self.X[range(batch_idx[j] + self.train_size - self.T, batch_idx[j] + self.train_size - <span class="number">1</span>), :]</span><br><span class="line">                    y_history[j, :] = self.y[range(batch_idx[j] + self.train_size - self.T,  batch_idx[j]+ self.train_size - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">            y_history = Variable(torch.from_numpy(y_history).type(torch.FloatTensor).to(device))</span><br><span class="line">            _, input_encoded = self.encoder(Variable(torch.from_numpy(X).type(torch.FloatTensor).to(device)))</span><br><span class="line">            y_pred[i:(i + self.batch_size)] = self.decoder(input_encoded, y_history).cpu().data.numpy()[:, <span class="number">0</span>]</span><br><span class="line">            i += self.batch_size</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">io_dir = <span class="string">'nasdaq100_padding.csv'</span></span><br><span class="line"></span><br><span class="line">model = da_rnn(file_data=<span class="string">'&#123;&#125;'</span>.format(io_dir), parallel=<span class="literal">False</span>, learning_rate=<span class="number">.001</span>)</span><br><span class="line"></span><br><span class="line">model.train(n_epochs=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">y_pred = model.predict()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.semilogy(range(len(model.iter_losses)), model.iter_losses)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.semilogy(range(len(model.epoch_losses)), model.epoch_losses)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(y_pred, label = <span class="string">'Predicted'</span>)</span><br><span class="line">plt.plot(model.y[model.train_size:], label = <span class="string">"True"</span>)</span><br><span class="line">plt.legend(loc = <span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Bugs fix for&lt;br&gt;&lt;a href=&quot;https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py&quot;&gt;https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Learn-Pytorch-Variable</title>
    <link href="http://arithmeticjia.github.io/2019/12/10/Learn-Pytorch-Variable/"/>
    <id>http://arithmeticjia.github.io/2019/12/10/Learn-Pytorch-Variable/</id>
    <published>2019-12-10T11:01:51.000Z</published>
    <updated>2019-12-10T11:14:08.493Z</updated>
    
    <content type="html"><![CDATA[<p>Tensor是Pytorch的一个完美组件(可以生成高维数组)，但是要构建神经网络还是远远不够的，我们需要能够计算图的Tensor，那就是Variable。Variable是对Tensor的一个封装，操作和Tensor是一样的，但是每个Variable都有三个属性，Varibale的Tensor本身的.data，对应Tensor的梯度.grad，以及这个Variable是通过什么方式得到的.grad_fn<br><a id="more"></a></p><h4 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h4><p>autograd.Variable 是包的核心类. 它包装了张量, 并且支持几乎所有的操作. 一旦你完成了你的计算, 你就可以调用 .backward() 方法, 然后所有的梯度计算会自动进行.你还可以通过 .data 属性来访问原始的张量, 而关于该 variable（变量）的梯度会被累计到 .grad上去.还有一个针对自动求导实现来说非常重要的类 - Function.Variable 和 Function 是相互联系的, 并且它们构建了一个非循环的图, 编码了一个完整的计算历史信息. 每一个 variable（变量）都有一个 .grad_fn 属性, 它引用了一个已经创建了 Variable 的 Function （除了用户创建的 Variable <code>之外 - 它们的</code>grad_fn is None ）.如果你想计算导数, 你可以在 Variable 上调用 .backward() 方法. 如果 Variable 是标量的形式（例如, 它包含一个元素数据）, 你不必指定任何参数给 backward(), 但是, 如果它有更多的元素. 你需要去指定一个 grad_output 参数, 该参数是一个匹配 shape（形状）的张量.</p><h4 id="创建一个2×2的变量"><a href="#创建一个2×2的变量" class="headerlink" title="创建一个2×2的变量"></a>创建一个2×2的变量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">View more, visit my tutorial page: https://arithmeticjia.github.io</span></span><br><span class="line"><span class="string">My Blog: https://www.guanacossj.com</span></span><br><span class="line"><span class="string">Dependencies:</span></span><br><span class="line"><span class="string">torch: 1.3.0</span></span><br><span class="line"><span class="string">matplotlib</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># Variable in torch is to build a computational graph,</span></span><br><span class="line"><span class="comment"># but this graph is dynamic compared with a static graph in Tensorflow or Theano.</span></span><br><span class="line"><span class="comment"># So torch does not have placeholder, torch can just pass variable to the computational graph.</span></span><br><span class="line"></span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])            <span class="comment"># build a tensor</span></span><br><span class="line">variable = Variable(tensor, requires_grad=<span class="literal">True</span>)      <span class="comment"># build a variable, usually for compute gradients</span></span><br><span class="line"></span><br><span class="line">print(tensor)       <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br><span class="line">print(variable)     <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]])</span><br><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]], requires_grad=True)</span><br></pre></td></tr></table></figure><h4 id="计算变量的点乘积、梯度"><a href="#计算变量的点乘积、梯度" class="headerlink" title="计算变量的点乘积、梯度"></a>计算变量的点乘积、梯度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">View more, visit my tutorial page: https://arithmeticjia.github.io</span></span><br><span class="line"><span class="string">My Blog: https://www.guanacossj.com</span></span><br><span class="line"><span class="string">Dependencies:</span></span><br><span class="line"><span class="string">torch: 1.3.0</span></span><br><span class="line"><span class="string">matplotlib</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># Variable in torch is to build a computational graph,</span></span><br><span class="line"><span class="comment"># but this graph is dynamic compared with a static graph in Tensorflow or Theano.</span></span><br><span class="line"><span class="comment"># So torch does not have placeholder, torch can just pass variable to the computational graph.</span></span><br><span class="line"></span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])            <span class="comment"># build a tensor</span></span><br><span class="line">variable = Variable(tensor, requires_grad=<span class="literal">True</span>)      <span class="comment"># build a variable, usually for compute gradients</span></span><br><span class="line"></span><br><span class="line">print(tensor)       <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br><span class="line">print(variable)     <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># till now the tensor and variable seem the same.</span></span><br><span class="line"><span class="comment"># However, the variable is a part of the graph, it's a part of the auto-gradient.</span></span><br><span class="line"></span><br><span class="line">t_out = torch.mean(tensor*tensor)       <span class="comment"># x^2</span></span><br><span class="line">v_out = torch.mean(variable*variable)   <span class="comment"># x^2</span></span><br><span class="line">print(t_out)</span><br><span class="line">print(v_out)                            <span class="comment"># 7.5</span></span><br><span class="line"></span><br><span class="line">print(variable*variable)</span><br><span class="line"><span class="comment"># 点乘操作</span></span><br><span class="line">print(torch.mm(variable,variable))</span><br><span class="line"><span class="comment"># 矩阵相乘</span></span><br><span class="line">v_out.backward()    <span class="comment"># backpropagation from v_out</span></span><br><span class="line"><span class="comment"># v_out = 1/4 * sum(variable*variable)</span></span><br><span class="line"><span class="comment"># the gradients w.r.t the variable, d(v_out)/d(variable) = 1/4*2*variable = variable/2</span></span><br><span class="line">print(variable.grad)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"> 0.5000  1.0000</span></span><br><span class="line"><span class="string"> 1.5000  2.0000</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]])</span><br><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]], requires_grad=True)</span><br><span class="line">tensor(7.5000)</span><br><span class="line">tensor(7.5000, grad_fn=&lt;MeanBackward0&gt;)</span><br><span class="line">tensor([[ 1.,  4.],</span><br><span class="line">        [ 9., 16.]], grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line">tensor([[ 7., 10.],</span><br><span class="line">        [15., 22.]], grad_fn=&lt;MmBackward&gt;)</span><br><span class="line">tensor([[0.5000, 1.0000],</span><br><span class="line">        [1.5000, 2.0000]])</span><br></pre></td></tr></table></figure><p>注意这里的变量的点乘和相乘的区别</p><h4 id="查看变量的数据"><a href="#查看变量的数据" class="headerlink" title="查看变量的数据"></a>查看变量的数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">print(variable)     <span class="comment"># this is data in variable format</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">print(variable.data)    <span class="comment"># this is data in tensor format</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]], requires_grad=True)</span><br><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]])</span><br></pre></td></tr></table></figure><h4 id="Variable转numpy"><a href="#Variable转numpy" class="headerlink" title="Variable转numpy"></a>Variable转numpy</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(variable.data.numpy())    <span class="comment"># numpy format</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[ 1.  2.]</span></span><br><span class="line"><span class="string"> [ 3.  4.]]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[1. 2.]</span><br><span class="line"> [3. 4.]]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Tensor是Pytorch的一个完美组件(可以生成高维数组)，但是要构建神经网络还是远远不够的，我们需要能够计算图的Tensor，那就是Variable。Variable是对Tensor的一个封装，操作和Tensor是一样的，但是每个Variable都有三个属性，Varibale的Tensor本身的.data，对应Tensor的梯度.grad，以及这个Variable是通过什么方式得到的.grad_fn&lt;br&gt;
    
    </summary>
    
    
      <category term="Pytorch" scheme="http://arithmeticjia.github.io/categories/Pytorch/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://arithmeticjia.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>2019-12-13周报</title>
    <link href="http://arithmeticjia.github.io/2019/12/10/2019-12-13%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2019/12/10/2019-12-13%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2019-12-10T09:46:58.000Z</published>
    <updated>2019-12-20T10:42:09.188Z</updated>
    
    <content type="html"><![CDATA[<p>Works between 2019/12/07-2019/12/13<br><a id="more"></a></p><h4 id="A-Dual-Stage-Attention-Based-Recurrent-Neural-Network-for-Time-Series-Prediction"><a href="#A-Dual-Stage-Attention-Based-Recurrent-Neural-Network-for-Time-Series-Prediction" class="headerlink" title="A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction"></a>A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction</h4><h5 id="Theory"><a href="#Theory" class="headerlink" title="Theory"></a>Theory</h5><p><img src="https://www.guanacossj.com/media/articlebodypics/lstm-attention.jpg" alt=""></p><ul><li>Bi-LSTM + Attention 就是在Bi-LSTM的模型上加入Attention层，在Bi-LSTM中我们会用最后一个时序的输出向量 作为特征向量，然后进行softmax分类。Attention是先计算每个时序的权重，然后将所有时序 的向量进行加权和作为特征向量，然后进行softmax分类</li><li>sigmoid把一个real value映射到(0,1)的区间(当然也可以是(-1,1)),这样可以用来做二分类</li><li>softmax把一个k维的real value向量(a1,a2,a3,a4…)映射成一个(b1,b2,b3,b4…)其中bi是一个0-1的常数，然后可以根据bi的大小来进行多分类的任务，如取权重最大的一维</li><li>传统的注意力机制只用在解码器的输入阶段，即对不同时刻产生不同的context vector不同，该文还在编码器的输入阶段引入了注意力机制，从而同时实现了选取特征因子(feature selection)和把握长期时序依赖关系(long-term temporal dependencies)</li><li>第一阶段，使用注意力机制自适应地提取每个时刻的相关feature<script type="math/tex; mode=display">e_{t}^{k}=v_{e}^{T}tanh(W_{e}[h_{t-1};s_{t-1}]+U_{e}x^{k})</script></li><li>用softmax函数将其归一化<script type="math/tex; mode=display">\alpha _{t}^{k}=\frac{exp(e_{t}^{k})}{\sum_{i-1}^{n}exp(e_{t}^{i})}</script></li><li>得到更新后的x<script type="math/tex; mode=display">\tilde{x} = (\alpha _{t}^{1}x_{t}^{1}, \alpha _{t}^{2}x_{t}^{2},...,\alpha _{t}^{n}x_{t}^{n})</script></li><li>选取LSTM作为编码器<script type="math/tex">f_{1}</script><script type="math/tex; mode=display">h_{t} = f_{1}(h_{t-1},  \tilde{x})</script></li><li><p>第二阶段，使用另一个注意力机制选取与之相关的encoder hidden states</p><script type="math/tex; mode=display">c_{t}^{'} = \sum_{t-1}^{T}\beta _{t^{'}}^{t}h_{t}</script></li><li><p><script type="math/tex">\beta _{t^{'}}^{t}</script>的设计类似于Bahanau的工作，基于前一个时刻解码器的hidden state[公式]和cell state[公式]计算得到：</p></li></ul><h5 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 10</span><br><span class="line"><span class="keyword">Test </span>RMSE: 0.259</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.056</span><br><span class="line"><span class="keyword">Test </span>Data: all</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/pred_0.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/final_predicted_reloaded.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/final_predicted_reloaded_standard.png" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 10</span><br><span class="line"><span class="keyword">Test </span>RMSE: 0.327</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.275</span><br><span class="line"><span class="keyword">Test </span>Data: all * 0.3</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/final_predicted.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/final_predicted_standard.png" alt=""></p><h4 id="Vm1-Power-Matrix-From-Paper"><a href="#Vm1-Power-Matrix-From-Paper" class="headerlink" title="Vm1-Power-Matrix-From-Paper"></a>Vm1-Power-Matrix-From-Paper</h4><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 10</span><br><span class="line"><span class="keyword">Test </span>RMSE: 2.602</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.207</span><br><span class="line"><span class="keyword">Test </span>Data: all<span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/all_server_final_predicted_reloaded-larger.png" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 10</span><br><span class="line"><span class="keyword">Test </span>RMSE: 3.057</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.361</span><br><span class="line"><span class="keyword">Test </span>Data: 49000<span class="string">-50000</span><span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/49000_50000_server_final_predicted_reloaded.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/49000_50000_server_final_predicted_reloaded-larger.png" alt=""><br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 10</span><br><span class="line"><span class="keyword">Test </span>RMSE: 2.949</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.259</span><br><span class="line"><span class="keyword">Test </span>Data: 32000<span class="string">-33000</span><span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><br><img src="https://www.guanacossj.com/media/articlebodypics/32000_33000_server_final_predicted_reloaded-larger.png" alt=""></p><h4 id="Vm1-Power-Matrix-From-Jia"><a href="#Vm1-Power-Matrix-From-Jia" class="headerlink" title="Vm1-Power-Matrix-From-Jia"></a>Vm1-Power-Matrix-From-Jia</h4><h5 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Training Data: 1111<span class="string">-1115</span></span><br><span class="line"><span class="keyword">Testing </span>Data: 1116<span class="string">-1120</span></span><br></pre></td></tr></table></figure><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="params">(n,m)</span> -&gt;</span> <span class="function"><span class="params">(n,m * timestamp)</span> -&gt;</span> (n,timestamp,m)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    var1(t<span class="number">-60</span>)  var2(t<span class="number">-60</span>)  var3(t<span class="number">-60</span>)  ...   var5(t)   var6(t)   var7(t)</span><br><span class="line"><span class="number">60</span>    <span class="number">0.655771</span>   <span class="number">-1.060919</span>   <span class="number">-0.735941</span>  ... <span class="number">-0.447461</span> <span class="number">-0.217168</span> <span class="number">-0.030555</span></span><br><span class="line"><span class="number">61</span>    <span class="number">0.560612</span>   <span class="number">-1.060919</span>   <span class="number">-0.735941</span>  ... <span class="number">-0.023654</span> <span class="number">-0.222058</span> <span class="number">-0.030555</span></span><br><span class="line"><span class="number">62</span>    <span class="number">0.655771</span>   <span class="number">-1.060919</span>   <span class="number">-0.735941</span>  ... <span class="number">-0.222916</span> <span class="number">-0.207537</span> <span class="number">-0.030555</span></span><br><span class="line"><span class="number">63</span>    <span class="number">0.465453</span>   <span class="number">-1.060919</span>   <span class="number">-0.735941</span>  ... <span class="number">-0.638782</span> <span class="number">-0.222058</span> <span class="number">-0.030555</span></span><br><span class="line"><span class="number">64</span>   <span class="number">-0.010342</span>   <span class="number">-1.047501</span>   <span class="number">-0.735941</span>  ...  <span class="number">1.133103</span> <span class="number">-0.222058</span> <span class="number">-0.030555</span></span><br></pre></td></tr></table></figure><h5 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h5><ul><li>Mean Normaliztion(均值归一化)</li></ul><script type="math/tex; mode=display">x^{*} = \frac{x-\mu}{\sigma }</script><ul><li>Min-Max Normalization</li></ul><script type="math/tex; mode=display">x^{*} = \frac{x-min}{max-min }</script><h5 id="Bi-GRU-Attention"><a href="#Bi-GRU-Attention" class="headerlink" title="Bi-GRU + Attention"></a>Bi-GRU + Attention</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 20</span><br><span class="line"><span class="keyword">Test </span>RMSE: 3.054</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.243</span><br><span class="line"><span class="keyword">Test </span>Data: all<span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/gru-bi-20-all-att-1620.jpg" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 20</span><br><span class="line"><span class="keyword">Test </span>RMSE: 3.458</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.418</span><br><span class="line"><span class="keyword">Test </span>Data: 49000<span class="string">-50000</span><span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/49000_50000_n_final_gru_pro_attention_bi_20.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/mark_49000_50000_n_final_gru_pro_attention_bi_20.png" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 20</span><br><span class="line"><span class="keyword">Test </span>RMSE: 3.196</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.287</span><br><span class="line"><span class="keyword">Test </span>Data: 32000<span class="string">-33000</span><span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/32000_33000_n_final_gru_pro_attention_bi_20.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/mark_32000_33000_n_final_gru_pro_attention_bi_20.png" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 20</span><br><span class="line"><span class="keyword">Test </span>RMSE: 4.573</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.45</span><br><span class="line"><span class="keyword">Test </span>Data: 70000<span class="string">-70500</span><span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/70000-70500-gru-bi-att-20-1620.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/mark_70000-70500-gru-bi-att-20-1620.jpg" alt=""></p><h5 id="Bi-LSTM-Attention"><a href="#Bi-LSTM-Attention" class="headerlink" title="Bi-LSTM + Attention"></a>Bi-LSTM + Attention</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 20</span><br><span class="line"><span class="keyword">Test </span>RMSE: 2.776</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.221</span><br><span class="line"><span class="keyword">Test </span>Data: all<span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/lstm-bi-20-att-all-1620.jpg" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 20</span><br><span class="line"><span class="keyword">Test </span>RMSE: 4.596</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.460</span><br><span class="line"><span class="keyword">Test </span>Data: 70000<span class="string">-70500</span><span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/70000-70500-lstm-bi-att-20-01-1620.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/mark_70000-70500-lstm-bi-att-20-01-1620.jpg" alt=""></p><h5 id="Bi-LSTM-Only"><a href="#Bi-LSTM-Only" class="headerlink" title="Bi-LSTM Only"></a>Bi-LSTM Only</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 500</span><br><span class="line">Train-Data: 1111<span class="string">-1115</span></span><br><span class="line">Test-Data: 1116<span class="string">-1120</span></span><br><span class="line"><span class="keyword">Test </span>RMSE: 3.880</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.308</span><br></pre></td></tr></table></figure><p>Test Data: all-1116-1120<br><img src="https://www.guanacossj.com/media/articlebodypics/lstm-bi-500-all-1620.jpg" alt=""></p><p>Test Data: 50000-50500-1116-1120<br><img src="https://www.guanacossj.com/media/articlebodypics/lstm-bi-500-01-1620.jpg" alt=""></p><p>Test Data: 60000-60500-1116-1120<br><img src="https://www.guanacossj.com/media/articlebodypics/lstm-bi-500-02-1620.jpg" alt=""></p><p>Test Data: 30000-30500-1116-1120<br><img src="https://www.guanacossj.com/media/articlebodypics/lstm-bi-500-03-1620.jpg" alt=""></p><h4 id="Bi-LSTM-Attenion-amp-GRU-LSTM-Attenion-In-Stock-With-Pytorch"><a href="#Bi-LSTM-Attenion-amp-GRU-LSTM-Attenion-In-Stock-With-Pytorch" class="headerlink" title="Bi-LSTM-Attenion &amp; GRU-LSTM-Attenion In Stock With Pytorch"></a>Bi-LSTM-Attenion &amp; GRU-LSTM-Attenion In Stock With Pytorch</h4><h5 id="Bi-GRU"><a href="#Bi-GRU" class="headerlink" title="Bi-GRU"></a>Bi-GRU</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 66.403</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.077</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/pytorch-bi-gru-stock.png" alt=""></p><h5 id="Bi-LSTM"><a href="#Bi-LSTM" class="headerlink" title="Bi-LSTM"></a>Bi-LSTM</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 88.421</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.103</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/pytorch-bi-lstm-stock.png" alt=""></p><h5 id="Bi-GRU-Attention-1"><a href="#Bi-GRU-Attention-1" class="headerlink" title="Bi-GRU-Attention"></a>Bi-GRU-Attention</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 86.775</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.101</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/pytorch-bi-gru-att-stock.png" alt=""></p><h5 id="Bi-LSTM-Attention-1"><a href="#Bi-LSTM-Attention-1" class="headerlink" title="Bi-LSTM-Attention"></a>Bi-LSTM-Attention</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 86.588</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.101</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/pytorch-bi-lstm-att-stock.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Works between 2019/12/07-2019/12/13&lt;br&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Django个人博客搭建教程-Django-Rest-Framework外键与多对多序列化</title>
    <link href="http://arithmeticjia.github.io/2019/12/09/Django%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B-Django-Rest-Framework%E5%A4%96%E9%94%AE%E4%B8%8E%E5%A4%9A%E5%AF%B9%E5%A4%9A%E5%BA%8F%E5%88%97%E5%8C%96/"/>
    <id>http://arithmeticjia.github.io/2019/12/09/Django%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B-Django-Rest-Framework%E5%A4%96%E9%94%AE%E4%B8%8E%E5%A4%9A%E5%AF%B9%E5%A4%9A%E5%BA%8F%E5%88%97%E5%8C%96/</id>
    <published>2019-12-09T09:45:05.000Z</published>
    <updated>2019-12-09T09:56:36.727Z</updated>
    
    <content type="html"><![CDATA[<p>如果对一个含有多对多、外键的模型进行序列化，这时候这些关联的字段会只展示id，因此需要对外键或者多对多的字段进行序列化处理<br><a id="more"></a></p><h4 id="外键序列化（ForeignKey）-amp-多对多序列化（manytomany"><a href="#外键序列化（ForeignKey）-amp-多对多序列化（manytomany" class="headerlink" title="外键序列化（ForeignKey）&amp;多对多序列化（manytomany)"></a>外键序列化（ForeignKey）&amp;多对多序列化（manytomany)</h4><p>这里要序列化的模型是Articles,其中的authorname、tags、category用了外键或者多对多关联<br>关联的两个模型是Tag和Category<br>models.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Category</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        Django 要求模型必须继承 models.Model 类。</span></span><br><span class="line"><span class="string">        Category 只需要一个简单的分类名 name 就可以了。</span></span><br><span class="line"><span class="string">        CharField 指定了分类名 name 的数据类型，CharField 是字符型，</span></span><br><span class="line"><span class="string">        CharField 的 max_length 参数指定其最大长度，超过这个长度的分类名就不能被存入数据库。</span></span><br><span class="line"><span class="string">        当然 Django 还为我们提供了多种其它的数据类型，如日期时间类型 DateTimeField、整数类型 IntegerField 等等。</span></span><br><span class="line"><span class="string">        Django 内置的全部类型可查看文档：</span></span><br><span class="line"><span class="string">        https://docs.djangoproject.com/en/1.10/ref/models/fields/#field-types</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">    name = models.CharField(max_length=<span class="number">100</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">catcount</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> Articles.objects.filter(category__name__exact=self.name).filter(status=<span class="string">'有效'</span>).count()</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.name</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tag</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        标签 Tag 也比较简单，和 Category 一样。</span></span><br><span class="line"><span class="string">        再次强调一定要继承 models.Model 类！</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">    name = models.CharField(max_length=<span class="number">100</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.name</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Articles</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    id = models.AutoField(primary_key=<span class="literal">True</span>)  <span class="comment"># id</span></span><br><span class="line">    title = models.CharField(max_length=<span class="number">150</span>)  <span class="comment"># 博客标题</span></span><br><span class="line">    body = models.TextField()  <span class="comment"># 博客正文</span></span><br><span class="line">    timestamp = models.DateTimeField()  <span class="comment"># 创建时间</span></span><br><span class="line">    authorname = models.ForeignKey(<span class="string">'JiaBlog.BlogUser'</span>, on_delete=models.CASCADE)  <span class="comment"># 作者姓名</span></span><br><span class="line">    views = models.PositiveIntegerField(default=<span class="number">0</span>)</span><br><span class="line">    category = models.ForeignKey(Category, on_delete=models.CASCADE, primary_key=<span class="literal">False</span>)</span><br><span class="line">    tags = models.ManyToManyField(Tag, blank=<span class="literal">True</span>, null=<span class="literal">True</span>)</span><br><span class="line">    greats = models.PositiveIntegerField(default=<span class="number">0</span>)</span><br><span class="line">    comments = models.IntegerField(default=<span class="number">0</span>)</span><br><span class="line">    status = models.CharField(max_length=<span class="number">20</span>, default=<span class="string">"DEL"</span>)</span><br><span class="line">    brief = models.CharField(max_length=<span class="number">200</span>, blank=<span class="literal">True</span>, null=<span class="literal">True</span>)</span><br><span class="line">    pic = models.ImageField(upload_to=<span class="string">'jiablogimages'</span>)</span><br><span class="line">    <span class="comment"># bodypic = models.ImageField(upload_to='jiablogimages', blank=True, null=True)</span></span><br><span class="line">    istop = models.CharField(max_length=<span class="number">5</span>, default=<span class="string">''</span>, null=<span class="literal">True</span>, blank=<span class="literal">True</span>)</span><br><span class="line">    articlebodybrief = models.TextField(blank=<span class="literal">True</span>, null=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br>views.py<br>这里的source对应的是字段名<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticlesSerializers</span><span class="params">(serializers.ModelSerializer)</span>:</span></span><br><span class="line">    authorname = serializers.CharField(source=<span class="string">'authorname.name'</span>)</span><br><span class="line">    category = serializers.CharField(source=<span class="string">'category.name'</span>)</span><br><span class="line">    tags = serializers.StringRelatedField(many=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Meta</span>:</span></span><br><span class="line">        model = Articles  <span class="comment"># 指定的模型类</span></span><br><span class="line">        fields = (<span class="string">'id'</span>, <span class="string">'title'</span>, <span class="string">'body'</span>, <span class="string">'timestamp'</span>, <span class="string">'authorname'</span>, <span class="string">'views'</span>, <span class="string">'tags'</span>, <span class="string">'category'</span>)  <span class="comment"># 需要序列化的属性</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如果对一个含有多对多、外键的模型进行序列化，这时候这些关联的字段会只展示id，因此需要对外键或者多对多的字段进行序列化处理&lt;br&gt;
    
    </summary>
    
    
      <category term="Django" scheme="http://arithmeticjia.github.io/categories/Django/"/>
    
    
      <category term="django" scheme="http://arithmeticjia.github.io/tags/django/"/>
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="restframework" scheme="http://arithmeticjia.github.io/tags/restframework/"/>
    
  </entry>
  
  <entry>
    <title>Learn-Pytorch-使用LSTM预测航班客流量</title>
    <link href="http://arithmeticjia.github.io/2019/12/08/Learn-Pytorch-%E4%BD%BF%E7%94%A8LSTM%E9%A2%84%E6%B5%8B%E8%88%AA%E7%8F%AD%E5%AE%A2%E6%B5%81%E9%87%8F/"/>
    <id>http://arithmeticjia.github.io/2019/12/08/Learn-Pytorch-%E4%BD%BF%E7%94%A8LSTM%E9%A2%84%E6%B5%8B%E8%88%AA%E7%8F%AD%E5%AE%A2%E6%B5%81%E9%87%8F/</id>
    <published>2019-12-08T14:22:52.000Z</published>
    <updated>2019-12-08T14:37:58.046Z</updated>
    
    <content type="html"><![CDATA[<p>本文会详细讲解如何使用Pytorch预测航班客流量，包括数据的处理、网络的结构<br><a id="more"></a></p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">data_csv = pd.read_csv(<span class="string">'airline-passengers.csv'</span>,usecols=[<span class="number">1</span>])</span><br><span class="line">plt.plot(data_csv)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>效果如下<br><img src="https://www.guanacossj.com/media/articlebodypics/1575815313343.jpg" alt=""><br>这里是真实的数据，接下来我们对数据进行预处理<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">data_csv = data_csv.dropna()    <span class="comment"># 滤除缺失数据</span></span><br><span class="line">dataset = data_csv.values       <span class="comment"># 获得csv的值</span></span><br><span class="line">print((dataset,type(dataset),dataset.shape))</span><br><span class="line">dataset = dataset.astype(<span class="string">'float32'</span>)</span><br><span class="line">max_value = np.max(dataset)     <span class="comment"># 获得最大值</span></span><br><span class="line">min_value = np.min(dataset)     <span class="comment"># 获得最小值</span></span><br><span class="line">scalar = max_value - min_value  <span class="comment"># 获得间隔数量</span></span><br><span class="line">dataset = list(map(<span class="keyword">lambda</span> x: x / scalar, dataset)) <span class="comment"># 归一化</span></span><br></pre></td></tr></table></figure><br>可以看一下最后处理完成的dataset，是一个list<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0.21621622] [0.22779922] [0.25482625]...</span><br></pre></td></tr></table></figure><br>创建输入输出，这里使用当前时间的前两个时刻<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span><span class="params">(dataset, look_back=<span class="number">2</span>)</span>:</span></span><br><span class="line">    dataX, dataY = [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(dataset) - look_back):</span><br><span class="line">        a = dataset[i:(i + look_back)]</span><br><span class="line">        dataX.append(a)</span><br><span class="line">        dataY.append(dataset[i + look_back])</span><br><span class="line">    <span class="keyword">return</span> np.array(dataX), np.array(dataY)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_X, data_Y = create_dataset(dataset)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文会详细讲解如何使用Pytorch预测航班客流量，包括数据的处理、网络的结构&lt;br&gt;
    
    </summary>
    
    
      <category term="Pytorch" scheme="http://arithmeticjia.github.io/categories/Pytorch/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://arithmeticjia.github.io/tags/pytorch/"/>
    
      <category term="lstm" scheme="http://arithmeticjia.github.io/tags/lstm/"/>
    
  </entry>
  
  <entry>
    <title>Learn-Pytorch-用Pytorch写一个神经网络</title>
    <link href="http://arithmeticjia.github.io/2019/12/07/Learn-Pytorch-%E7%94%A8Pytorch%E5%86%99%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://arithmeticjia.github.io/2019/12/07/Learn-Pytorch-%E7%94%A8Pytorch%E5%86%99%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2019-12-07T10:00:11.000Z</published>
    <updated>2019-12-07T11:05:50.476Z</updated>
    
    <content type="html"><![CDATA[<p>本文将用Pytorch构建一个最简单的线性神经网络，练习Pytorch中神经网络模型的保存和重载<br><a id="more"></a></p><h4 id="定义一个线性神经网络"><a href="#定义一个线性神经网络" class="headerlink" title="定义一个线性神经网络"></a>定义一个线性神经网络</h4><script type="math/tex; mode=display">y = wx + b</script><p>这是一个基本的网络Net，它只包含一个全连接层<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net,self).__init__()</span><br><span class="line">        self.layer = nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">        self.layer.weight = nn.Parameter(torch.FloatTensor([[<span class="number">10</span>]]))</span><br><span class="line">        self.layer.bias = nn.Parameter(torch.FloatTensor([<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        y = self.layer(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>]])</span><br><span class="line">net = Net()</span><br><span class="line">linearout = net(x)</span><br><span class="line">print(linearout)</span><br></pre></td></tr></table></figure><br>这里假设输入x=1<br>y的值应为11<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[11.]], grad_fn=&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></figure></p><h4 id="保存Net的参数值"><a href="#保存Net的参数值" class="headerlink" title="保存Net的参数值"></a>保存Net的参数值</h4><p>查看网络的状态字典<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(net.state_dict())</span><br></pre></td></tr></table></figure><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OrderedDict([('layer.weight', tensor([[10.]])), ('layer.bias', tensor([1.]))])</span><br></pre></td></tr></table></figure><br>保存状态字典<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(obj=net.state_dict(), f=<span class="string">"models/net.pth"</span>)</span><br></pre></td></tr></table></figure></p><h4 id="加载Net参数值并用于新的模型"><a href="#加载Net参数值并用于新的模型" class="headerlink" title="加载Net参数值并用于新的模型"></a>加载Net参数值并用于新的模型</h4><p>重新定义一个相同结构的模型NewNet<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net,self).__init__()</span><br><span class="line">        self.layer = nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">        self.layer.weight = nn.Parameter(torch.FloatTensor([[<span class="number">10</span>]]))</span><br><span class="line">        self.layer.bias = nn.Parameter(torch.FloatTensor([<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        y = self.layer(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NewNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(NewNet, self).__init__()</span><br><span class="line">        self.layer = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.layer.weight = nn.Parameter(torch.FloatTensor([[<span class="number">0</span>]]))</span><br><span class="line">        self.layer.bias = nn.Parameter(torch.FloatTensor([<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.layer(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>]])</span><br><span class="line">net = NewNet()</span><br><span class="line">print(net.state_dict())                             <span class="comment"># 初始的NewNet的状态字典</span></span><br><span class="line">net.load_state_dict(torch.load(<span class="string">"models/net.pth"</span>))</span><br><span class="line">print(net.state_dict())                             <span class="comment"># 加载参数值的NewNet的状态字典</span></span><br></pre></td></tr></table></figure><br>net的w和b值就不再是0了，而是之前保存的模型中w和b对应的10和1<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">OrderedDict([('layer.weight', tensor([[0.]])), ('layer.bias', tensor([0.]))])</span><br><span class="line">OrderedDict([('layer.weight', tensor([[10.]])), ('layer.bias', tensor([1.]))])</span><br></pre></td></tr></table></figure></p><h4 id="优化器与epoch的保存"><a href="#优化器与epoch的保存" class="headerlink" title="优化器与epoch的保存"></a>优化器与epoch的保存</h4><p>保存优化器参数值和epoch值的主要目的是用于继续训练，保存的流程依旧是先“torch.save()”再“torch.load_state_dict()”<br>我们首先定义一个Adam优化器、一个任意的epoch值与net如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">mport torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn,optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net,self).__init__()</span><br><span class="line">        self.layer = nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">        self.layer.weight = nn.Parameter(torch.FloatTensor([[<span class="number">10</span>]]))</span><br><span class="line">        self.layer.bias = nn.Parameter(torch.FloatTensor([<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        y = self.layer(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">Adam = optim.Adam(params=net.parameters(), lr=<span class="number">0.001</span>, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">epoch = <span class="number">50</span></span><br><span class="line">all_states = &#123;<span class="string">"net"</span>: net.state_dict(), <span class="string">"Adam"</span>: Adam.state_dict(), <span class="string">"epoch"</span>: epoch&#125;</span><br><span class="line">torch.save(obj=all_states, f=<span class="string">"models/all_states.pth"</span>)</span><br></pre></td></tr></table></figure><br>查看模型所有的参数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">all_states = torch.load(<span class="string">"models/all_states.pth"</span>)</span><br><span class="line">print(all_states)</span><br></pre></td></tr></table></figure><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">'net': OrderedDict([('layer.weight', tensor([[10.]])), ('layer.bias', tensor([1.]))]),</span><br><span class="line">'Adam': &#123;</span><br><span class="line">'state': &#123;&#125;,</span><br><span class="line">'param_groups': [&#123;</span><br><span class="line">'lr': 0.001,</span><br><span class="line">'betas': (0.5, 0.999),</span><br><span class="line">'eps': 1e-08,</span><br><span class="line">'weight_decay': 0,</span><br><span class="line">'amsgrad': False,</span><br><span class="line">'params': [4660776392, 4559888248]</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">'epoch': 50</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将用Pytorch构建一个最简单的线性神经网络，练习Pytorch中神经网络模型的保存和重载&lt;br&gt;
    
    </summary>
    
    
      <category term="Pytorch" scheme="http://arithmeticjia.github.io/categories/Pytorch/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://arithmeticjia.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>2019/12/06周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2019/12/06/2019-12-06%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2019/12/06/2019-12-06%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2019-12-06T08:38:04.000Z</published>
    <updated>2019-12-06T10:22:35.734Z</updated>
    
    <content type="html"><![CDATA[<p>Nothing<br><a id="more"></a></p><h3 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h3><p>11.11-11.15<br>40万</p><h3 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cpu.all.usage.percent</span><br><span class="line">memory.used.percent</span><br><span class="line">interface.eth0.if_octets.rx</span><br><span class="line">interface.eth0.if_octets.tx</span><br><span class="line">disk.vda.disk_octets.write</span><br><span class="line">disk.vda.disk_octets.read</span><br></pre></td></tr></table></figure><h3 id="训练参数"><a href="#训练参数" class="headerlink" title="训练参数"></a>训练参数</h3><p>epoch = 200<br>bach_size = 72<br>BiLSTM + Attention</p><h3 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/1575621413951.jpg" alt=""><br><img src="https://www.guanacossj.com/media/articlebodypics/1575621002903.jpg" alt=""><br><img src="https://www.guanacossj.com/media/articlebodypics/1575627714970.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>epoch 不够多</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Nothing&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>Keras以及Tensorflow强制使用GPU</title>
    <link href="http://arithmeticjia.github.io/2019/12/06/Keras%E4%BB%A5%E5%8F%8ATensorflow%E5%BC%BA%E5%88%B6%E4%BD%BF%E7%94%A8GPU/"/>
    <id>http://arithmeticjia.github.io/2019/12/06/Keras%E4%BB%A5%E5%8F%8ATensorflow%E5%BC%BA%E5%88%B6%E4%BD%BF%E7%94%A8GPU/</id>
    <published>2019-12-06T07:39:41.000Z</published>
    <updated>2019-12-06T07:44:53.704Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍在tensorflow2.0下如何强制使用GPU，方法一在tensorflow1.x版本中失效<br><a id="more"></a></p><h3 id="环境：python3-6-tensorflow-2-0-keras2-3-1"><a href="#环境：python3-6-tensorflow-2-0-keras2-3-1" class="headerlink" title="环境：python3.6+tensorflow==2.0+keras2.3.1"></a>环境：python3.6+tensorflow==2.0+keras2.3.1</h3><h3 id="方法一："><a href="#方法一：" class="headerlink" title="方法一："></a>方法一：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras.backend.tensorflow_backend <span class="keyword">as</span> KTF</span><br><span class="line"> </span><br><span class="line">KTF.set_session(tf.Session(config=tf.ConfigProto(device_count=&#123;<span class="string">'gpu'</span>:<span class="number">0</span>&#125;)))</span><br></pre></td></tr></table></figure><p>这里在tensorflow2.0中肯定报错<br>修改如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras.backend.tensorflow_backend <span class="keyword">as</span> KTF</span><br><span class="line"> </span><br><span class="line">KTF.set_session(tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(device_count=&#123;<span class="string">'gpu'</span>:<span class="number">0</span>&#125;)))</span><br></pre></td></tr></table></figure><br>然而这样还是不行，就是告诉你tensorflow2.0中不能这样用<br>遂放弃</p><h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 python3 xxx.py</span><br></pre></td></tr></table></figure><p>貌似可行<br><img src="https://www.guanacossj.com/media/articlebodypics/1575618190733.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍在tensorflow2.0下如何强制使用GPU，方法一在tensorflow1.x版本中失效&lt;br&gt;
    
    </summary>
    
    
    
      <category term="keras" scheme="http://arithmeticjia.github.io/tags/keras/"/>
    
      <category term="tensorflow" scheme="http://arithmeticjia.github.io/tags/tensorflow/"/>
    
      <category term="gpu" scheme="http://arithmeticjia.github.io/tags/gpu/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode[840]Magic-Squares-In-Grid</title>
    <link href="http://arithmeticjia.github.io/2019/12/04/Leetcode840Magic-Squares-In-Grid/"/>
    <id>http://arithmeticjia.github.io/2019/12/04/Leetcode840Magic-Squares-In-Grid/</id>
    <published>2019-12-03T16:31:32.000Z</published>
    <updated>2019-12-04T12:30:01.712Z</updated>
    
    <content type="html"><![CDATA[<p>python3暴力解，不难看懂<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numMagicSquaresInside</span><span class="params">(self, grid)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type grid: List[List[int]]</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"></span><br><span class="line">        grids = self.generate_matrix(grid)</span><br><span class="line"></span><br><span class="line">        count  = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> grids:</span><br><span class="line">            <span class="keyword">if</span> self.checkmagic(i) == <span class="literal">True</span> <span class="keyword">and</span> self.checksame(i) == <span class="literal">True</span>:</span><br><span class="line">                count = count + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> count</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generate_matrix</span><span class="params">(self, grid)</span>:</span></span><br><span class="line">        all = []</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 生成所有矩阵组合</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> n <span class="keyword">in</span> range(len(grid[<span class="number">0</span>])<span class="number">-2</span>):</span><br><span class="line">            temmatrix = []</span><br><span class="line">            <span class="keyword">for</span> m <span class="keyword">in</span> range(len(grid)<span class="number">-2</span>):</span><br><span class="line">                tem = []</span><br><span class="line">                tem.append(grid[m][n:n + <span class="number">3</span>])</span><br><span class="line">                tem.append(grid[m + <span class="number">1</span>][n:n + <span class="number">3</span>])</span><br><span class="line">                tem.append(grid[m + <span class="number">2</span>][n:n + <span class="number">3</span>])</span><br><span class="line">                temmatrix.append(tem)</span><br><span class="line">            <span class="keyword">for</span> o <span class="keyword">in</span> range(len(temmatrix)):</span><br><span class="line">                all.append(temmatrix[o])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> all</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">checksame</span><span class="params">(self,matrix)</span>:</span></span><br><span class="line">        l = len(matrix)</span><br><span class="line">        flag = <span class="literal">True</span></span><br><span class="line">        tem = []</span><br><span class="line">        <span class="keyword">for</span> g <span class="keyword">in</span> range(l):</span><br><span class="line">            <span class="keyword">for</span> h <span class="keyword">in</span> range(l):</span><br><span class="line">                tem.append(matrix[g][h])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> tem:</span><br><span class="line">            <span class="keyword">if</span> i &gt;= <span class="number">10</span>:</span><br><span class="line">                flag = <span class="literal">False</span>        </span><br><span class="line">        <span class="keyword">if</span> len(set(tem)) == <span class="number">1</span>:</span><br><span class="line">            flag = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> flag</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">checkmagic</span><span class="params">(self, matrix)</span>:</span></span><br><span class="line">        l = len(matrix)</span><br><span class="line"></span><br><span class="line">        flag = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 斜</span></span><br><span class="line">        tmp = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(l):</span><br><span class="line">            tmp += matrix[i][i]</span><br><span class="line">        <span class="keyword">if</span> tmp != <span class="number">15</span>:</span><br><span class="line">            flag = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 行</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(l):</span><br><span class="line">            tmp = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(l):</span><br><span class="line">                tmp += matrix[i][j]</span><br><span class="line">            <span class="keyword">if</span> tmp != <span class="number">15</span>:</span><br><span class="line">                flag = <span class="literal">False</span></span><br><span class="line">        <span class="comment"># 列</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(l):</span><br><span class="line">            tmp = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(l):</span><br><span class="line">                tmp += matrix[j][i]</span><br><span class="line">            <span class="keyword">if</span> tmp != <span class="number">15</span>:</span><br><span class="line">                flag = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> flag</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    Solution().numMagicSquaresInside(</span><br><span class="line">        [[<span class="number">4</span>, <span class="number">3</span>, <span class="number">8</span>, <span class="number">4</span>],</span><br><span class="line">         [<span class="number">9</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">9</span>],</span><br><span class="line">         [<span class="number">2</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">2</span>]]</span><br><span class="line">    )</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;python3暴力解，不难看懂&lt;br&gt;
    
    </summary>
    
    
      <category term="Leetcode" scheme="http://arithmeticjia.github.io/categories/Leetcode/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="leetcode" scheme="http://arithmeticjia.github.io/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>Learn-Pytorch-基本数据类型</title>
    <link href="http://arithmeticjia.github.io/2019/12/03/Learn-Pytorch-%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
    <id>http://arithmeticjia.github.io/2019/12/03/Learn-Pytorch-%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</id>
    <published>2019-12-03T12:43:45.000Z</published>
    <updated>2019-12-03T13:55:09.097Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍pytoch和numpy的数据之间的转化和pytorch中张量tensor的使用<br><a id="more"></a></p><h4 id="使用numpy新建一个一维数组"><a href="#使用numpy新建一个一维数组" class="headerlink" title="使用numpy新建一个一维数组"></a>使用numpy新建一个一维数组</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np_data = np.arange(<span class="number">6</span>)</span><br><span class="line">print(np_data,type(np_data))</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0 1 2 3 4 5] &lt;class 'numpy.ndarray'&gt;</span><br></pre></td></tr></table></figure><h4 id="把数组转化为二维"><a href="#把数组转化为二维" class="headerlink" title="把数组转化为二维"></a>把数组转化为二维</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np_data = np.arange(<span class="number">6</span>)</span><br><span class="line">print(np_data,type(np_data))</span><br><span class="line">np_data = np_data.reshape((<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">print(np_data,type(np_data))</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[0 1 2 3 4 5] &lt;class 'numpy.ndarray'&gt;</span><br><span class="line">[[0 1 2]</span><br><span class="line"> [3 4 5]] &lt;class 'numpy.ndarray'&gt;</span><br></pre></td></tr></table></figure><p>这个时候可以看到，数组变为二维的了，两行三列</p><h4 id="把二维数组转化为tensor"><a href="#把二维数组转化为tensor" class="headerlink" title="把二维数组转化为tensor"></a>把二维数组转化为tensor</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line">print(</span><br><span class="line">    <span class="string">'\nnumpy:'</span>, np_data,</span><br><span class="line">    <span class="string">'\ntorch:'</span>, torch_data,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">numpy: [[0 1 2]</span><br><span class="line"> [3 4 5]] </span><br><span class="line">torch: tensor([[0, 1, 2],</span><br><span class="line">        [3, 4, 5]])</span><br></pre></td></tr></table></figure><h4 id="把tensor转为数组"><a href="#把tensor转为数组" class="headerlink" title="把tensor转为数组"></a>把tensor转为数组</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">np_data = np.arange(<span class="number">6</span>).reshape((<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">torch_data = torch.from_numpy(np_data)</span><br><span class="line">tensor2array = torch_data.numpy()</span><br><span class="line">print(</span><br><span class="line">    <span class="string">'\nnumpy:'</span>, np_data,</span><br><span class="line">    <span class="string">'\ntorch:'</span>, torch_data,</span><br><span class="line">    <span class="string">'\ntensor2array'</span>, tensor2array,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">numpy: [[0 1 2]</span><br><span class="line"> [3 4 5]] </span><br><span class="line">torch: tensor([[0, 1, 2],</span><br><span class="line">        [3, 4, 5]]) </span><br><span class="line">tensor2array [[0 1 2]</span><br><span class="line"> [3 4 5]]</span><br></pre></td></tr></table></figure><h4 id="随机创建一个二维tensor"><a href="#随机创建一个二维tensor" class="headerlink" title="随机创建一个二维tensor"></a>随机创建一个二维tensor</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch_data = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">print(</span><br><span class="line">    <span class="string">'\ntorch_data:'</span>,torch_data</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch_data: tensor([[ 0.1533, -0.1010,  1.8385],</span><br><span class="line">        [-1.0455,  0.3707,  0.0191]])</span><br></pre></td></tr></table></figure><h4 id="构造初始化为0，1的tensor"><a href="#构造初始化为0，1的tensor" class="headerlink" title="构造初始化为0，1的tensor"></a>构造初始化为0，1的tensor</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch_data_0 = torch.zeros(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">torch_data_1 = torch.ones(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">print(</span><br><span class="line">    <span class="string">'\ntorch_data_0:'</span>,torch_data_0,</span><br><span class="line">    <span class="string">'\ntorch_data_1:'</span>,torch_data_1,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch_data_0: tensor([[0., 0., 0.],</span><br><span class="line">        [0., 0., 0.]]) </span><br><span class="line">torch_data_1: tensor([[1., 1., 1.],</span><br><span class="line">        [1., 1., 1.]])</span><br></pre></td></tr></table></figure><h4 id="从python列表直接构造tensor"><a href="#从python列表直接构造tensor" class="headerlink" title="从python列表直接构造tensor"></a>从python列表直接构造tensor</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch_data_from_list = torch.Tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">print(</span><br><span class="line">    <span class="string">'\ntorch_data_from_list:'</span>,torch_data_from_list,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch_data_from_list: tensor([[1., 2., 3.],</span><br><span class="line">        [4., 5., 6.]])</span><br></pre></td></tr></table></figure><p>需要注意的是，在pytorch中，默认的数据类型就是torch.FloatTensor，所以在这里torch.FloatTensor就等于torch.Tensor<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">torch_data_from_list = torch.Tensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">torch_data_from_list_32 = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">print(</span><br><span class="line">    <span class="string">'\ntorch_data_from_list:'</span>,torch_data_from_list,</span><br><span class="line">    <span class="string">'\ntorch_data_from_list_32'</span>,torch_data_from_list_32</span><br><span class="line">)</span><br></pre></td></tr></table></figure><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">torch_data_from_list: tensor([[1., 2., 3.],</span><br><span class="line">        [4., 5., 6.]]) </span><br><span class="line">torch_data_from_list_32 tensor([[1., 2., 3.],</span><br><span class="line">        [4., 5., 6.]])</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍pytoch和numpy的数据之间的转化和pytorch中张量tensor的使用&lt;br&gt;
    
    </summary>
    
    
      <category term="Pytorch" scheme="http://arithmeticjia.github.io/categories/Pytorch/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://arithmeticjia.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Pytorch中的LSTM的理解</title>
    <link href="http://arithmeticjia.github.io/2019/12/03/Pytorch%E4%B8%AD%E7%9A%84LSTM%E7%9A%84%E7%90%86%E8%A7%A3/"/>
    <id>http://arithmeticjia.github.io/2019/12/03/Pytorch%E4%B8%AD%E7%9A%84LSTM%E7%9A%84%E7%90%86%E8%A7%A3/</id>
    <published>2019-12-03T08:35:41.000Z</published>
    <updated>2019-12-03T08:47:16.718Z</updated>
    
    <content type="html"><![CDATA[<p>本文转载于<a href="https://zhuanlan.zhihu.com/p/41261640" target="_blank" rel="noopener" title="https://zhuanlan.zhihu.com/p/41261640">https://zhuanlan.zhihu.com/p/41261640</a><br><a id="more"></a></p><h4 id="参数列表"><a href="#参数列表" class="headerlink" title="参数列表"></a>参数列表</h4><ul><li>input_size：x的特征维度</li><li>hidden_size：隐藏层的特征维度</li><li>num_layers：lstm隐层的层数，默认为1</li><li>bias：False则bih=0和bhh=0. 默认为True</li><li>batch_first：True则输入输出的数据格式为 (batch, seq, feature)</li><li>dropout：除最后一层，每一层的输出都进行dropout，默认为: 0</li><li>bidirectional：True则为双向lstm默认为False</li><li>输入：input, (h0, c0)</li><li>输出：output, (hn,cn)</li></ul><h4 id="输入数据格式"><a href="#输入数据格式" class="headerlink" title="输入数据格式"></a>输入数据格式</h4><ul><li>input(seq_len, batch, input_size)</li><li>h0(num_layers * num_directions, batch, hidden_size)</li><li>c0(num_layers * num_directions, batch, hidden_size)</li></ul><h4 id="输出数据格式："><a href="#输出数据格式：" class="headerlink" title="输出数据格式："></a>输出数据格式：</h4><ul><li>output(seq_len, batch, hidden_size * num_directions)</li><li>hn(num_layers * num_directions, batch, hidden_size)</li><li>cn(num_layers * num_directions, batch, hidden_size)</li></ul><p>Pytorch里的LSTM单元接受的输入都必须是3维的张量(Tensors).每一维代表的意思不能弄错。</p><p>第一维体现的是序列（sequence）结构,也就是序列的个数，用文章来说，就是每个句子的长度，因为是喂给网络模型，一般都设定为确定的长度，也就是我们喂给LSTM神经元的每个句子的长度，当然，如果是其他的带有带有序列形式的数据，则表示一个明确分割单位长度，</p><p>例如是如果是股票数据内，这表示特定时间单位内，有多少条数据。这个参数也就是明确这个层中有多少个确定的单元来处理输入的数据。</p><p>第二维度体现的是batch_size，也就是一次性喂给网络多少条句子，或者股票数据中的，一次性喂给模型多少是个时间单位的数据，具体到每个时刻，也就是一次性喂给特定时刻处理的单元的单词数或者该时刻应该喂给的股票数据的条数</p><p>第三位体现的是输入的元素（elements of input），也就是，每个具体的单词用多少维向量来表示，或者股票数据中 每一个具体的时刻的采集多少具体的值，比如最低价，最高价，均价，5日均价，10均价，等等</p><p>H0-Hn是什么意思呢？就是每个时刻中间神经元应该保存的这一时刻的根据输入和上一课的时候的中间状态值应该产生的本时刻的状态值，</p><p>这个数据单元是起的作用就是记录这一时刻之前考虑到所有之前输入的状态值，形状应该是和特定时刻的输出一致</p><p>c0-cn就是开关，决定每个神经元的隐藏状态值是否会影响的下一时刻的神经元的处理，形状应该和h0-hn一致。</p><p>当然如果是双向，和多隐藏层还应该考虑方向和隐藏层的层数。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文转载于&lt;a href=&quot;https://zhuanlan.zhihu.com/p/41261640&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://zhuanlan.zhihu.com/p/41261640&quot;&gt;https://zhuanlan.zhihu.com/p/41261640&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="Pytorch" scheme="http://arithmeticjia.github.io/categories/Pytorch/"/>
    
    
      <category term="pytorch" scheme="http://arithmeticjia.github.io/tags/pytorch/"/>
    
      <category term="lstm" scheme="http://arithmeticjia.github.io/tags/lstm/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode[27]Remove Element</title>
    <link href="http://arithmeticjia.github.io/2019/12/02/Leetcode27Remove-Element/"/>
    <id>http://arithmeticjia.github.io/2019/12/02/Leetcode27Remove-Element/</id>
    <published>2019-12-02T13:00:52.000Z</published>
    <updated>2019-12-04T12:31:07.229Z</updated>
    
    <content type="html"><![CDATA[<p>leetcode上一道简单题27.移除元素，给定一个数组 nums 和一个值 val，你需要原地移除所有数值等于 val 的元素，返回移除后数组的新长度。<br><a id="more"></a><br>比较常规的想法<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeElement</span><span class="params">(self, nums, val)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type val: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        k = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            <span class="keyword">if</span> i != val:</span><br><span class="line">                nums[k] = i</span><br><span class="line">                k += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> k</span><br></pre></td></tr></table></figure><br>其实python自带的pop方法也很好用<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeElement</span><span class="params">(self, nums, val)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :type val: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># k = 0</span></span><br><span class="line">        <span class="comment"># for i in nums:</span></span><br><span class="line">        <span class="comment">#     if i != val:</span></span><br><span class="line">        <span class="comment">#         nums[k] = i</span></span><br><span class="line">        <span class="comment">#         k += 1</span></span><br><span class="line">        <span class="comment"># return k</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> nums[i] == val:</span><br><span class="line">                nums.pop(i)</span><br><span class="line">        <span class="keyword">return</span> len(nums)</span><br></pre></td></tr></table></figure><br>那为啥一定要逆向呢？正向不可以么？还真不行，因为这个循环的i是固定的，假设列表为[3,2,3,1,4,6,3,1]，需要移除的元素是3，正向是从0,1,…,7,当移除至少一个元素后，就无法访问下标为7的元素，因为不存在了，而逆向是7,6,…0，这个时候，下标是递减的，即使被pop了，接下来访问的都是比它小的下标，一定存在。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;leetcode上一道简单题27.移除元素，给定一个数组 nums 和一个值 val，你需要原地移除所有数值等于 val 的元素，返回移除后数组的新长度。&lt;br&gt;
    
    </summary>
    
    
      <category term="Leetcode" scheme="http://arithmeticjia.github.io/categories/Leetcode/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="leetcode" scheme="http://arithmeticjia.github.io/tags/leetcode/"/>
    
      <category term="list" scheme="http://arithmeticjia.github.io/tags/list/"/>
    
      <category term="pop" scheme="http://arithmeticjia.github.io/tags/pop/"/>
    
  </entry>
  
  <entry>
    <title>python列表推导式及其简单应用</title>
    <link href="http://arithmeticjia.github.io/2019/12/02/python%E5%88%97%E8%A1%A8%E6%8E%A8%E5%AF%BC%E5%BC%8F%E5%8F%8A%E5%85%B6%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8/"/>
    <id>http://arithmeticjia.github.io/2019/12/02/python%E5%88%97%E8%A1%A8%E6%8E%A8%E5%AF%BC%E5%BC%8F%E5%8F%8A%E5%85%B6%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8/</id>
    <published>2019-12-02T07:37:54.000Z</published>
    <updated>2019-12-02T08:59:17.003Z</updated>
    
    <content type="html"><![CDATA[<p>列表推导式（又称列表解析式）提供了一种简明扼要的方法来创建列表<br><a id="more"></a></p><h4 id="一个简单平方"><a href="#一个简单平方" class="headerlink" title="一个简单平方"></a>一个简单平方</h4><p>普通for循环<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>):</span><br><span class="line">    print(i*i,end=<span class="string">''</span>)</span><br></pre></td></tr></table></figure><br>列表推导式<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">res = [x*x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>)]</span><br><span class="line">print(res)</span><br></pre></td></tr></table></figure></p><h4 id="执行顺序"><a href="#执行顺序" class="headerlink" title="执行顺序"></a>执行顺序</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[x*y <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>) <span class="keyword">if</span> x &gt; <span class="number">2</span> <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>) <span class="keyword">if</span> y &lt; <span class="number">3</span>]</span><br></pre></td></tr></table></figure><p>等价于<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">if</span> x &gt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">4</span>):</span><br><span class="line">            <span class="keyword">if</span> y &lt; <span class="number">3</span>:</span><br><span class="line">                <span class="keyword">return</span> x*y</span><br></pre></td></tr></table></figure></p><h4 id="leetcode17电话号码的字母组合"><a href="#leetcode17电话号码的字母组合" class="headerlink" title="leetcode17电话号码的字母组合"></a>leetcode17电话号码的字母组合</h4><p>给定一个仅包含数字 2-9 的字符串，返回所有它能表示的字母组合。<br>给出数字到字母的映射如下（与电话按键相同）。注意 1 不对应任何字母。<br><img src="https://www.guanacossj.com/media/articlebodypics/17_telephone_keypad.png" alt=""><br>这题看起来递归比较合适，但是强行循环也不是不可以<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">letterCombinations</span><span class="params">(self, digits)</span>:</span></span><br><span class="line">        m = &#123;</span><br><span class="line">            <span class="string">'2'</span>: list(<span class="string">'abc'</span>),</span><br><span class="line">            <span class="string">'3'</span>: list(<span class="string">'def'</span>),</span><br><span class="line">            <span class="string">'4'</span>: list(<span class="string">'ghi'</span>),</span><br><span class="line">            <span class="string">'5'</span>: list(<span class="string">'jkl'</span>),</span><br><span class="line">            <span class="string">'6'</span>: list(<span class="string">'mno'</span>),</span><br><span class="line">            <span class="string">'7'</span>: list(<span class="string">'pqrs'</span>),</span><br><span class="line">            <span class="string">'8'</span>: list(<span class="string">'tuv'</span>),</span><br><span class="line">            <span class="string">'9'</span>: list(<span class="string">'wxyz'</span>),</span><br><span class="line">            &#125;</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> digits:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        res = [<span class="string">''</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> digits:</span><br><span class="line">            res = [x + y <span class="keyword">for</span> x <span class="keyword">in</span> res <span class="keyword">for</span> y <span class="keyword">in</span> m[i]]</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><br>这里的循环其实不止两层，取决于你输入的数字的位数。可以打印输出看一下，假设输入的数字是234<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">['a', 'b', 'c']</span><br><span class="line">['ad', 'ae', 'af', 'bd', 'be', 'bf', 'cd', 'ce', 'cf']</span><br><span class="line">['adg', 'adh', 'adi', 'aeg', 'aeh', 'aei', 'afg', 'afh', 'afi', 'bdg', 'bdh', 'bdi', 'beg', 'beh', 'bei', 'bfg', 'bfh', 'bfi', 'cdg', 'cdh', 'cdi', 'ceg', 'ceh', 'cei', 'cfg', 'cfh', 'cfi']</span><br></pre></td></tr></table></figure><br>第一个2对应的字母是[‘a’, ‘b’, ‘c’]<br>第二个3对于的字母是[‘d’, ‘e’, ‘f’]<br>第三个4对于的字母是[‘g’, ‘h’, ‘i’]<br>开始的时候res长度为1，可以理解为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">res = [<span class="string">''</span>]</span><br><span class="line">m = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> res:</span><br><span class="line">    tem = []</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> m:</span><br><span class="line">        res = x + y</span><br><span class="line">        tem.append(res)</span><br><span class="line">    print(tem)</span><br></pre></td></tr></table></figure><br>当有两个数字时<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">res = [<span class="string">''</span>]</span><br><span class="line">m = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>]</span><br><span class="line">n = [<span class="string">'d'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>]</span><br><span class="line">tem = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> res:</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> m:</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> n:</span><br><span class="line">            res = x + y + k</span><br><span class="line">            tem.append(res)</span><br><span class="line">    print(tem)</span><br></pre></td></tr></table></figure><br>这样一层一层加下去就可以，不过即使知道要循环几次，也很难表达出来，这个时候用列表推导式就很方便</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;列表推导式（又称列表解析式）提供了一种简明扼要的方法来创建列表&lt;br&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://arithmeticjia.github.io/categories/Python/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="列表推导式" scheme="http://arithmeticjia.github.io/tags/%E5%88%97%E8%A1%A8%E6%8E%A8%E5%AF%BC%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>Flask-Vue前后端分离（一）</title>
    <link href="http://arithmeticjia.github.io/2019/11/30/Flask-Vue%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>http://arithmeticjia.github.io/2019/11/30/Flask-Vue%E5%89%8D%E5%90%8E%E7%AB%AF%E5%88%86%E7%A6%BB%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2019-11-30T15:43:36.000Z</published>
    <updated>2019-11-30T16:00:17.919Z</updated>
    
    <content type="html"><![CDATA[<p>简单介绍下如何在Pycharm下集成Flask和Vue<br><a id="more"></a></p><h4 id="新建flask项目"><a href="#新建flask项目" class="headerlink" title="新建flask项目"></a>新建flask项目</h4><p>略<br>我这里是Jia-Prophet</p><h4 id="新建vue项目"><a href="#新建vue项目" class="headerlink" title="新建vue项目"></a>新建vue项目</h4><p>在Jia-Prophet目录下<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vue init webpack jia-vue</span><br></pre></td></tr></table></figure><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">Arithmetic@qingjiaowosuanshujiadeMacBook-Pro Jia-Prophet % vue init webpack jia-vue</span><br><span class="line"></span><br><span class="line">? Project name jia-vue</span><br><span class="line">? Project description A Vue.js project</span><br><span class="line">? Author ArithmeticJia &lt;1097197237@qq.com&gt;</span><br><span class="line">? Vue build standalone</span><br><span class="line">? Install vue-router? Yes</span><br><span class="line">? Use ESLint to lint your code? No</span><br><span class="line">? Set up unit tests No</span><br><span class="line">? Setup e2e tests with Nightwatch? No</span><br><span class="line">? Should we run `npm install` for you after the project has been created? (recommended) n</span><br><span class="line">pm</span><br><span class="line"></span><br><span class="line">   vue-cli · Generated "jia-vue".</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Installing project dependencies ...</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ========================</span></span><br><span class="line"></span><br><span class="line">npm WARN deprecated extract-text-webpack-plugin@3.0.2: Deprecated. Please use https://github.com/webpack-contrib/mini-css-extract-plugin</span><br><span class="line">npm WARN deprecated browserslist@2.11.3: Browserslist 2 could fail on reading Browserslist &gt;3.0 config used in other tools.</span><br><span class="line">npm WARN deprecated bfj-node4@5.3.1: Switch to the `bfj` package for fixes and new features!</span><br><span class="line">npm WARN deprecated core-js@2.6.10: core-js@&lt;3.0 is no longer maintained and not recommended for usage due to the number of issues. Please, upgrade your dependencies to the actual version of core-js@3.</span><br><span class="line">npm WARN deprecated fsevents@1.2.9: One of your dependencies needs to upgrade to fsevents v2: 1) Proper nodejs v10+ support 2) No more fetching binaries from AWS, smaller package size</span><br><span class="line">npm WARN deprecated browserslist@1.7.7: Browserslist 2 could fail on reading Browserslist &gt;3.0 config used in other tools.</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> fsevents@1.2.9 install /Users/Arithmetic/PycharmProjects/Jia/Jia-Prophet/jia-vue/node_modules/fsevents</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> node install</span></span><br><span class="line"></span><br><span class="line">node-pre-gyp WARN Using needle for node-pre-gyp https download </span><br><span class="line">[fsevents] Success: "/Users/Arithmetic/PycharmProjects/Jia/Jia-Prophet/jia-vue/node_modules/fsevents/lib/binding/Release/node-v72-darwin-x64/fse.node" is installed via remote</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> core-js@2.6.10 postinstall /Users/Arithmetic/PycharmProjects/Jia/Jia-Prophet/jia-vue/node_modules/core-js</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> node postinstall || <span class="built_in">echo</span> <span class="string">"ignore"</span></span></span><br><span class="line"></span><br><span class="line">Thank you for using core-js ( https://github.com/zloirock/core-js ) for polyfilling JavaScript standard library!</span><br><span class="line"></span><br><span class="line">The project needs your help! Please consider supporting of core-js on Open Collective or Patreon: </span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> https://opencollective.com/core-js </span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> https://www.patreon.com/zloirock </span></span><br><span class="line"></span><br><span class="line">Also, the author of core-js ( https://github.com/zloirock ) is looking for a good job -)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> ejs@2.7.4 postinstall /Users/Arithmetic/PycharmProjects/Jia/Jia-Prophet/jia-vue/node_modules/ejs</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> node ./postinstall.js</span></span><br><span class="line"></span><br><span class="line">Thank you for installing EJS: built with the Jake JavaScript build tool (https://jakejs.com/)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> uglifyjs-webpack-plugin@0.4.6 postinstall /Users/Arithmetic/PycharmProjects/Jia/Jia-Prophet/jia-vue/node_modules/webpack/node_modules/uglifyjs-webpack-plugin</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> node lib/post_install.js</span></span><br><span class="line"></span><br><span class="line">npm notice created a lockfile as package-lock.json. You should commit this file.</span><br><span class="line">npm WARN ajv-keywords@3.4.1 requires a peer of ajv@^6.9.1 but none is installed. You must install peer dependencies yourself.</span><br><span class="line"></span><br><span class="line">added 1284 packages from 686 contributors and audited 11868 packages in 47.259s</span><br><span class="line"></span><br><span class="line">11 packages are looking for funding</span><br><span class="line">  run `npm fund` for details</span><br><span class="line"></span><br><span class="line">found 11 vulnerabilities (1 low, 6 moderate, 4 high)</span><br><span class="line">  run `npm audit fix` to fix them, or `npm audit` for details</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Project initialization finished!</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ========================</span></span><br><span class="line"></span><br><span class="line">To get started:</span><br><span class="line"></span><br><span class="line">  cd jia-vue</span><br><span class="line">  npm run dev</span><br><span class="line">  </span><br><span class="line">Documentation can be found at https://vuejs-templates.github.io/webpack</span><br></pre></td></tr></table></figure><br>然后执行<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cnpm run build</span><br><span class="line">cnpm run dev</span><br></pre></td></tr></table></figure><br>这里我之前换成淘宝源了，详见之前的教程<br>如果没有问题的话，访问 <a href="http://localhost:8080" target="_blank" rel="noopener">http://localhost:8080</a><br><img src="https://www.guanacossj.com/media/articlebodypics/1575129241168.jpg" alt=""></p><h4 id="修改编译生成的静态文件"><a href="#修改编译生成的静态文件" class="headerlink" title="修改编译生成的静态文件"></a>修改编译生成的静态文件</h4><p>找到jia-vue/config/index.js文件，修改如下：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">build: &#123;</span><br><span class="line">    // Template for index.html</span><br><span class="line">    index: path.resolve(__dirname, '../../templates/index.html'), //这里是index.html生成的路径，在templates下</span><br><span class="line"></span><br><span class="line">    // Paths</span><br><span class="line">    assetsRoot: path.resolve(__dirname, '../../static'), //这里指定vue生成的js和css文件路径为/static/vue/</span><br><span class="line">    assetsSubDirectory: 'vue',</span><br><span class="line">    assetsPublicPath: '/static',</span><br><span class="line"></span><br><span class="line">    /**</span><br><span class="line">     * Source Maps</span><br><span class="line">     */</span><br><span class="line"></span><br><span class="line">    productionSourceMap: true,</span><br><span class="line">    // https://webpack.js.org/configuration/devtool/#production</span><br><span class="line">    devtool: '#source-map',</span><br><span class="line"></span><br><span class="line">    // Gzip off by default as many popular static hosts such as</span><br><span class="line">    // Surge or Netlify already gzip all static assets for you.</span><br><span class="line">    // Before setting to `true`, make sure to:</span><br><span class="line">    // npm install --save-dev compression-webpack-plugin</span><br><span class="line">    productionGzip: false,</span><br><span class="line">    productionGzipExtensions: ['js', 'css'],</span><br><span class="line"></span><br><span class="line">    // Run the build command with an extra argument to</span><br><span class="line">    // View the bundle analyzer report after build finishes:</span><br><span class="line">    // `npm run build --report`</span><br><span class="line">    // Set to `true` or `false` to always turn it on or off</span><br><span class="line">    bundleAnalyzerReport: process.env.npm_config_report</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;简单介绍下如何在Pycharm下集成Flask和Vue&lt;br&gt;
    
    </summary>
    
    
      <category term="Flask" scheme="http://arithmeticjia.github.io/categories/Flask/"/>
    
    
      <category term="flask" scheme="http://arithmeticjia.github.io/tags/flask/"/>
    
      <category term="vue" scheme="http://arithmeticjia.github.io/tags/vue/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04.5LTS安装SVN</title>
    <link href="http://arithmeticjia.github.io/2019/11/30/Ubuntu16.04.5LTS%E5%AE%89%E8%A3%85SVN/"/>
    <id>http://arithmeticjia.github.io/2019/11/30/Ubuntu16.04.5LTS%E5%AE%89%E8%A3%85SVN/</id>
    <published>2019-11-30T10:15:44.000Z</published>
    <updated>2019-11-30T11:23:41.988Z</updated>
    
    <content type="html"><![CDATA[<p>本文简单介绍Ubuntu系统下SVN的搭建过程<br><a id="more"></a></p><h4 id="更新源"><a href="#更新源" class="headerlink" title="更新源"></a>更新源</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure><h4 id="安装SVN"><a href="#安装SVN" class="headerlink" title="安装SVN"></a>安装SVN</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install subversion</span><br></pre></td></tr></table></figure><h4 id="创建文件夹"><a href="#创建文件夹" class="headerlink" title="创建文件夹"></a>创建文件夹</h4><figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">mkdir</span> /<span class="built_in">home</span>/svn</span><br><span class="line">sudo <span class="built_in">mkdir</span> /<span class="built_in">home</span>/svn/repository</span><br><span class="line">sudo chmod -R <span class="number">777</span> /<span class="built_in">home</span>/svn/repository</span><br><span class="line">sudo svnadmin create /<span class="built_in">home</span>/svn/repository</span><br><span class="line">cd /<span class="built_in">home</span>/svn/repository/</span><br><span class="line">sudo chmod -R <span class="number">777</span> db</span><br></pre></td></tr></table></figure><h4 id="修改svnserve-conf"><a href="#修改svnserve-conf" class="headerlink" title="修改svnserve.conf"></a>修改svnserve.conf</h4><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> /home/svn/repository/<span class="keyword">conf</span>/</span><br><span class="line">sudo <span class="keyword">vi</span> svnserve.<span class="keyword">conf</span></span><br></pre></td></tr></table></figure><p>修改这四行如下所示<br>anon-access = none 匿名用户不可读<br>auth-access = write 权限用户可写<br>password-db = passwd 密码文件为passwd<br>authz-db = authz 权限文件为authz<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## users have read and write access to the repository.</span></span></span><br><span class="line">anon-access = none</span><br><span class="line">auth-access = write</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## The password-db option controls the location of the password</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## database file.  Unless you specify a path starting with a /,</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## the file's location is relative to the directory containing</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## this configuration file.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## If SASL is enabled (see below), this file will NOT be used.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## Uncomment the line below to use the default password file.</span></span></span><br><span class="line">password-db = passwd</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## The authz-db option controls the location of the authorization</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## rules for path-based access control.  Unless you specify a path</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## starting with a /, the file's location is relative to the</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## directory containing this file.  The specified path may be a</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## repository relative URL (^/) or an absolute file:// URL to a text</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## file in a Subversion repository.  If you don't specify an authz-db,</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## no path-based access control is done.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## Uncomment the line below to use the default authorization file.</span></span></span><br><span class="line">authz-db = authz</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">## The groups-db option controls the location of the groups file.</span></span></span><br></pre></td></tr></table></figure></p><h4 id="修改password文件，添加访问用户"><a href="#修改password文件，添加访问用户" class="headerlink" title="修改password文件，添加访问用户"></a>修改password文件，添加访问用户</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi passwd</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[users]</span><br><span class="line"><span class="meta">#</span><span class="bash"> harry = harryssecret</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> sally = sallyssecret</span></span><br><span class="line">lidata = lidata429</span><br></pre></td></tr></table></figure><h4 id="给用户增加目录权限"><a href="#给用户增加目录权限" class="headerlink" title="给用户增加目录权限"></a>给用户增加目录权限</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vi authz</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[groups]</span><br><span class="line"><span class="meta">#</span><span class="bash"> harry_and_sally = harry,sally</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> harry_sally_and_joe = harry,sally,&amp;joe</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> [/foo/bar]</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> harry = rw</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> &amp;joe = r</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> * =</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> [repository:/baz/fuz]</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> @harry_and_sally = rw</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> * = r</span></span><br><span class="line">[/]</span><br><span class="line">lidata=rw</span><br></pre></td></tr></table></figure><h4 id="启动服务，并且监听81端口"><a href="#启动服务，并且监听81端口" class="headerlink" title="启动服务，并且监听81端口"></a>启动服务，并且监听81端口</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo svnserve -d -r /home/svn --listen-port 81</span><br></pre></td></tr></table></figure><h4 id="查看svn是否启动"><a href="#查看svn是否启动" class="headerlink" title="查看svn是否启动"></a>查看svn是否启动</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -ef | grep svnserve</span><br></pre></td></tr></table></figure><h4 id="停止服务"><a href="#停止服务" class="headerlink" title="停止服务"></a>停止服务</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kill all svnserve</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文简单介绍Ubuntu系统下SVN的搭建过程&lt;br&gt;
    
    </summary>
    
    
      <category term="SVN" scheme="http://arithmeticjia.github.io/categories/SVN/"/>
    
    
      <category term="svn" scheme="http://arithmeticjia.github.io/tags/svn/"/>
    
  </entry>
  
  <entry>
    <title>Hexo-next主题支持数学公式</title>
    <link href="http://arithmeticjia.github.io/2019/11/28/Hexo-next%E4%B8%BB%E9%A2%98%E6%94%AF%E6%8C%81%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/"/>
    <id>http://arithmeticjia.github.io/2019/11/28/Hexo-next%E4%B8%BB%E9%A2%98%E6%94%AF%E6%8C%81%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</id>
    <published>2019-11-28T15:31:03.000Z</published>
    <updated>2019-11-28T15:39:25.275Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要解决Hexo-next主题支持数学公式问题<br><a id="more"></a></p><h4 id="更换Hexo的markdown渲染引擎"><a href="#更换Hexo的markdown渲染引擎" class="headerlink" title="更换Hexo的markdown渲染引擎"></a>更换Hexo的markdown渲染引擎</h4><p>hexo-renderer-kramed引擎是在默认的渲染引擎，hexo-renderer-marked的基础上修改了一些bug，两者比较接近，也比较轻量级。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure></p><h4 id="解决语义冲突"><a href="#解决语义冲突" class="headerlink" title="解决语义冲突"></a>解决语义冲突</h4><p>找到<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">node_modules\kramed\lib\rules\inline.js</span><br></pre></td></tr></table></figure><br>修改如下两处<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span><br><span class="line">escape: /^\\([`*\[\]()#$+\-.!_&gt;])/,</span><br></pre></td></tr></table></figure><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br><span class="line">em: /^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span><br></pre></td></tr></table></figure></p><h4 id="在next主题中开启mathJax开关"><a href="#在next主题中开启mathJax开关" class="headerlink" title="在next主题中开启mathJax开关"></a>在next主题中开启mathJax开关</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Math Formulas Render Support</span></span><br><span class="line">math:</span><br><span class="line"><span class="meta">  #</span><span class="bash"> Default (<span class="literal">true</span>) will load mathjax / katex script on demand.</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> That is it only render those page <span class="built_in">which</span> has `mathjax: <span class="literal">true</span>` <span class="keyword">in</span> Front-matter.</span></span><br><span class="line"><span class="meta">  #</span><span class="bash"> If you <span class="built_in">set</span> it to <span class="literal">false</span>, it will load mathjax / katex srcipt EVERY PAGE.</span></span><br><span class="line">  per_page: true</span><br><span class="line">  engine: mathjax   # 添加这个，反正我的主题默认没有这一行</span><br><span class="line"><span class="meta">  #</span><span class="bash"> hexo-renderer-pandoc (or hexo-renderer-kramed) required <span class="keyword">for</span> full MathJax support.</span></span><br><span class="line">  mathjax:</span><br><span class="line">    enable: true    # 这个改为true</span><br><span class="line">    # See: https://mhchem.github.io/MathJax-mhchem/</span><br><span class="line">    mhchem: true    # 这个改为true</span><br><span class="line"></span><br><span class="line"><span class="meta">  #</span><span class="bash"> hexo-renderer-markdown-it-plus (or hexo-renderer-markdown-it with markdown-it-katex plugin) required <span class="keyword">for</span> full Katex support.</span></span><br><span class="line">  katex:</span><br><span class="line">    enable: false</span><br><span class="line">    # See: https://github.com/KaTeX/KaTeX/tree/master/contrib/copy-tex</span><br><span class="line">    copy_tex: false</span><br></pre></td></tr></table></figure><h4 id="打开mathjax开关"><a href="#打开mathjax开关" class="headerlink" title="打开mathjax开关"></a>打开mathjax开关</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: Using-Machine-Learning-for-Data-Center-Cooling-Infrastructure-Efficiency-Prediction</span><br><span class="line">date: 2019-11-28 22:53:32</span><br><span class="line">tags: [datacenter, machinelearning]</span><br><span class="line">category: DataCenter</span><br><span class="line">mathjax: true</span><br><span class="line">---</span><br></pre></td></tr></table></figure><h4 id="添加公式"><a href="#添加公式" class="headerlink" title="添加公式"></a>添加公式</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br><span class="line">COP = \frac&#123;Q_&#123;CoolingCircuits&#125;&#125;&#123;P_&#123;CoolingCircuits&#125;&#125;</span><br><span class="line"><span class="meta">$</span><span class="bash">$</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要解决Hexo-next主题支持数学公式问题&lt;br&gt;
    
    </summary>
    
    
      <category term="Hexo" scheme="http://arithmeticjia.github.io/categories/Hexo/"/>
    
    
      <category term="hexo" scheme="http://arithmeticjia.github.io/tags/hexo/"/>
    
      <category term="next" scheme="http://arithmeticjia.github.io/tags/next/"/>
    
      <category term="markdown" scheme="http://arithmeticjia.github.io/tags/markdown/"/>
    
      <category term="math" scheme="http://arithmeticjia.github.io/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>Using Machine Learning for Data Center Cooling Infrastructure Efficiency Prediction</title>
    <link href="http://arithmeticjia.github.io/2019/11/28/Using-Machine-Learning-for-Data-Center-Cooling-Infrastructure-Efficiency-Prediction/"/>
    <id>http://arithmeticjia.github.io/2019/11/28/Using-Machine-Learning-for-Data-Center-Cooling-Infrastructure-Efficiency-Prediction/</id>
    <published>2019-11-28T14:53:32.000Z</published>
    <updated>2019-11-30T08:01:49.975Z</updated>
    
    <content type="html"><![CDATA[<p>Using Machine Learning for Data Center Cooling Infrastructure Efficiency Prediction<br><a id="more"></a></p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>Power consumption continues to remain a critical aspect for High Performance Computing (HPC) data centers. It becomes even more crucial for Exascale computing since scaling today’s fastest system to an Exaflop level would consume more than 168 MW power which is 8 times higher than the 20 MW power consumption goal set, at the time of this publication, by the US Department of Energy.</p><p>This naturally leads to a necessity for energy efficiency improvement that will encompass the full chain of the power consumers, starting from the data center infrastructure, including cooling overheads and electrical losses, up to compute resource scheduling and application scaling.</p><p>In this paper a machine learning approach is proposed to model the Coefficient of Performance (COP) of HPC data center’s hot water cooling loop. The suggested model is validated on operational data obtained at Leibniz Supercomputing Centre (LRZ). The paper shows how this COP model can help to improve the energy efficiency of modern HPC data centers.</p><h4 id="PUE"><a href="#PUE" class="headerlink" title="PUE"></a>PUE</h4><script type="math/tex; mode=display">PUE = \frac{P_{Tocal}}{P_{IT}} = \frac{P_{cooling}+P_{IT}+P_{electricalLosses}+P_{misc}}{P_{IT}}</script><h4 id="COP"><a href="#COP" class="headerlink" title="COP"></a>COP</h4><ul><li>COP: Coefficient of Performance<script type="math/tex; mode=display">COP = \frac{Q_{CoolingCircuits}}{P_{CoolingCircuits}}</script></li><li>$Q_{CoolingCircuits}$ - is the aggregated amount of cold generated by the four cooling circuits (measured in watts)</li><li>$P_{CoolingCircuits}$ - is the aggregated amount of power consumed by the four cooling circuits (measured in watts)</li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/1575015799772.jpg" alt=""></p><ul><li>T(WB):the wet bulb temperature</li><li>P(DC):the complete power consumption of the data center</li><li>P(SuperMUC IT):the power consumption profile of the deployed HPC system SuperMUC-Phase1</li><li>P(DC KT+I):the power consumption of the cooling infrastructure</li><li>P(Cluster IT):the power profile of an old HPC cluster system installed at LRZ</li><li>P(DC EV):the power losses from electrical distribution and conversion</li></ul><h4 id="WHY-COP"><a href="#WHY-COP" class="headerlink" title="WHY COP?"></a>WHY COP?</h4><p>A similar study using neural networks for predicting a datacenter’s PUE was presented by Jim Gao from Google in [10].The study considered Google data centers where the author uses approximately 2 years of Google’s operational data for modeling the PUE. The main drawback of PUE is that it indicates the exact relation between the IT power consumption and the complete facility power consumption; implying that the more power is put into the IT equipment the better(optimal PUE is 1). If multiple cooling technologies are used(e.g. air cooling according to ASHRAE (American Society of Heating, Refrigerating and Air-Conditioning Engineers)[11] class A1 specifications (air cooling with 18 ◦C - 27 ◦C inlet temperature [12]), water cooling according to ASHRAE W1-W4 standards, etc.) PUE combines the COPs from all cooling technologies into one number. This makes it impossible to calculate cooling overheads for individual IT systems. This paper focuses on COP since COP reflects the impacts of various control parameters on the overall efficiency of the data center cooling infrastructure. This allows for the identification of key parameters and their optimal configuration for a given point in time.There are other studies that aim to model, simulate,and optimize the energy efficiency of modern HPC liquid cooled data centers with the help of multiphysical network simulators [13], [14], [15]. Using these studies, it might be 956 possible to formally describe the relationships in each part of the liquid cooling circuit and use statistical modeling for predicting the efficiency of the cooling infrastructure.These type of solutions can be: (a) time intensive and/or (b) erroneous since they could fail to completely capture the the complex interactions of various components affecting the overall efficiency of the cooling infrastructure. Examples of such complex interactions include the dynamic variation of ambient outside conditions combined with high fluctuating IT load on the cooling infrastructure.<br>This paper proposes a different approach - a neural network [16] based framework that learns from the actual operational data and aims to predict the impacts of various infrastructure configurations on the overall cooling efficiency of a typical HPC data center. With this approach, it is possible to capture the complex interactions of various aspects affecting the overall data center power/energy efficiency (as identified in Figure 4).</p><h4 id="SETUP-AT-LRZ"><a href="#SETUP-AT-LRZ" class="headerlink" title="SETUP AT LRZ"></a>SETUP AT LRZ</h4><h5 id="Hot-Water-Cooling-Circuit"><a href="#Hot-Water-Cooling-Circuit" class="headerlink" title="Hot Water Cooling Circuit"></a>Hot Water Cooling Circuit</h5><p>Figure 5 illustrates the schematic overview of LRZ’s chiller-less (also called hot water) cooling infrastructure, consisting of four identical cooling circuits. In this paper, the term ”hot water” refers to ASHRAEs W4 specification [11]. Each cooling circuit has a 2 MW wet cooling tower (KLT11, KLT12, KLT13, and KLT14). On average, at least two cooling circuits are active, the third and the fourth are added depending on the load.</p><p><img src="https://www.guanacossj.com/media/articlebodypics/1575008791198.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/1575002886514.jpg" alt=""></p><ul><li>Subcircuit I: The water enters the cooling circuit from the distribution bar via a hydraulic gate. The water then flows with the help of a redundant pump pair to the heat exchanger and back again via a hydraulic gate to the distribution bar.</li><li>Subcircuit II: The ethylene glycol water mix (this mixture prevents the water from freezing) is pumped to the corresponding cooling tower (connection A) by two redundant pumps.</li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/1575002918635.jpg" alt=""></p><h4 id="The-COP-Network-Model-Design"><a href="#The-COP-Network-Model-Design" class="headerlink" title="The COP Network Model Design"></a>The COP Network Model Design</h4><p>Recurrent neural networks [19] are a special class of artificial neural networks where neuron-like units form directed cycles - introducing feedback and thus memory to the network. This allows a better capturing of the time evolution of the underlying discrete time random signals. The Long Short-Term Memory (LSTM) [20] networks, used in this paper, are a special class of recurrent neural networks that are capable of capturing behaviors having relatively long timelags.</p><h5 id="Inputs-to-the-model"><a href="#Inputs-to-the-model" class="headerlink" title="Inputs to the model:"></a>Inputs to the model:</h5><ul><li>aggregated amount of cold generated by each cooling circuit</li><li>aggregated amount of power consumed by the fans of each cooling tower </li><li>number of active cooling towers</li><li>wet bulb temperature<br>  湿球温度是指同等焓值空气状态下，空气中水蒸汽达到饱和时的空气温度，在空气焓湿图上是由空气状态点沿等焓线下降至100%相对湿度线上，对应点的干球温度。   </li><li>inlet water temperature (to the distribution bar) from each cooling circuit (Figure 6) (×4)</li><li>return water temperature (to the distribution bar) to each cooling circuit (Figure 6) (×4)</li></ul><h5 id="Output-of-the-model"><a href="#Output-of-the-model" class="headerlink" title="Output of the model:"></a>Output of the model:</h5><ul><li>COP of the hot water cooling loop</li></ul><p>And so, depending on the inlet temperature of the cooling towers, the fan speeds have to be adjusted to create the desired outlet temperature from the cooling tower. The efficiency of the cooling infrastructure will vary with fan speeds which leads to differences in the power consumption. This is why fan power is considered as an affecting factor to the COP of the cooling infrastructure.</p><h4 id="COP-Prediction-Results"><a href="#COP-Prediction-Results" class="headerlink" title="COP Prediction Results"></a>COP Prediction Results</h4><p>A network, with two hidden LSTM recurrent layers and a linear output layer, was created using Adam [23] as an optimizer. Figure 8 shows the COP prediction results for the complete year 2016 when the data for the complete year 2015 was learned.<br><img src="https://www.guanacossj.com/media/articlebodypics/1575005434173.jpg" alt=""></p><h4 id="REFINING-THE-INPUT-SET"><a href="#REFINING-THE-INPUT-SET" class="headerlink" title="REFINING THE INPUT SET"></a>REFINING THE INPUT SET</h4><p>Having shown that the neural network based COP prediction is possible, the follow up work focused on the improvement of its useability. The usage of the shown network is not very practical since a change in one of the input parameters (e.g. inlet water temperature entering the cooling circuit from the distribution bar) directly affects other input parameters (e.g. the amount of power consumed by the fans of the cooling towers).</p><h5 id="The-improved-COP-prediction-network"><a href="#The-improved-COP-prediction-network" class="headerlink" title="The improved COP prediction network"></a>The improved COP prediction network</h5><p>The list below indicates the refined set of input parameters:</p><ul><li>aggregated amount of cold generated by each cooling circuit</li><li>number of active cooling towers</li><li>inlet water temperature from each cooling circuit (Figure 6) (×4)</li><li>return water temperature to each cooling circuit (Figure 6) (×4)</li></ul><p>A. Deriving Wet Bulb Temperature<br>Wet bulb temperature can be derived analytically using the dry bulb temperature and the relative humidity using the formula presented in [24], which in their turn can also be made accessible from weather forecast frameworks.</p><p>B. Deriving Power of Cooling Tower Fans<br>In order to predict the power consumption of the cooling tower fans (each cooling tower in our case has two fans, Figure 7) the estimation of the water flow rates (before entering the hydraulic gate) is required in advance. This can be determined analytically via the following equation:</p><script type="math/tex; mode=display">flowRate_{KLT_{i}} = \frac{Q_{KLT_{i}}}{c_{p}*\rho *\Delta{T_{KLT_{i}}}}</script><ul><li><p><script type="math/tex">QKLT_{i}</script>- is the amount of cold generated by the cooling circuit connected to $KLT_{i}$</p></li><li><p>$\Delta T_{KLT_{i}}$- is the temperature difference between inlet and return water (from and to the distribution bar correspondingly) for the cooling circuit connected to $KLT_{i}$</p></li><li><p>$c_{p}$- is the water specific heat capacity;</p></li><li><p>$\rho$- is the water density.</p></li></ul><p>Having estimated the water flow rates, a network with two hidden LSTM layers using Stochastic Gradient Decent [25] as an optimizer was built taking the following inputs:</p><ul><li>$QKLT_{i}$amount of generated cold for a given cooling circuit (1≤ i ≤4)<script type="math/tex">KLT_{i}</script></li><li>$flowRate_{KLT_{i}}$ water flow rate (before entering the hydraulic gate) for a given cooling circuit (1≤ i ≤4)<script type="math/tex">KLT_{i}</script></li></ul><p>C. Deriving Cold Generation from HPC System’s Power Profile</p><p>Figure 13 shows the prediction results for the linear regression model that takes as an input the power consumption data of SuperMUC and outputs the aggregated amount of cold generated by all four cooling circuits. The power data for 2015 was learned and the amount of heat was predicted for 2016. As can be seen, the prediction has a relatively high accuracy.</p><p><img src="https://www.guanacossj.com/media/articlebodypics/1575010626556.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Using Machine Learning for Data Center Cooling Infrastructure Efficiency Prediction&lt;br&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="http://arithmeticjia.github.io/categories/Paper/"/>
    
    
      <category term="datacenter" scheme="http://arithmeticjia.github.io/tags/datacenter/"/>
    
      <category term="machinelearning" scheme="http://arithmeticjia.github.io/tags/machinelearning/"/>
    
  </entry>
  
  <entry>
    <title>Machine Learning Applications for Data Center Optimization</title>
    <link href="http://arithmeticjia.github.io/2019/11/28/Machine-Learning-Applications-for-Data-Center-Optimization/"/>
    <id>http://arithmeticjia.github.io/2019/11/28/Machine-Learning-Applications-for-Data-Center-Optimization/</id>
    <published>2019-11-28T13:59:22.000Z</published>
    <updated>2019-11-30T08:01:49.971Z</updated>
    
    <content type="html"><![CDATA[<p>Machine Learning Applications for Data Center Optimization<br><a id="more"></a></p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>The modern data center (DC) is a complex interaction of multiple mechanical, electrical and controls systems. The sheer number of possible operating configurations and nonlinear interdependencies make it difficult to understand and optimize energy efficiency. We develop a neural network framework that learns from actual operations data to model plant performance and predict PUE within a range of 0.004 +/­ 0.005 (mean absolute error +/­ 1 standard deviation), or 0.4% error for a PUE of 1.1. The model has been extensively tested and validated at Google DCs. The results demonstrate that machine learning is an effective way of leveraging existing sensor data to model DC performance and improve energy efficiency.</p><h4 id="Model-Implementation"><a href="#Model-Implementation" class="headerlink" title="Model Implementation"></a>Model Implementation</h4><p>A generic three­layered neural network is illustrated in Figure 2. In this study, the input matrix is an (m x n) array where is the number of training examples and is the number of features (DC input variables) including the IT load, weather conditions, number of chillers and cooling towers running, equipment setpoints, etc. The input matrix is then multiplied by the model parameters matrix θ 1 to produce the hidden state matrix [6]. In practice, acts as an intermediary state that interacts with the second parameters matrix θ 2 to calculate the output (x) [6]. The size and number of hidden layers can be varied to model systems of varying complexity.xmnxaahθ<br>Note that (x) is the output variable of interest and can represent a range of metrics that we wish to optimize. PUE is selected here to represent DC operational efficiency, with recognition that the metric is a ratio and not indicative of total facility­level energy consumption. Other examples include using server utilization data to maximize machine productivity, or equipment failure data to understand how the DC environment impacts reliability. The neural network will search for relationships between data features to generate a mathematical model that describes (x) as a function of the inputs. Understanding the underlying mathematical behavior of (x) allows us to control and optimize it.hθhθhθ<br><img src="https://www.guanacossj.com/media/articlebodypics/1574950962566.jpg" alt=""><br>The process of training a neural network model can be broken down into four steps, each of which are covered in greater detail below: </p><ul><li>Randomly initialize the model parameters θ </li><li>Implement the forward propagation algorithm,</li><li>Compute the cost function (θ)</li><li>Implement the back propagation algorithm and (5) Repeat steps 2 ­ 4 until convergence or the desired number of iterations</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Machine Learning Applications for Data Center Optimization&lt;br&gt;
    
    </summary>
    
    
      <category term="Paper" scheme="http://arithmeticjia.github.io/categories/Paper/"/>
    
    
      <category term="datacenter" scheme="http://arithmeticjia.github.io/tags/datacenter/"/>
    
      <category term="machinelearning" scheme="http://arithmeticjia.github.io/tags/machinelearning/"/>
    
  </entry>
  
  <entry>
    <title>Django个人博客搭建教程-登录登出模块</title>
    <link href="http://arithmeticjia.github.io/2019/11/28/Django%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B-%E7%99%BB%E5%BD%95%E7%99%BB%E5%87%BA%E6%A8%A1%E5%9D%97/"/>
    <id>http://arithmeticjia.github.io/2019/11/28/Django%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B-%E7%99%BB%E5%BD%95%E7%99%BB%E5%87%BA%E6%A8%A1%E5%9D%97/</id>
    <published>2019-11-28T12:57:47.000Z</published>
    <updated>2019-11-28T13:07:03.522Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要讲解自定义的登录登出模块，不涉及Django自带的认证模块，包括登录检查装饰器、链接自动跳转登录前url等<br><a id="more"></a></p><h4 id="models-py"><a href="#models-py" class="headerlink" title="models.py"></a>models.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BlogUser</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="string">'''用户表'''</span></span><br><span class="line"></span><br><span class="line">    gender = (</span><br><span class="line">        (<span class="string">'male'</span>,<span class="string">'男'</span>),</span><br><span class="line">        (<span class="string">'female'</span>,<span class="string">'女'</span>),</span><br><span class="line">    )</span><br><span class="line">    status = (</span><br><span class="line">        (<span class="string">'active'</span>,<span class="string">'有效'</span>),</span><br><span class="line">        (<span class="string">'disabled'</span>,<span class="string">'无效'</span>),</span><br><span class="line">    )</span><br><span class="line">    id = models.AutoField(primary_key=<span class="literal">True</span>)</span><br><span class="line">    name = models.CharField(max_length=<span class="number">128</span>,unique=<span class="literal">True</span>)</span><br><span class="line">    password = models.CharField(max_length=<span class="number">256</span>)</span><br><span class="line">    email = models.EmailField(unique=<span class="literal">True</span>)</span><br><span class="line">    sex = models.CharField(max_length=<span class="number">32</span>,choices=gender,default=<span class="string">'男'</span>)</span><br><span class="line">    c_time = models.DateTimeField(auto_now_add=<span class="literal">True</span>)</span><br><span class="line">    userpic = models.ImageField(upload_to=<span class="string">'userpic'</span>,blank=<span class="literal">True</span>,null=<span class="literal">True</span>)</span><br><span class="line">    status = models.CharField(max_length=<span class="number">32</span>,choices=status,default=<span class="string">'有效'</span>)</span><br><span class="line">    brief = models.CharField(max_length=<span class="number">1024</span>,blank=<span class="literal">True</span>,null=<span class="literal">True</span>)</span><br><span class="line">    role = models.ManyToManyField(BlogRole, blank=<span class="literal">True</span>,null=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h4 id="views-py"><a href="#views-py" class="headerlink" title="views.py"></a>views.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check_login</span><span class="params">(f)</span>:</span></span><br><span class="line"><span class="meta">    @wraps(f)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">inner</span><span class="params">(request, *arg, **kwargs)</span>:</span></span><br><span class="line">        next_url = request.path_info</span><br><span class="line">        print(next_url)</span><br><span class="line">        <span class="keyword">if</span> request.session.get(<span class="string">'is_login'</span>) == <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">return</span> f(request, *arg, **kwargs)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> redirect(<span class="string">'/JiaBlog/login/?next=%s'</span> % next_url)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inner</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">login_view</span><span class="params">(request)</span>:</span></span><br><span class="line">    <span class="comment"># 当前端点击登录按钮时，提交数据到后端，进入该POST方法</span></span><br><span class="line">    <span class="keyword">if</span> request.method == <span class="string">"POST"</span>:</span><br><span class="line">        <span class="comment"># 获取用户名和密码</span></span><br><span class="line">        username = request.POST.get(<span class="string">"username"</span>)</span><br><span class="line">        password = request.POST.get(<span class="string">"password"</span>)</span><br><span class="line">        <span class="comment"># 在前端传回时也将跳转链接传回来</span></span><br><span class="line">        next_url = request.POST.get(<span class="string">"next_url"</span>)</span><br><span class="line">        print(next_url)</span><br><span class="line">        <span class="comment"># 对用户进行验证</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            user = models.BlogUser.objects.get(name=username)</span><br><span class="line">            <span class="keyword">if</span> user.status == <span class="string">'active'</span> <span class="keyword">or</span> user.status == <span class="string">'有效'</span>:</span><br><span class="line">                <span class="keyword">if</span> user.password == password:</span><br><span class="line">                    request.session[<span class="string">'is_login'</span>] = <span class="literal">True</span></span><br><span class="line">                    request.session[<span class="string">'user_id'</span>] = user.id</span><br><span class="line">                    request.session[<span class="string">'user_name'</span>] = user.name</span><br><span class="line">                    <span class="comment"># 如果跳转链接不为空并且跳转页面不是登出页面，则登录成功后跳转，否则直接进入主页</span></span><br><span class="line">                    <span class="keyword">if</span> next_url <span class="keyword">and</span> next_url != <span class="string">"/JiaBlog/logout/"</span>:</span><br><span class="line">                        response = redirect(next_url)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        response = redirect(<span class="string">"/JiaBlog/index/"</span>)</span><br><span class="line">                    <span class="keyword">return</span> response</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    message = <span class="string">"密码不正确！"</span></span><br><span class="line">            <span class="comment"># 若用户名或密码失败,则将提示语与跳转链接继续传递到前端</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                message = <span class="string">"用户状态信息异常，请联系管理员(18351922995)! "</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            message = <span class="string">"用户不存在！"</span></span><br><span class="line">        <span class="keyword">return</span> render(request, <span class="string">'login.html'</span>, locals())</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        next_url = request.GET.get(<span class="string">"next"</span>, <span class="string">''</span>)</span><br><span class="line">        <span class="keyword">return</span> render(request, <span class="string">"login.html"</span>, &#123;<span class="string">'next_url'</span>: next_url&#125;,locals())</span><br></pre></td></tr></table></figure><h4 id="urls-py"><a href="#urls-py" class="headerlink" title="urls.py"></a>urls.py</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">url(<span class="string">r'^login/$'</span>, views.login_view,name=<span class="string">'login'</span>),</span><br></pre></td></tr></table></figure><h4 id="login-html"><a href="#login-html" class="headerlink" title="login.html"></a>login.html</h4><figure class="highlight html"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"container"</span>&gt;</span></span><br><span class="line"></span><br><span class="line">      <span class="tag">&lt;<span class="name">form</span> <span class="attr">class</span>=<span class="string">"form-signin"</span> <span class="attr">method</span>=<span class="string">"post"</span> <span class="attr">action</span>=<span class="string">"&#123;% url 'JiaBlog:login' %&#125;"</span>&gt;</span></span><br><span class="line">          &#123;% if message %&#125;</span><br><span class="line">              <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"alert alert-warning"</span>&gt;</span>&#123;&#123; message &#125;&#125;<span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">          &#123;% endif %&#125;</span><br><span class="line">          &#123;% csrf_token %&#125;</span><br><span class="line">        <span class="tag">&lt;<span class="name">h2</span> <span class="attr">class</span>=<span class="string">"form-signin-heading"</span> <span class="attr">style</span>=<span class="string">"text-align: center"</span>&gt;</span>Sign In<span class="tag">&lt;/<span class="name">h2</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">"inputEmail"</span> <span class="attr">class</span>=<span class="string">"sr-only"</span>&gt;</span>Email address<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">id</span>=<span class="string">"inputUsername"</span> <span class="attr">name</span>=<span class="string">'username'</span> <span class="attr">class</span>=<span class="string">"form-control"</span> <span class="attr">placeholder</span>=<span class="string">"Username"</span> <span class="attr">required</span> <span class="attr">autofocus</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">br</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">label</span> <span class="attr">for</span>=<span class="string">"inputPassword"</span> <span class="attr">class</span>=<span class="string">"sr-only"</span>&gt;</span>Password<span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"password"</span> <span class="attr">id</span>=<span class="string">"inputPassword"</span> <span class="attr">name</span>=<span class="string">"password"</span> <span class="attr">class</span>=<span class="string">"form-control"</span> <span class="attr">placeholder</span>=<span class="string">"Password"</span> <span class="attr">required</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">"display: none;"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">input</span> <span class="attr">id</span>=<span class="string">"next"</span> <span class="attr">type</span>=<span class="string">"text"</span> <span class="attr">name</span>=<span class="string">"next_url"</span> <span class="attr">value</span>=<span class="string">"&#123;&#123; next_url &#125;&#125;"</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"checkbox"</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">label</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">input</span> <span class="attr">type</span>=<span class="string">"checkbox"</span> <span class="attr">value</span>=<span class="string">"remember-me"</span>&gt;</span> Remember me</span><br><span class="line">          <span class="tag">&lt;/<span class="name">label</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">button</span> <span class="attr">class</span>=<span class="string">"btn btn-lg btn-primary btn-block"</span> <span class="attr">type</span>=<span class="string">"submit"</span>&gt;</span>Sign in<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">form</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"form-signin"</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">button</span> <span class="attr">class</span>=<span class="string">"btn btn-lg btn-primary btn-block"</span> <span class="attr">onclick</span>=<span class="string">"window.location.href = '/JiaBlog/register'"</span>&gt;</span>Register in<span class="tag">&lt;/<span class="name">button</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">"text-align: center"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">"display:inline-block;"</span>&gt;</span><span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"iconfont icon-github"</span>&gt;</span>&amp;nbsp;<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">style</span>=<span class="string">"color: black;text-decoration: none;"</span> <span class="attr">href</span>=<span class="string">"&#123;% url "</span><span class="attr">social:begin</span>" "<span class="attr">github</span>" %&#125;"&gt;</span>GitHub<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span>&amp;nbsp;&amp;nbsp;&amp;nbsp;</span><br><span class="line">            <span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">"display:inline-block;"</span>&gt;</span><span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"iconfont icon-weibo"</span>&gt;</span>&amp;nbsp;<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">style</span>=<span class="string">"color: black;text-decoration: none;"</span> <span class="attr">href</span>=<span class="string">"&#123;% url "</span><span class="attr">social:begin</span>" "<span class="attr">weibo</span>" %&#125;"&gt;</span>Weibo<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span>&amp;nbsp;&amp;nbsp;&amp;nbsp;</span><br><span class="line">            <span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">"display:inline-block;"</span>&gt;</span><span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"iconfont icon-Facebook"</span>&gt;</span>&amp;nbsp;<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">style</span>=<span class="string">"color: black;text-decoration: none;"</span> <span class="attr">href</span>=<span class="string">"&#123;% url "</span><span class="attr">social:begin</span>" "<span class="attr">facebook</span>" %&#125;"&gt;</span>Facebook<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span>&amp;nbsp;&amp;nbsp;&amp;nbsp;</span><br><span class="line">            <span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">"display:inline-block;"</span>&gt;</span><span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"iconfont icon-google"</span>&gt;</span>&amp;nbsp;<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">style</span>=<span class="string">"color: black;text-decoration: none;"</span> <span class="attr">href</span>=<span class="string">"&#123;% url "</span><span class="attr">social:begin</span>" "<span class="attr">google-oauth2</span>" %&#125;"&gt;</span>Google<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">&#123;#          <span class="tag">&lt;<span class="name">div</span> <span class="attr">style</span>=<span class="string">"text-align: center"</span>&gt;</span><span class="tag">&lt;<span class="name">span</span> <span class="attr">class</span>=<span class="string">"iconfont icon-github"</span>&gt;</span>&amp;nbsp;<span class="tag">&lt;/<span class="name">span</span>&gt;</span><span class="tag">&lt;<span class="name">a</span> <span class="attr">style</span>=<span class="string">"color: black;text-decoration: none;"</span> <span class="attr">href</span>=<span class="string">"&#123;% url "</span><span class="attr">social:begin</span>" "<span class="attr">facebook</span>" %&#125;"&gt;</span>Facebook Login<span class="tag">&lt;/<span class="name">a</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span>#&#125;</span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">br</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"fb-root"</span> <span class="attr">style</span>=<span class="string">"text-align: center"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">script</span> <span class="attr">async</span> <span class="attr">defer</span> <span class="attr">crossorigin</span>=<span class="string">"anonymous"</span> <span class="attr">src</span>=<span class="string">"https://connect.facebook.net/zh_CN/sdk.js#xfbml=1&amp;version=v5.0&amp;appId=2513272488741954&amp;autoLogAppEvents=1"</span>&gt;</span><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">div</span> <span class="attr">class</span>=<span class="string">"fb-login-button"</span> <span class="attr">data-width</span>=<span class="string">""</span> <span class="attr">data-size</span>=<span class="string">"medium"</span> <span class="attr">data-button-type</span>=<span class="string">"continue_with"</span> <span class="attr">data-auto-logout-link</span>=<span class="string">"true"</span> <span class="attr">data-use-continue-as</span>=<span class="string">"true"</span> <span class="attr">data-onlogin</span>=<span class="string">"checkLoginState()"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"status"</span>&gt;</span><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">&#123;#            <span class="tag">&lt;<span class="name">fb:login-button</span> <span class="attr">scope</span>=<span class="string">"public_profile,email"</span> <span class="attr">onlogin</span>=<span class="string">"checkLoginState();"</span>&gt;</span><span class="tag">&lt;/<span class="name">fb:login-button</span>&gt;</span>#&#125;</span><br><span class="line">        <span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">div</span>&gt;</span> <span class="comment">&lt;!-- /container --&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要讲解自定义的登录登出模块，不涉及Django自带的认证模块，包括登录检查装饰器、链接自动跳转登录前url等&lt;br&gt;
    
    </summary>
    
    
      <category term="Django" scheme="http://arithmeticjia.github.io/categories/Django/"/>
    
    
      <category term="django" scheme="http://arithmeticjia.github.io/tags/django/"/>
    
  </entry>
  
</feed>
