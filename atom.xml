<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>请叫我算术嘉的博客</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://arithmeticjia.github.io/"/>
  <updated>2020-03-06T05:14:18.771Z</updated>
  <id>http://arithmeticjia.github.io/</id>
  
  <author>
    <name>请叫我算术嘉</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2020-3.6周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/03/06/2020-3-6%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/03/06/2020-3-6%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-03-06T04:34:50.000Z</published>
    <updated>2020-03-06T05:14:18.771Z</updated>
    
    <content type="html"><![CDATA[<p>Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models<br><a id="more"></a></p><h3 id="Shape-and-Time-Distortion-Loss-for-Training-DeepTime-Series-Forecasting-Models"><a href="#Shape-and-Time-Distortion-Loss-for-Training-DeepTime-Series-Forecasting-Models" class="headerlink" title="Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models"></a>Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models</h3><p>NeurIPS 2019</p><p>训练深度时间序列预测模型的形状和时间失真损失</p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><p>This paper addresses the problem of time series forecasting for non-stationary signals and multiple future steps prediction. </p><p>DILATE (DIstortion Loss including shApe and TimE) 形状和时间失真损失</p><p>DILATE aims at accurately predicting sudden changes, and explicitly incorporates two terms supporting precise shape and temporal change detection.</p><h4 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h4><p>Time series forecasting [6] consists in analyzing the dynamics and correlations between historical data for predicting future behavior</p><p>In one-step prediction problems [39, 30], future prediction reduces to a single scalar value. This is in sharp contrast with multi-step time series prediction [49, 2, 48], which consists in predicting a complete trajectory[trəˈdʒektəri] of future data at a rather long temporal extent. Multi-step forecasting thus requires to accurately describe time series evolution.</p><p><img src="https://pic1.zhimg.com/v2-b872cd50b4a341901005bf4246493fa0_r.jpg" alt=""></p><p>(a) Non informative prediction 非信息性预测<br>(b) Correct shape, time delay<br>(c) Correct time, inaccurate shape</p><p>In contrast, the DILATE loss proposed in this work, which disentangles shape and temporal decay terms,<br>supports predictions (b) and (c) over prediction (a) that does not capture the sharp change of regime.</p><h4 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h4><p>Time series forecasting Traditional methods for time series forecasting include linear autoregressive models, such as the ARIMA model [6], and Exponential[ˌekspəˈnenʃl] Smoothing [27], which both fall into the broad category of linear State Space Models (SSMs) [17].</p><h4 id="Training-Deep-Neural-Networks-with-DILATE"><a href="#Training-Deep-Neural-Networks-with-DILATE" class="headerlink" title="Training Deep Neural Networks with DILATE"></a>Training Deep Neural Networks with DILATE</h4><p><img src="https://pic3.zhimg.com/v2-fc51e16266d817369cbd3bcbd6624552_b.jpg" alt=""></p><p>Input:</p><script type="math/tex; mode=display">A=\{ X_{i} \}_{i\in \{1:N\}}</script><p>对于</p><script type="math/tex; mode=display">x_{i} = (x_{i}^{1},...,x_{i}^{n})</script><p>predicts the future k-step ahead trajectory </p><script type="math/tex; mode=display">\hat{y}_{i} = (\hat{y}_{i}^{1},...,\hat{y}_{i}^{k})</script><p>actual ground truth future trajectory</p><script type="math/tex; mode=display">\dot{y}_{i} = (\dot{y}_{i}^{1},...,\dot{y}_{i}^{k})</script><p><img src="https://www.guanacossj.com/media/articlebodypics/1583471554346.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Shape and Time Distortion Loss for Training DeepTime Series Forecasting Models&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-2-28周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/02/28/2020-2-28%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/02/28/2020-2-28%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-02-28T04:22:34.000Z</published>
    <updated>2020-02-28T06:32:20.573Z</updated>
    
    <content type="html"><![CDATA[<p>Memory In Memory（学习高阶非平稳特征信息）<br><a id="more"></a><br>Memory In Memory: A Predictive Neural Network for Learning Higher-Order Non-Stationarity from Spatiotemporal Dynamics<br>一种用于高阶学习的预测神经网络时空动力学中的非平稳性</p><p>cvpr2019</p><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>any non-stationary process can be decomposed into deterministic, time-variant polynomials[ˌpɒlɪˈnəʊmiəlz] , plus a zero-mean stochastic term. </p><p>任何一个非平稳过程都可以分解为：确定项+时间变量多项式+零均值随机项</p><p>By applying differencing operations appropriately, we may turn time-variant polynomials into a constant, making the deterministic[dɪˌtɜːmɪˈnɪstɪk] component predictable.</p><p>通过差分的操作，我们可以把时间变量多项式转换成一个常量，使确定性的组成部分可预测</p><p>We propose the Memory In Memory (MIM) networks and corresponding recurrent blocks for this purpose. The MIM blocks exploit the differential signals between adjacent recurrent states to model the non-stationary and approximately stationary properties in spatiotemporal dynamics with two cascaded, self-renewed memory modules.</p><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>We attempt to resolve this problem by proposing a generic RNNs architecture that is more effective in non-stationarity modeling. </p><p>In particular, the forget gates in the recent PredRNN model [32] does not work appropriately on precipitation forecasting: about 80% of them are saturated over all timestamps, implying almost timeinvariant memory state transitions. </p><h3 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h3><h4 id="ARIMA-Autoregressive-Integrated-Moving-Average-Model"><a href="#ARIMA-Autoregressive-Integrated-Moving-Average-Model" class="headerlink" title="ARIMA(Autoregressive Integrated Moving Average Model)"></a>ARIMA(Autoregressive Integrated Moving Average Model)</h4><p>A time-series random variable whose power spectrum remains constant over time can be viewed as a combination of signal and noise. </p><p>功率谱是功率谱密度函数（PSD）的简称，它定义为单位频带内的信号功率</p><p><img src="https://www.guanacossj.com/media/articlebodypics/w.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/w_f.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/p.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/f.jpg" alt=""></p><script type="math/tex; mode=display">|S(f)|^2</script><script type="math/tex; mode=display">\lim_{T->\propto }\frac{1}{T}|S(f)|^2</script><h4 id="Deterministic-Spatiotemporal-Prediction"><a href="#Deterministic-Spatiotemporal-Prediction" class="headerlink" title="Deterministic Spatiotemporal Prediction"></a>Deterministic Spatiotemporal Prediction</h4><h4 id="Stochastic-Spatiotemporal-Prediction"><a href="#Stochastic-Spatiotemporal-Prediction" class="headerlink" title="Stochastic Spatiotemporal Prediction"></a>Stochastic Spatiotemporal Prediction</h4><h3 id="Memory-In-Memory"><a href="#Memory-In-Memory" class="headerlink" title="Memory In Memory"></a>Memory In Memory</h3><p><img src="https://pic1.zhimg.com/v2-dc4a2024ce0201315661daf3b43c6ab8_r.jpg" alt=""></p><p>左边是ST-LSTM结构，右边是更改的</p><p>ST-LSTM中的忘记门基本是饱和的，所以它基本上只获取了平稳的信息，而整个直接联系就是C状态值，再加上下面的输入为差分，而差分的转换其实就是非平稳的信息</p><p><img src="https://pic1.zhimg.com/v2-69b9793882f1a8739d6b1d37336cc808_r.jpg" alt=""></p><p><img src="https://pic3.zhimg.com/v2-f1eac55dddaa00ab46da1b4ea116072e_r.jpg" alt=""></p><p><img src="https://pic3.zhimg.com/80/v2-6ec88fa03dd40e82a037c13ff6b64bd2_1440w.jpg" alt=""></p><h3 id="Memory-In-Memory-Networks"><a href="#Memory-In-Memory-Networks" class="headerlink" title="Memory In Memory Networks"></a>Memory In Memory Networks</h3><p><img src="https://pic3.zhimg.com/v2-06741d37a4c01fce37ef43901f5b311e_r.jpg" alt=""></p><p>红色箭头：用于微分建模的H的对角状态转移路径</p><p>蓝色箭头：存储单元C，N和S的水平转换路径</p><p>黑色箭头：之字形状态</p><h3 id="experience"><a href="#experience" class="headerlink" title="experience"></a>experience</h3><p>模型参数：一共四层，第一层是ST-LSTM，其余三层为MIM，MIM的feature channel为64，利用l2损失，ADAM optimizer，lr为0.001，利用了两个trick，为layer nomalization和scheduled sampling</p><p><img src="https://pic3.zhimg.com/v2-fec039cb2962195deb890b6680c5395a_r.jpg" alt=""></p><h3 id="GluonTS-AWS"><a href="#GluonTS-AWS" class="headerlink" title="GluonTS(AWS)"></a>GluonTS(AWS)</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/gluonts_all.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/gluonts67.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Memory In Memory（学习高阶非平稳特征信息）&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-2-21周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/02/20/2020-2-21%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/02/20/2020-2-21%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-02-20T12:53:19.000Z</published>
    <updated>2020-02-21T07:20:17.207Z</updated>
    
    <content type="html"><![CDATA[<p>PredRNN++…<br><a id="more"></a></p><h3 id="PredRNN"><a href="#PredRNN" class="headerlink" title="PredRNN++:"></a>PredRNN++:</h3><p>Towards A Resolution of the Deep-in-Time Dilemma in Spatiotemporal Predictive Learning</p><p>旨在解决时空预测的深层次时间困境</p><p>ICML2018 Tsinghua</p><h4 id="PredRNN-1"><a href="#PredRNN-1" class="headerlink" title="PredRNN"></a>PredRNN</h4><p>nips2017 Tsinghua</p><p>PredRNN: Recurrent Neural Networks for Predictive Learning using Spatiotemporal LSTMs</p><p>用ST-LSTM的预测学习循环神经网络(spatial + temporal)</p><p>PredRNN利用了一种双重记忆机制，通过简单的门控级联，将水平更新的时间记忆C与垂直转换的空间记忆M结合起来</p><p>先来回忆一下LSTM</p><p><img src="https://pic2.zhimg.com/v2-810f2d553fa6e6f43854efdc881be8a1_r.jpg" alt=""></p><ol><li>$h_{t-1}$与$X_{t}$做concat操作，之后经过sigmoid形成[0, 1]的忘记门，输入门，输出门</li><li>Ct-1通过忘记门 </li><li>ht-1与Xt做concat操作通过tanh激活函数，通过输入门（这里在通过输入门之前相当于生成了此时的输入生成状态） </li><li>以上通过遗忘门和输入门的两个向量相加就是最后的Ct，也就是此时的cell state</li><li>最后，这个cell state通过再一次的非线性变化tanh 最终通过输出门输出得到最后的ht</li></ol><p>Spatiotemporal memory flow</p><p><img src="https://pic4.zhimg.com/80/v2-7c898aed50f1e1d9aee647e4c273ad33_hd.jpg" alt=""></p><p><img src="https://pic1.zhimg.com/80/v2-f5c836f08237baea9393aefce80d0fd8_hd.jpg" alt=""></p><p>缺点:</p><ol><li>去掉水平方向的时间流，会牺牲时间上的一致性，因为在同一层的不同时间没有时间流了。 </li><li>记忆需要在遥远的状态之间流动更长的路径，更容易造成梯度消失。 所以引入了一个新的building blocks为ST-LSTM。</li></ol><p>Spatiotemporal LSTM</p><p><img src="https://pic4.zhimg.com/v2-f3cd76086384ede22b29e6c8a5f7f45b_r.jpg" alt=""></p><p><img src="https://pic2.zhimg.com/80/v2-fdb1465c439cd9f3eea4ee52bf2b4125_hd.jpg" alt=""></p><p>震惊！！！</p><ul><li>上半部分就是LSTM(Standard Temporal Memory)</li><li>下半部分相当于把c和h一起更改为M，M即时空记忆状态(Spatiotemporal Memory)</li></ul><p><img src="https://pic2.zhimg.com/80/v2-bbe7560a50ff9d511746fb94562ebd39_hd.jpg" alt=""></p><h4 id="PredRNN-2"><a href="#PredRNN-2" class="headerlink" title="PredRNN++"></a>PredRNN++</h4><ul><li>Stacked ConvLSTMs(nips2015)</li><li>Deep Transition ConvLSTMs</li><li>Pred RNN 红线表示空间记忆的深度过渡路径，水平的黑色箭头表示时间记忆的更新方向<br><img src="https://img-blog.csdnimg.cn/20191222210421460.png?#pic_center" alt=""></li></ul><p>Causal LSTM(因果长短期记忆)<br>通过这种方式，将获得更强大的建模能力，以实现更强的空间相关性和短期动态</p><p><img src="https://img-blog.csdnimg.cn/20191222205916540.png?#pic_center" alt=""></p><ul><li>每个门不是由X和H决定，而是由X和H以及C决定，通过输入门之前的状态也是由三者决定的</li><li>两个memory结构，即C和M，C为temporal state，M为spatial state，因为输入C为上一个时刻的C，M是上一层的M，所以这里C与时间维度有关，M与空间维度有关</li><li>M作为第二部分的state输入，并且通过忘记门之前做了一个非线性操作tanh</li></ul><p>对比ST-LSTM来说，Causal LSTM对于M和H定义更加清晰，并且不是简单的concat，而是采用了一个递归深度更深的一个级联结构最终输出H</p><p>Gradient Highway(高速梯度)</p><p>通过Recurrent Highway Networks的思想能够证明高速网络能够有效的在非常深的网络中传递梯度，继而防止长时导致的梯度消失</p><p><img src="https://pic3.zhimg.com/80/v2-3b541bda171d5f4c9299c77326e13702_hd.jpg" alt=""></p><p><img src="https://pic1.zhimg.com/80/v2-cf06c70d336a89700435195a0574b39c_hd.jpg" alt=""></p><p>总体架构</p><p><img src="https://pic3.zhimg.com/80/v2-61d4c59ff38010cb9613574bf0290c9a_hd.jpg" alt=""></p><p>GHU连接了当前时刻以及前一个时刻的输入，引导的结果就是梯度不再是一股线传播了，而是可以直接在第一层与第二层之间有个高速的传播，换句话讲就是传播的距离缩短了，也就变得没有之前的那么’深‘了，可以有效的解决梯度消失的问题</p><h3 id="Block-Hankel-Tensor-ARIMA-for-Multiple-Short-Time-Series-Forecasting"><a href="#Block-Hankel-Tensor-ARIMA-for-Multiple-Short-Time-Series-Forecasting" class="headerlink" title="Block Hankel Tensor ARIMA for Multiple Short Time Series Forecasting"></a>Block Hankel Tensor ARIMA for Multiple Short Time Series Forecasting</h3><h4 id="Hankel-汉克尔矩阵（每一条逆对角线上的元素都相等的矩阵）"><a href="#Hankel-汉克尔矩阵（每一条逆对角线上的元素都相等的矩阵）" class="headerlink" title="Hankel:汉克尔矩阵（每一条逆对角线上的元素都相等的矩阵）"></a>Hankel:汉克尔矩阵（每一条逆对角线上的元素都相等的矩阵）</h4><script type="math/tex; mode=display">\begin{bmatrix} 1&  2&  3&  4&  5&  6& 7\\  2&  3&  4&  5&  6&  7& 8\\  3&  4&  5&  6&  7&  8& 9\end{bmatrix}</script><h4 id="Tucker分解"><a href="#Tucker分解" class="headerlink" title="Tucker分解"></a>Tucker分解</h4><p>这是一个三阶张量</p><p><img src="/Users/Arithmetic/Pictures/tensor.png" alt=""></p><p>秩一张量：如果一个K阶张量能够表示成K个向量的外积，那么该张量称为秩一张量<br>[[3 4],[6 8]]这个二阶张量可以表示为[1 2]○[3 4]的外积，那么这就是一个二阶秩一张量<br><img src="https://www.guanacossj.com/media/articlebodypics/1582207111495.jpg" alt=""></p><p>CP分解<br>CP分解其实就是多个rank-one tensors的和</p><p><img src="/Users/Arithmetic/Pictures/cp.png" alt=""></p><p>公式表示如下：</p><p><img src="/Users/Arithmetic/Pictures/cp_f.png" alt=""></p><p>tucker分解</p><p><img src="http://www.xiongfuli.com/assets/img/201606/tucker.png" alt=""></p><p><img src="/Users/Arithmetic/Pictures/tucker_f.png" alt=""></p><p>这里A$\in$R$^{I\times P}$,B$\in$R$^{J\times Q}$,C$\in$R$^{K\times R}$是因子矩阵（通常是正交的），可以当做是每一维上的主要成分。核张量表示每一维成分之间的联系<br>因此，对于一个三阶张量，可以通过tucker分解为三个二阶因子矩阵和一个三阶核向量</p><h4 id="Step1-Block-Hankel-Tensor-via-MDT"><a href="#Step1-Block-Hankel-Tensor-via-MDT" class="headerlink" title="Step1: Block Hankel Tensor via MDT"></a>Step1: Block Hankel Tensor via MDT</h4><p>MDT:multi-way delay embedding transform(多路延迟变换)<br>目的是利用MDT将多个TS转换成一个高阶的块Hankel张量<br>假设有1000条时间序列，每条序列的长度为40，即 I = 1000，T=40，设置参数t = 5<br>经MDT沿着时间维度变换后，得到一个1000✖️5*✖️（40-5+1）=1000✖️5✖️36的三维张量</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PredRNN++…&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-2.14周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/02/13/2020-2-14%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/02/13/2020-2-14%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-02-13T13:27:22.000Z</published>
    <updated>2020-02-14T07:33:34.201Z</updated>
    
    <content type="html"><![CDATA[<p>DA-RNN + Transformer + CW-RNN<br><a id="more"></a></p><h3 id="彩蛋"><a href="#彩蛋" class="headerlink" title="彩蛋"></a>彩蛋</h3><ul><li>写了一篇关于使用百度Echarts绘制新型冠状病毒全国分布图的博客</li><li>收到来自皖南医学院弋矶山医院教育处金来润的邮件希望合作</li><li>其实就是他们团队收集了一点疫情数据打算发一篇文章，希望我按照他们的要求给他们画个图</li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/QQ20200214-0.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/QQ20200214-1.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/echarts.png" alt=""></p><h3 id="DA-RNN"><a href="#DA-RNN" class="headerlink" title="DA-RNN"></a>DA-RNN</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-67-1.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-67-2.jpg" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.256</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.348</span><br></pre></td></tr></table></figure><h3 id="CW-RNN-Clock-Work-RNN"><a href="#CW-RNN-Clock-Work-RNN" class="headerlink" title="CW-RNN(Clock Work RNN)"></a>CW-RNN(Clock Work RNN)</h3><p>ICML2014</p><p>本文介绍了对标准RNN体系结构的一个简单而强大的改进，即时钟工作RNN（CW-RNN），它将隐藏层划分为不同的模块，每个处理以自己的时间粒度输入，仅以指定的时钟速率进行计算。<br>CW-RNN没有使标准RNN模型更加复杂，而是减少了RNN参数的数量，显著提高了测试任务的性能，加快了网络评估的速度</p><p>Input = ($x_{1}$,$x_{2}$,…,$x_{t}$,…)</p><p>Output = ($y_{1}$,$y_{2}$,…,$y_{t}$,…)</p><p><img src="https://www.guanacossj.com/media/articlebodypics/cwrnn.jpg" alt=""></p><ul><li>把隐含层节点分成了若干个模块（在图中分成了3个模块，是为了方便说明，实际中的模块个数可以自定义），而且每个模块都分配了一个时钟周期（Ti），便于独立管理</li><li>隐含层之间的连接，在一个模块内部是全连接，但是模块之间是有方向的。模块之间的连接是从高时钟频率的模块指向低时钟频率的模块</li><li>标准RNN<script type="math/tex; mode=display">y_{H}^{(t)} = f_{H}(W_{H}*y^{(t-1)}+W_{I}*x^{(t)})</script><script type="math/tex; mode=display">y_{O}^{(t)} = f_{O}(W_{O}*y_{H}^{(t)})</script></li><li>只有当$t$ MOD $T_{i}$ = 0时才会被执行</li><li>{$T_{1}$,…,$T_{g}$}的设置是任意的，在论文中使用$T_{i}=2^{i-1}$</li></ul><p>$T_{1}=1$,$T_{2}=2$,$T_{3}=4$,$T_{4}=8$,$T_{5}=16$,$T_{6}=32$,$T_{7}=64$</p><ul><li>分块<script type="math/tex; mode=display">W_{H} = \begin{pmatrix}\\ W_{H_{1}}\\ .\\ .\\ W_{H_{g}}\end{pmatrix}</script><script type="math/tex; mode=display">W_{I} = \begin{pmatrix}\\ W_{I_{1}}\\ .\\ .\\ W_{I_{g}}\end{pmatrix}</script></li><li>不参与运算的部分置零<script type="math/tex; mode=display">\left\{\begin{matrix}\\ W_{H_{i}},t mod T_{i} = 0\\ 0,otherwise\end{matrix}\right.</script></li><li>将$W_{h}$强制转成上三角<script type="math/tex; mode=display">W_{H_{i}}={0_{1},...,0_{i-1},W_{H_{i,i}},...,W_{H_{i,g}}}</script></li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/cwrnn-2.jpg" alt=""></p><p>我们要处理序列中第6（t=6）个元素的时候，通过t与每个模块的时钟周期进行MOD（求余数）计算后可以得到只有前两个模块会参与运算。所以$W_{h}$和$W_{x}$矩阵除了上面两行之外，其他元素的值都是0。经过计算之后，得到的$h_{t}$也只有前两个模块有值。因此，我们也可以把CW-RNN过程看成是通过一些人工的干预，选择不同的隐含层节点进行工作</p><p><img src="http://ir.dlut.edu.cn/Uploads/ue/image/20151201/6358457411082925904194442.jpg" alt=""></p><ul><li>低时钟速率模块处理、保留和输出从输入中获得的长期信息序列，</li><li>高时钟速率模块则侧重于本地的高频信息</li></ul><p>作者对比了传统RNN、LSTM、CW-RNN，在取局部图的时候可以观察到，LSTM的回归效果相对平滑，而CW-RNN并没有这种缺陷<br><img src="https://www.guanacossj.com/media/articlebodypics/comparecwrnn.jpg" alt=""></p><p>我复现了下股票数据集上的效果，实验还没完全完成<br><img src="https://www.guanacossj.com/media/articlebodypics/cwrnn-stock.jpg" alt=""></p><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/multi_head_net.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/transf-67-1.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/transf-67-2.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;DA-RNN + Transformer + CW-RNN&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-1-17周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/01/17/2020-1-17%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/01/17/2020-1-17%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-01-17T08:38:22.000Z</published>
    <updated>2020-01-17T09:58:07.962Z</updated>
    
    <content type="html"><![CDATA[<p>Django+uwsgi+Nginx<br><a id="more"></a></p><h3 id="WSGI"><a href="#WSGI" class="headerlink" title="WSGI"></a>WSGI</h3><p>WSGI Web Server Gateway Interface</p><p><img src="https://pic1.zhimg.com/80/v2-6c4572c783816364f2569af961814430_hd.jpg" alt=""></p><p>WSGI是一种通信协议，WSGI 不是框架，也不是一个模块，而是介于 Web应用程序（Web框架）与 Web Server 之间交互的一种规范。</p><h3 id="uwsgi"><a href="#uwsgi" class="headerlink" title="uwsgi"></a>uwsgi</h3><ul><li>二进制协议，可以携带任何类型的数据。一个uwsgi分组的头4个字节描述了这个分组包含的数据类型。</li><li>uwsgi是一种线路协议而不是通信协议，在此常用于在uWSGI服务器与其他网络服务器的数据通信。</li></ul><h3 id="uWSGI"><a href="#uWSGI" class="headerlink" title="uWSGI"></a>uWSGI</h3><p>uWSGI是实现了uwsgi和WSGI两种协议的Web服务器，使用c语言开发。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> uwsgi</span><br></pre></td></tr></table></figure><ul><li>两级结构 在这种结构里，uWSGI作为服务器，它用到了HTTP协议以及wsgi协议，flask应用作为application，实现了wsgi协议。当有客户端发来请求，uWSGI接受请求，调用flask app得到相应，之后相应给客户端。 这里说一点，通常来说，Flask等web框架会自己附带一个wsgi服务器(这就是flask应用可以直接启动的原因)，但是这只是在开发阶段用到的，在生产环境是不够用的，所以用到了uwsgi这个性能高的wsgi服务器。</li><li>三级结构 在这种结构里，uWSGI作为中间件，它用到了uwsgi协议(与nginx通信)，wsgi协议(调用Flask app)。</li><li>提高web server性能(uWSGI处理静态资源不如nginx；nginx会在收到一个完整的http请求后再转发给wWSGI)。</li><li>nginx可以做负载均衡(前提是有多个服务器)，保护了实际的web服务器(客户端是和nginx交互而不是uWSGI)。</li></ul><h3 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a>Nginx</h3><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install nginx</span><br></pre></td></tr></table></figure><p>Nginx是一款轻量级的Web服务器、反向代理服务器，由于它的内存占用少，启动极快，高并发能力强，在互联网项目中广泛应用。</p><p><img src="https://pic2.zhimg.com/80/v2-4787a512240b238ebf928cd0651e1d99_hd.jpg" alt=""></p><h3 id="Django-uWSGI-Nginx"><a href="#Django-uWSGI-Nginx" class="headerlink" title="Django + uWSGI + Nginx"></a>Django + uWSGI + Nginx</h3><p><img src="https://img-blog.csdnimg.cn/20181216174304355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d5bWFpc3ls,size_16,color_FFFFFF,t_70" alt=""></p><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">user</span> www-data;</span><br><span class="line"><span class="attribute">worker_processes</span> auto;</span><br><span class="line"><span class="attribute">pid</span> /run/nginx.pid;</span><br><span class="line"><span class="attribute">include</span> /etc/nginx/modules-enabled/<span class="regexp">*.conf</span>;</span><br><span class="line"></span><br><span class="line"><span class="section">events</span> &#123;</span><br><span class="line"><span class="attribute">worker_connections</span> <span class="number">768</span>;</span><br><span class="line"><span class="comment"># multi_accept on;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">http</span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Basic Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="section">server</span> &#123;   <span class="comment"># 这个server标识我要配置了</span></span><br><span class="line"><span class="attribute">listen</span> <span class="number">80</span>;  <span class="comment"># 我要监听那个端口</span></span><br><span class="line"><span class="attribute">server_name</span> <span class="number">118.25.79.249</span> ;  <span class="comment"># 你访问的路径前面的url名称</span></span><br><span class="line"><span class="attribute">charset</span>  utf-<span class="number">8</span>; <span class="comment"># Nginx编码</span></span><br><span class="line"><span class="attribute">gzip</span> <span class="literal">on</span>;  <span class="comment"># 启用压缩,这个的作用就是给用户一个网页,比如3M压缩后1M这样传输速度就会提高很多</span></span><br><span class="line"><span class="attribute">gzip_types</span> text/plain application/x-javascript text/css text/javascript application/x-httpd-php application/json text/json image/jpeg image/gif image/png application/octet-stream;  <span class="comment"># 支持压缩的类型</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">error_page</span>  <span class="number">404</span>           /<span class="number">404</span>.html;  <span class="comment"># 错误页面</span></span><br><span class="line"><span class="attribute">error_page</span>   <span class="number">500</span> <span class="number">502</span> <span class="number">503</span> <span class="number">504</span>  /50x.html;  <span class="comment"># 错误页面</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定项目路径uwsgi</span></span><br><span class="line"><span class="attribute">location</span> / &#123;        <span class="comment"># 这个location就和咱们Django的url(r'^admin/', admin.site.urls),</span></span><br><span class="line"><span class="attribute">include</span> uwsgi_params;  <span class="comment"># 导入一个Nginx模块他是用来和uWSGI进行通讯的</span></span><br><span class="line"><span class="attribute">uwsgi_connect_timeout</span> <span class="number">30</span>;  <span class="comment"># 设置连接uWSGI超时时间</span></span><br><span class="line"><span class="attribute">uwsgi_pass</span>  <span class="number">127.0.0.1:8000</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定静态文件路径</span></span><br><span class="line"><span class="attribute">location</span> /static/ &#123;</span><br><span class="line"><span class="attribute">alias</span>  /home/mysite/static/;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="attribute">sendfile</span> <span class="literal">on</span>;</span><br><span class="line"><span class="attribute">tcp_nopush</span> <span class="literal">on</span>;</span><br><span class="line"><span class="attribute">tcp_nodelay</span> <span class="literal">on</span>;</span><br><span class="line"><span class="attribute">keepalive_timeout</span> <span class="number">65</span>;</span><br><span class="line"><span class="attribute">types_hash_max_size</span> <span class="number">2048</span>;</span><br><span class="line"><span class="comment"># server_tokens off;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># server_names_hash_bucket_size 64;</span></span><br><span class="line"><span class="comment"># server_name_in_redirect off;</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">include</span> /etc/nginx/mime.types;</span><br><span class="line"><span class="attribute">default_type</span> application/octet-stream;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># SSL Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">ssl_protocols</span> TLSv1 TLSv1.<span class="number">1</span> TLSv1.<span class="number">2</span>; <span class="comment"># Dropping SSLv3, ref: POODLE</span></span><br><span class="line"><span class="attribute">ssl_prefer_server_ciphers</span> <span class="literal">on</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Logging Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">access_log</span> /var/log/nginx/access.log;</span><br><span class="line"><span class="attribute">error_log</span> /var/log/nginx/error.log;</span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Gzip Settings</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">gzip</span> <span class="literal">on</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment"># gzip_vary on;</span></span><br><span class="line"><span class="comment"># gzip_proxied any;</span></span><br><span class="line"><span class="comment"># gzip_comp_level 6;</span></span><br><span class="line"><span class="comment"># gzip_buffers 16 8k;</span></span><br><span class="line"><span class="comment"># gzip_http_version 1.1;</span></span><br><span class="line"><span class="comment"># gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"><span class="comment"># Virtual Host Configs</span></span><br><span class="line"><span class="comment">##</span></span><br><span class="line"></span><br><span class="line"><span class="attribute">include</span> /etc/nginx/conf.d/<span class="regexp">*.conf</span>;</span><br><span class="line"><span class="attribute">include</span> /etc/nginx/sites-enabled/*;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#mail &#123;</span></span><br><span class="line"><span class="comment">## See sample authentication script at:</span></span><br><span class="line"><span class="comment">## http://wiki.nginx.org/ImapAuthenticateWithApachePhpScript</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">## auth_http localhost/auth.php;</span></span><br><span class="line"><span class="comment">## pop3_capabilities "TOP" "USER";</span></span><br><span class="line"><span class="comment">## imap_capabilities "IMAP4rev1" "UIDPLUS";</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#server &#123;</span></span><br><span class="line"><span class="comment">#listen     localhost:110;</span></span><br><span class="line"><span class="comment">#protocol   pop3;</span></span><br><span class="line"><span class="comment">#proxy      on;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"><span class="comment"># </span></span><br><span class="line"><span class="comment">#server &#123;</span></span><br><span class="line"><span class="comment">#listen     localhost:143;</span></span><br><span class="line"><span class="comment">#protocol   imap;</span></span><br><span class="line"><span class="comment">#proxy      on;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br><span class="line"><span class="comment">#&#125;</span></span><br></pre></td></tr></table></figure><figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[uwsgi] </span><br><span class="line">chdir = /home/mysite</span><br><span class="line">module = mysite.wsgi:application</span><br><span class="line">socket = 127.0.0.1:8000</span><br><span class="line">master = true </span><br><span class="line">processes = 1</span><br><span class="line">threads = 2</span><br><span class="line">max-requests = 6000</span><br><span class="line">chmod-socket = 666</span><br><span class="line">buffer-size = 65535</span><br><span class="line">logto = /var/log/mysite.log</span><br><span class="line">async</span><br><span class="line">ugreen =''</span><br><span class="line">http-timeout = 300</span><br><span class="line"><span class="comment">#plugins=python</span></span><br></pre></td></tr></table></figure><h3 id="Activemq"><a href="#Activemq" class="headerlink" title="Activemq"></a>Activemq</h3><p>ActiveMQ 是 Apache 出品，最流行的，能力强劲的开源消息总线。ActiveMQ 是一个完全支持 JMS1.1 和 J2EE 1.4 规范的 JMS Provider 实现。</p><h4 id="queue"><a href="#queue" class="headerlink" title="queue"></a>queue</h4><p><img src="https://pic4.zhimg.com/80/v2-b7edcfa850af9627bed67ef9e89f8d3f_hd.jpg" alt=""></p><h4 id="topic"><a href="#topic" class="headerlink" title="topic"></a>topic</h4><p><img src="https://pic2.zhimg.com/80/v2-b1874d392a6119fb4e497425dcc58609_hd.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Django+uwsgi+Nginx&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>2020-1-10周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2020/01/07/2020-1-10%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2020/01/07/2020-1-10%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2020-01-07T06:07:24.000Z</published>
    <updated>2020-01-10T10:47:51.445Z</updated>
    
    <content type="html"><![CDATA[<p>“All you need is attention”<br><a id="more"></a></p><h3 id="LSTM-Attention"><a href="#LSTM-Attention" class="headerlink" title="LSTM + Attention"></a>LSTM + Attention</h3><p>FEED-FORWARD NETWORKS WITH ATTENTION CAN SOLVE SOME LONG-TERM MEMORY PROBLEMS</p><h4 id="FEED-FORWARD-ATTENTION"><a href="#FEED-FORWARD-ATTENTION" class="headerlink" title="FEED-FORWARD ATTENTION"></a>FEED-FORWARD ATTENTION</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/FEED-FORWARD-ATTENTION.jpg" alt=""></p><script type="math/tex; mode=display">e_{t} = a(h_{t})</script><script type="math/tex; mode=display">\alpha_{t} = \frac{exp(e_{t})}{\sum_{k=1}^{T}exp(e_{k})}</script><script type="math/tex; mode=display">c = \sum_{t=1}^{T}\alpha_{t}h_{t}</script><h4 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/all-lstmattention.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/test-lstmattention.png" alt=""></p><h3 id="Seq2Seq-Attention"><a href="#Seq2Seq-Attention" class="headerlink" title="Seq2Seq + Attention"></a>Seq2Seq + Attention</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/all-seq2seqattention.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/test-seq2seqattention.png" alt=""></p><h3 id="DA-RNN"><a href="#DA-RNN" class="headerlink" title="DA-RNN"></a>DA-RNN</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-gru.jpg" alt=""></p><h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h4><p>顾名思义，指的不是Target和Source之间的Attention机制，而是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力计算机制</p><p>给出信息输入：用X = [x1, · · · , xN ]表示N 个输入信息；通过线性变换得到为查询向量序列，键向量序列和值向量序列，其中$W^{Q}$,$W^{K}$,$W^{V}$是我们模型训练过程学习到的合适的参数</p><script type="math/tex; mode=display">Q = W^{Q}X</script><script type="math/tex; mode=display">K = W^{K}X</script><script type="math/tex; mode=display">V = W^{V}X</script><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix}*[v^{T}_{1},v^{T}_{2},...,v^{T}_{n}])*\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix} = softmax(QK^{T})V</script><p><img src="https://pic2.zhimg.com/v2-07c4c02a9bdecb23d9664992f142eaa5_r.jpg" alt=""></p><p>Source中的构成元素想象成是由一系列的<Key,Value>数据对构成<br>Target中的某个元素Query<br>(在Seq2Se2中，Q是Decoder的隐藏态，K和V都是Encoder的隐藏态)</p><ul><li>1、根据Query和Key计算权重系数，常用的相似度函数有点积，拼接，感知机等</li><li>2、使用softmax函数对这些权重进行归一化</li><li>3、根据权重系数对Value进行加权求和得到attention</li></ul><h4 id="Scaled-Dot-Product-Attention"><a href="#Scaled-Dot-Product-Attention" class="headerlink" title="Scaled Dot-Product Attention"></a>Scaled Dot-Product Attention</h4><p>防止Q和K点乘积结果过大，会除以一个尺度标度 </p><script type="math/tex; mode=display">Attention(Q,K,V) = sofrmax(\frac{QK^{T}}{\sqrt{d_{k}}})V</script><h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><ul><li>$Q$，$K$，$V$首先进过一个线性变换，然后输入到放缩点积attention</li><li>每次$Q$，$K$，$V$进行线性变换的参数$W$是不一样的</li><li>通过$h$个不同的线性变换对$Q$，$K$，$V$进行投影，最后将不同的attention结果拼接起来</li></ul><script type="math/tex; mode=display">Multihead(Q,K,V) = Concat(head_{1},...,head_{h})W^{O}</script><script type="math/tex; mode=display">head_{i} = Attention(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})</script><h4 id="Experiment-1"><a href="#Experiment-1" class="headerlink" title="Experiment"></a>Experiment</h4><p><img src="https://www.guanacossj.com/media/articlebodypics/all-transformer.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/test-transformer.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;“All you need is attention”&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>说说LSTM</title>
    <link href="http://arithmeticjia.github.io/2019/12/29/%E8%AF%B4%E8%AF%B4LSTM/"/>
    <id>http://arithmeticjia.github.io/2019/12/29/%E8%AF%B4%E8%AF%B4LSTM/</id>
    <published>2019-12-29T15:22:35.000Z</published>
    <updated>2019-12-29T15:34:02.931Z</updated>
    
    <content type="html"><![CDATA[<p>Long Short Term Memory<br><a id="more"></a></p><h4 id="从RNN开始"><a href="#从RNN开始" class="headerlink" title="从RNN开始"></a>从RNN开始</h4><p>RNN(Recurrent Neural Network)是一类用于处理序列数据的神经网络，擅长对序列数据进行建模处理。LSTM(Long Short-Term Memory) 在传统的 RNN 的基础上增加了状态$c$，称为记忆单元态 (cell state)，用以取代传统的隐含神经元节点。它负责把记忆信息从序列的初始位置，传递到序列的末端。</p><h4 id="LSTM的组成"><a href="#LSTM的组成" class="headerlink" title="LSTM的组成"></a>LSTM的组成</h4><p>在$t$时刻，当前神经元的输入有三个：当前时刻输入值$x_{t}$、前一时刻输出值$s_{t-1}$,和前一时刻的记忆单元状态$c_{t-1}$, 输出有两个，当前时刻LSTM的输出值$s_{t}$和当前时刻的记忆单元状态$c_{t}$。<br>LSTM通过三个门控开关传递记忆状态。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Long Short Term Memory&lt;br&gt;
    
    </summary>
    
    
      <category term="LSTM" scheme="http://arithmeticjia.github.io/categories/LSTM/"/>
    
    
      <category term="lstm" scheme="http://arithmeticjia.github.io/tags/lstm/"/>
    
      <category term="deeplearning" scheme="http://arithmeticjia.github.io/tags/deeplearning/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode78Pascal-Triangle-2-Java</title>
    <link href="http://arithmeticjia.github.io/2019/12/29/Leetcode78Pascal-Triangle-2-Java/"/>
    <id>http://arithmeticjia.github.io/2019/12/29/Leetcode78Pascal-Triangle-2-Java/</id>
    <published>2019-12-29T03:54:18.000Z</published>
    <updated>2019-12-29T03:58:39.143Z</updated>
    
    <content type="html"><![CDATA[<p>Java 找规律法<br><a id="more"></a><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">getRow</span><span class="params">(<span class="keyword">int</span> rowIndex)</span> </span>&#123;</span><br><span class="line">        List&lt;Integer&gt; res = <span class="keyword">new</span> ArrayList&lt;Integer&gt;();</span><br><span class="line">        <span class="keyword">long</span> k = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(rowIndex &gt;= <span class="number">0</span>)</span><br><span class="line">            res.add(<span class="number">1</span>);</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= rowIndex + <span class="number">1</span>; i++) &#123;</span><br><span class="line">            k = k * (rowIndex + <span class="number">2</span> - i) / (i-<span class="number">1</span>);</span><br><span class="line">            res.add((<span class="keyword">int</span>)k);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><br>这里用到了杨辉三角的规律，第n行m个数等于</p><p>譬如第三行第二个数</p><script type="math/tex; mode=display">C_{3-1}^{2-1} = C_{2}^{1} = 2</script><p>譬如第四行第三个数</p><script type="math/tex; mode=display">C_{4-1}^{3-1} = C_{3}^{2} = 3</script><p>那这个对我们的算法有啥帮助呢？</p><p>举个栗子，看第四行</p><p>应该是1 3 3 1</p><p>在本题中是1 4 6 4 1</p><p>$C_{5-1}^{1-1} = C_{4}^{0} = 1$，$C_{5-1}^{2-1} = C_{4}^{1} = 4$，$C_{5-1}^{3-1} = C_{4}^{2} = 6$，$C_{5-1}^{4-1} = C_{4}^{3} = 4$，$C_{5-1}^{5-1} = C_{4}^{4} = 1$</p><p>找规律如下：</p><p>第一个数：<script type="math/tex">C_{5-1}^{1-1} = C_{4}^{0} = 1</script></p><p>第二个数：<script type="math/tex">C_{5-1}^{2-1} = C_{4}^{1} = C_{5-1}^{1-1} * \frac{(rowIndex-2+2)}{2-1}</script></p><p>第n行m个数：第m-1个数 × $ \frac{(rowIndex-m+2)}{m-1} $，第n行第一个数永远是1</p><p>晚安~~~</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Java 找规律法&lt;br&gt;
    
    </summary>
    
    
      <category term="Leetcode" scheme="http://arithmeticjia.github.io/categories/Leetcode/"/>
    
    
      <category term="leetcode" scheme="http://arithmeticjia.github.io/tags/leetcode/"/>
    
      <category term="java" scheme="http://arithmeticjia.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode[78]Pascal&#39;s Triangle II</title>
    <link href="http://arithmeticjia.github.io/2019/12/28/Leetcode78Pascal-Triangle-2/"/>
    <id>http://arithmeticjia.github.io/2019/12/28/Leetcode78Pascal-Triangle-2/</id>
    <published>2019-12-28T14:05:49.000Z</published>
    <updated>2019-12-28T14:07:35.863Z</updated>
    
    <content type="html"><![CDATA[<p>python3 最优雅解法<br><a id="more"></a><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getRow</span><span class="params">(self, rowIndex)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type rowIndex: int</span></span><br><span class="line"><span class="string">        :rtype: List[int]</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        res = [<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, rowIndex + <span class="number">1</span>):</span><br><span class="line">            res.insert(<span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">            <span class="comment"># j循环每次算出r[0]...r[j-1]，再加上最后一个永远存在的1，正好是rowIndex+1个数</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">                res[j] = res[j] + res[j + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;python3 最优雅解法&lt;br&gt;
    
    </summary>
    
    
      <category term="Leetcode" scheme="http://arithmeticjia.github.io/categories/Leetcode/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="leetcode" scheme="http://arithmeticjia.github.io/tags/leetcode/"/>
    
  </entry>
  
  <entry>
    <title>2019-12-27周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2019/12/27/2019-12-27%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2019/12/27/2019-12-27%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2019-12-27T07:40:46.000Z</published>
    <updated>2020-01-10T09:53:38.616Z</updated>
    
    <content type="html"><![CDATA[<p>RNN -&gt; LSTM -&gt; GRU -&gt; Seq2Seq -&gt; Attention -&gt; Transformer<br><a id="more"></a></p><h3 id="Encoder-Decoder-Seq2Seq"><a href="#Encoder-Decoder-Seq2Seq" class="headerlink" title="Encoder-Decoder(Seq2Seq)"></a>Encoder-Decoder(Seq2Seq)</h3><p><img src="https://pic4.zhimg.com/80/v2-77e8a977fc3d43bec8b05633dc52ff9f_hd.jpg" alt=""></p><ul><li>Encoder-Decoder结构先将输入数据编码成一个上下文向量$c$</li><li>把Encoder的最后一个隐状态赋值给$c$,还可以对最后的隐状态做一个变换得到$c$，也可以对所有的隐状态做变换</li><li>拿到c之后，就用另一个RNN网络对其进行解码(Decoder),将c当做之前的初始状态$h_{0}$输入到Decoder中</li><li>还有一种做法是将$c$当做每一步的输入</li></ul><p><img src="https://pic4.zhimg.com/80/v2-e0fbb46d897400a384873fc100c442db_hd.jpg" alt=""></p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><ul><li>在Encoder-Decoder结构中，Encoder把所有的输入序列都编码成一个统一的语义特征$c$再解码，因此，$c$中必须包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈</li><li>Attention机制通过在每个时间输入不同的$c$来解决这个问题</li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/v2-8da16d429d33b0f2705e47af98e66579_hd_gaitubao_525x551_gaitubao_345x362.jpg" alt=""></p><ul><li>每一个$c$会自动去选取与当前所要输出的$y$最合适的上下文信息。具体来说，我们用$\alpha_{ij}$衡量Encoder中第$j$阶段的$h_{j}$和解码时第$i$阶段的相关性，最终Decoder中第$i$阶段的输入的上下文信息$c_{i}$就来自于所有$h_{j}$对$\alpha_{ij}$的加权和。</li><li>$\alpha_{ij}$和Decoder的第$i$阶段的隐藏状态、Encoder第$j$个阶段的隐藏状态有关</li><li>在Encoder的过程中保留每个RNN单元的隐藏状态(hidden state)得到($h_{1}$…$h_{N}$)，取$h_{j}$，表示Encoder层的隐层第$j$时刻的输出</li><li>在Decoder的过程中根据$x_{i}$和$h’_{i-1}$(这里和Encoder的$h_{i}$区分一下)得到$h’_{i}$，设为$s_{i}$</li><li>注：最开始的论文在Encoder-Decoder里面的当前Decoder的attention得分用的是$s_{i-1}$和$h_{j}$来算，但斯坦福教材上图上确实是画的$s_{i}$和$h_{j}$来算，而且后续论文大多是用的这种方式，即当前步的attention score用的当前步的隐藏状态$s_{i}$和前面的$h_{j}$去算的</li><li>通过Decoder的hidden states加上Encoder的hidden states来计算一个分数，用于计算权重<script type="math/tex; mode=display">e_{ij} = score(s_{i},h_{j})</script></li><li>注：这里有很多计算方式<script type="math/tex; mode=display">score(s_{i},h_{j}) = \left\{\begin{matrix}s^{T}_{i}h_{j}\\ s^{T}_{i}W_{a}h_{j}\\ v^{T}_{a}tanh(W_{a}[s^{T}_{i};h_{j}])\end{matrix}\right.</script></li><li>softmax权重归一化<script type="math/tex; mode=display">\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}</script></li><li>计算$c$<script type="math/tex; mode=display">c_{i} = \sum_{j=1}^{T_{x}}\alpha_{ij}h_{j}</script></li></ul><p><img src="https://pic4.zhimg.com/80/v2-8ddf993a95ee6e525fe2cd5ccd49bba7_hd.jpg" alt=""></p><p>(1)$h_{t} = RNN_{enc}(x_{t},h_{t-1})$, Encoder方面接受的是每一个单词word embedding，和上一个时间点的hidden state。输出的是这个时间点的hidden state。</p><p>(2)$s_{t} = RNN_{dnc}(y_{t},s_{t-1})$, Decoder方面接受的是目标句子里单词的word embedding，和上一个时间点的hidden state。</p><p>(3)$c_{i} = \sum_{j=1}^{T_{x}}\alpha _{ij}h_{j}$, context vector是一个对于encoder输出的hidden states的一个加权平均。</p><p>(4)$\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_{x}}exp(e_{ik})}$, 每一个encoder的hidden states对应的权重。</p><p>(5)$e_{ij} = score(s_{i},h_{j})$, 通过decoder的hidden states加上encoder的hidden states来计算一个分数，用于计算权重(4)</p><p>(6)$\hat{s}_{t}=tanh(W_{c}[c_{t};s_{t}])$, 将context vector 和 decoder的hidden states 串起来。</p><p>(7)$p(y_{t}|y_{&lt;t},x) = softmax(W_{s}\hat{s}_{t})$, 计算最后的输出概率。</p><h3 id="Transformer—-Attention-Is-All-You-Need"><a href="#Transformer—-Attention-Is-All-You-Need" class="headerlink" title="Transformer—-Attention Is All You Need"></a>Transformer—-Attention Is All You Need</h3><p><img src="https://pic1.zhimg.com/80/v2-4b53b731a961ee467928619d14a5fd44_hd.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/v2-4b53b731a961ee467928619d14a5fd44_r.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/4155986-208004e73fb93c97.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/4155986-e7fd5fcf3acc00a3.png" alt=""></p><ul><li>Transformer 的 Encoder 由 6 个编码器叠加组成，Decoder 也由 6 个解码器组成，在结构上都是相同的，但它们不共享权重。</li><li>Encoder的每一层有两个操作，分别是Self-Attention和Feed Forward；</li><li>Decoder的每一层有三个操作，分别是Self-Attention、Encoder-Decoder Attention以及Feed Forward操作。</li><li>这里的Self-Attention和Encoder-Decoder Attention都是用的是Multi-Head Attention机制</li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/v2-df2ca1b7a60d829245b7b7c37f80a3aa_r.jpg" alt=""></p><h4 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h4><ul><li>RNN的循环特性导致其不利于并行计算，模型训练时间较长</li><li>在传统的seq2seq中，我们通过RNN获取hidden state去做attention，那么当我们完全抛弃RNN的时候，怎么去做attention呢？</li><li>对每个input做embedding，代替hidden state，embedding通过三个不同的线性层生成$Q$，$K$，$V$。</li><li>Q: query;K: key; V: value</li><li>K = V = Q</li></ul><script type="math/tex; mode=display">Q = W_{Q}X</script><script type="math/tex; mode=display">K = W_{K}X</script><script type="math/tex; mode=display">V = W_{V}X</script><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix}*[v^{T}_{1},v^{T}_{2},...,v^{T}_{n}])*\begin{bmatrix}v_{1}\\ v_{2}\\ ...\\ v_{n}\end{bmatrix} = softmax(QK^{T})V</script><p>举个栗子</p><p><img src="https://pic1.zhimg.com/80/v2-087b831f622f83e4529c1bbf646530f0_hd.jpg" alt=""></p><ul><li>假如我们要翻译一个词组Thinking Machines，其中Thinking的输入的embedding vector用$x_{1}$表示，Machines的embedding vector用$x_{2}$表示</li><li>$W^{Q}$，$W^{K}$，$W^{V}$是我们模型训练过程学习到的合适的参数</li><li>$x$与$W^{Q}$，$W^{K}$，$W^{V}$相乘获得$q$，$k$，$v$</li><li>如上图中所示我们分别得到了$q_{1}$与$k_{1}$，$k_{2}$的点乘积，然后我们进行尺度缩放与softmax归一化</li></ul><h4 id="Scaled-Dot-Product-Attention-缩放了的点乘注意力"><a href="#Scaled-Dot-Product-Attention-缩放了的点乘注意力" class="headerlink" title="Scaled Dot-Product Attention(缩放了的点乘注意力)"></a>Scaled Dot-Product Attention(缩放了的点乘注意力)</h4><script type="math/tex; mode=display">Attention(Q,K,V) = sofrmax(\frac{QK^{T}}{\sqrt{d_{k}}})V</script><ul><li>输入包含$d_{k}$维的query和key，以及$d_{v}$维的value。通过计算query和各个key的点积，除以$\sqrt{d_{k}}$归一化，然后经过softmax激活变成权重，最后再乘value。点积注意力机制的优点是速度快、占用空间小。</li></ul><h4 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h4><ul><li>$Q$，$K$，$V$首先进过一个线性变换，然后输入到放缩点积attention</li><li>每次$Q$，$K$，$V$进行线性变换的参数$W$是不一样的</li><li>通过$h$个不同的线性变换对$Q$，$K$，$V$进行投影，最后将不同的attention结果拼接起来</li></ul><script type="math/tex; mode=display">Multihead(Q,K,V) = Concat(head_{1},...,head_{h})W^{O}</script><script type="math/tex; mode=display">head_{i} = Attention(QW^{Q}_{i},KW^{K}_{i},VW^{V}_{i})</script><h4 id="Position-wise-feed-forward-networks-位置全链接前馈网络"><a href="#Position-wise-feed-forward-networks-位置全链接前馈网络" class="headerlink" title="Position-wise feed-forward networks(位置全链接前馈网络)"></a>Position-wise feed-forward networks(位置全链接前馈网络)</h4><ul><li>由两个线性变换（Wx+b）和一个ReLU（relu的数学表达式就是f(x)=max(0,x)）<script type="math/tex; mode=display">FFN(x) = max(0,xW_{1} + b_{1})W_{2} + b_{2}</script></li></ul><h4 id="Positional-Encoding-位置编码"><a href="#Positional-Encoding-位置编码" class="headerlink" title="Positional Encoding(位置编码)"></a>Positional Encoding(位置编码)</h4><h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1115<span class="string">-1120</span> after data smoothing</span><br><span class="line">T = 10</span><br><span class="line">features = 70</span><br><span class="line">train = all * 0.7</span><br><span class="line"><span class="keyword">test </span>= all * 0.3</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-1115-1120-all.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-1115-1120-test-m.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/da-rnn-1115-1120-test.jpg" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 3.955</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.289</span><br></pre></td></tr></table></figure><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nasdaq100_padding</span><br><span class="line">T = 10</span><br><span class="line">features = 81</span><br><span class="line">train = all * 0.7</span><br><span class="line"><span class="keyword">test </span>= all * 0.3</span><br></pre></td></tr></table></figure><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">Encoder:</span> LSTM</span><br><span class="line"><span class="symbol">Decoder:</span> LSTM</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-lstm.jpg" alt=""><br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.579</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.105</span><br></pre></td></tr></table></figure></p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">Encoder:</span> BiLSTM</span><br><span class="line"><span class="symbol">Decoder:</span> LSTM</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-bi-lstm.jpg" alt=""><br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.384</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.069</span><br></pre></td></tr></table></figure></p><figure class="highlight avrasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">Encoder:</span> GRU</span><br><span class="line"><span class="symbol">Decoder:</span> LSTM</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/nasdaq-gru.jpg" alt=""><br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 0.252</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.046</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;RNN -&amp;gt; LSTM -&amp;gt; GRU -&amp;gt; Seq2Seq -&amp;gt; Attention -&amp;gt; Transformer&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>Leetcode38Count-and-Say</title>
    <link href="http://arithmeticjia.github.io/2019/12/24/Leetcode38Count-and-Say/"/>
    <id>http://arithmeticjia.github.io/2019/12/24/Leetcode38Count-and-Say/</id>
    <published>2019-12-24T08:01:01.000Z</published>
    <updated>2019-12-24T08:02:18.323Z</updated>
    
    <content type="html"><![CDATA[<p>Nothing<br><a id="more"></a><br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">countAndSay</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(n == <span class="number">1</span>)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="string">"1"</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        String str = countAndSay(n-<span class="number">1</span>) + <span class="string">"*"</span>;<span class="comment">// 这样末尾的数才能被循环处理到</span></span><br><span class="line">        <span class="keyword">char</span>[] str_c = str.toCharArray();</span><br><span class="line">        <span class="keyword">int</span> count = <span class="number">1</span>;</span><br><span class="line">        StringBuilder temp = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">        <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">while</span> (i &lt; str_c.length-<span class="number">1</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span>(str_c[i] == str_c[i+<span class="number">1</span>])&#123;</span><br><span class="line">                count++;  <span class="comment">//遇到相同的计数器加</span></span><br><span class="line">                i++;</span><br><span class="line">            &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                temp.append(Integer.toString(count)+ str_c[i]);</span><br><span class="line">                <span class="comment">// 遇到不同的，先append计数器的值，再append最后一个相同的值</span></span><br><span class="line">                <span class="comment">// temp.append("" + count + str_c[i]);</span></span><br><span class="line">                count = <span class="number">1</span>;</span><br><span class="line">                i++;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> temp.toString();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Nothing&lt;br&gt;
    
    </summary>
    
    
      <category term="Leetcode" scheme="http://arithmeticjia.github.io/categories/Leetcode/"/>
    
    
      <category term="leetcode" scheme="http://arithmeticjia.github.io/tags/leetcode/"/>
    
      <category term="java" scheme="http://arithmeticjia.github.io/tags/java/"/>
    
  </entry>
  
  <entry>
    <title>Seq2seq模型及注意力机制模型</title>
    <link href="http://arithmeticjia.github.io/2019/12/22/Seq2seq%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A8%A1%E5%9E%8B/"/>
    <id>http://arithmeticjia.github.io/2019/12/22/Seq2seq%E6%A8%A1%E5%9E%8B%E5%8F%8A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E6%A8%A1%E5%9E%8B/</id>
    <published>2019-12-22T07:21:43.000Z</published>
    <updated>2019-12-24T10:48:12.961Z</updated>
    
    <content type="html"><![CDATA[<p>对于处理输出序列为不定长情况的问题，例如机器翻译，例如英文到法语的句子翻译，输入和输出均为不定长。前人提出了seq2seq模型，basic idea是设计一个encoder与decoder，其中encoder将输入序列编码为一个包含输入序列所有信息的context vector $ c $，decoder通过对$ c $的解码获得输入序列的信息，从而得到输出序列。encoder及decoder都通常为RNN循环神经网络<br><a id="more"></a></p><h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><ul><li>input: 当前时刻输入值$x_{t}$,上一时刻LSTM的输出值$h_{t-1}$,上一时刻的单元状态$c_{t-1}$</li><li>output: 当前时刻LSTM的输出值$h_{t}$,当前时刻的单元状$c_{t}$</li><li>forget gate:</li></ul><script type="math/tex; mode=display">f_{t} = \sigma (W_{f}[h_{t-1};x_{t}]+b_{f})</script><p>$W_{f}$是遗忘门的权重矩阵，$[h_{t-1};x_{t}]$表示把两个向量连接成一个更长的向量，$b_{f}$是遗忘门的偏置项，$\sigma$是sigmoid函数<br>如果输入的维度是$d_{x}$，隐藏层的维度是$d_{h}$，单元状态的维度是$d_{c}$（通常$d_{c} = d_{h}$），则遗忘门的权重矩阵$W_{f}$的维度是$d_{c}×(d_{h}+d_{x})$</p><ul><li><p>input gate:</p><script type="math/tex; mode=display">i_{t} = \sigma (W_{i}[h_{t-1};x_{t}]+b_{i})</script></li><li><p>output gate:</p><script type="math/tex; mode=display">o_{t} = \sigma (W_{o}[h_{t-1};x_{t}]+b_{o})</script></li><li><p>final out:</p><script type="math/tex; mode=display">\tilde{c}_{t}= tanh(W_{c}[h_{t-1};x_{t}]+b_{c})</script><script type="math/tex; mode=display">c_{t} = f_{t} * c_{t-1} + i_{t} * \tilde{c}_{t}</script><script type="math/tex; mode=display">h_{t} = o_{t} * tanh(c_{t})</script></li><li><p>前向计算每个神经元的输出值，对于LSTM来说就是$f_{t}$,$i_{t}$,$c_{t}$,$o_{t}$,$h_{t}$ 5个向量的值</p></li><li>反向计算每个神经元的误差项$\delta$，包括两个方向，一是沿时间的反向传播，即从当前t时刻开始，计算每个时刻的误差项；另一个是将误差项向上一层传播</li><li>根据相应的误差项，计算每个权重的梯度</li><li>sigmoid</li></ul><script type="math/tex; mode=display">\delta (x) = \frac{1}{1+e^{-x}}</script><script type="math/tex; mode=display">\delta^{'} (x) = \frac{e^{-x}}{(1+e^{-x})^{2}}=\delta(x)(1-\delta(x))</script><ul><li>tanh</li></ul><script type="math/tex; mode=display">tanh(x) = \frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}</script><script type="math/tex; mode=display">tanh^{'}(x) = 1 - tanh^{2}(x)</script><p>LSTM需要学习的参数共有8组，分别是：</p><ul><li>遗忘门的权重矩阵$W_{f}$和偏置项$b_{f}$</li><li>输入门的权重矩阵$W_{i}$和偏置项$b_{i}$</li><li>输出门的权重矩阵$W_{o}$和偏置项$b_{o}$</li><li>计算单元状态的权重矩阵$W_{c}$和偏置项$b_{c}$</li></ul><h4 id="seq2seq模型"><a href="#seq2seq模型" class="headerlink" title="seq2seq模型"></a>seq2seq模型</h4><h5 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h5><p>编码器的作用是把一个不定长的输入序列$ x_{1},x_{2},…,x_{T} $转化成一个定长的context vector $c$. 该context vector编码了输入序列$ x_{1},x_{2},…,x_{T} $的序列。回忆一下循环神经网络，假设该循环神经网络单元为$f$（可以为vanilla RNN, LSTM, GRU)，那么hidden state为</p><script type="math/tex; mode=display">h_{t} = f(x_{t},h_{t-1})</script><p>编码器的context vector是所有时刻hidden state的函数，即：</p><script type="math/tex; mode=display">c=q(h_{1},...,h_{T})</script><p>简单地，我们可以把最终时刻的hidden state[公式]作为context vecter。当然我们也可以取各个时刻hidden states的平均，以及其他方法。</p><h5 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h5><p>编码器最终输出一个context vector $c$，该context vector编码了输入序列$ x_{1},x_{2},…,x_{T} $的信息。</p><p>假设训练数据中的输出序列为$y_{1}y_{2},…,y_{T}^{‘}$,我们希望每个$t$时刻的输出即取决于之前的输出也取决于context vector，即估计$P(y_{t’}|y_{1},…,y_{t’-1},c)$，从而得到输出序列的联合概率分布：</p><script type="math/tex; mode=display">P(y_{1},...,y_{T'})=\prod_{t'-1}^{T'}P(y_{t'}|y_{1},...,y_{t'-1},c)</script><p>并定义该序列的损失函数loss function</p><script type="math/tex; mode=display">-\log P(y_{1},...,y_{T'})</script><p>通过最小化损失函数来训练seq2seq模型。</p><p>那么如何估计$ P(y_{t’}|y_{1},…,y_{t’-1},c) $？</p><p>我们使用另一个循环神经网络作为解码器。解码器使用函数$p$来表示$t’$时刻输出$y_{t’}$的概率</p><script type="math/tex; mode=display">P(y_{t'}|y_{1},...,y_{t'-1},c) = p(y_{t'-1},s_{t'},c)</script><p>为了区分编码器中的hidden state[公式]，其中[公式]为[公式]时刻解码器的hidden state。区别于编码器，解码器中的循环神经网络的输入除了前一个时刻的输出序列[公式]，和前一个时刻的hidden state[公式]以外，还包含了context vector[公式]。即：</p><script type="math/tex; mode=display">s_{t'} = g(y_{t'-1},s_{t'-1},c)</script><p>其中函数g为解码器的循环神经网络单元。</p><h4 id="DA-RNN"><a href="#DA-RNN" class="headerlink" title="DA-RNN"></a>DA-RNN</h4><h5 id="第一阶段，使用注意力机制自适应地提取每个时刻的相关feature"><a href="#第一阶段，使用注意力机制自适应地提取每个时刻的相关feature" class="headerlink" title="第一阶段，使用注意力机制自适应地提取每个时刻的相关feature"></a>第一阶段，使用注意力机制自适应地提取每个时刻的相关feature</h5><script type="math/tex; mode=display">e_{t}^{k}=v_{e}^{T}tanh(W_{e}[h_{t-1};c_{t-1}]+U_{e}x^{k})</script><ul><li>用softmax函数将其归一化<script type="math/tex; mode=display">\alpha _{t}^{k}=\frac{exp(e_{t}^{k})}{\sum_{i-1}^{n}exp(e_{t}^{i})}</script></li><li>得到更新后的x<script type="math/tex; mode=display">\tilde{x} = (\alpha _{t}^{1}x_{t}^{1}, \alpha _{t}^{2}x_{t}^{2},...,\alpha _{t}^{n}x_{t}^{n})</script></li></ul><p><img src="https://www.guanacossj.com/media/articlebodypics/lstm.jpg" alt=""></p><ul><li><p>选取LSTM作为编码器<script type="math/tex">f_{1}</script></p><script type="math/tex; mode=display">h_{t} = f_{1}(h_{t-1},  \tilde{x})</script></li><li><p>Encoder方面接受的是每一个输入，和上一个时间点的隐藏态。输出的是当前时间点的隐藏态</p></li></ul><h5 id="第二阶段，使用另一个注意力机制选取与之相关的encoder-hidden-states"><a href="#第二阶段，使用另一个注意力机制选取与之相关的encoder-hidden-states" class="headerlink" title="第二阶段，使用另一个注意力机制选取与之相关的encoder hidden states"></a>第二阶段，使用另一个注意力机制选取与之相关的encoder hidden states</h5><ul><li><p>Decoder方面接受的是目标输入，和上一个时间点的隐藏态</p></li><li><p>对所有时刻的$h_{t’}$取加权平均，即：</p></li></ul><script type="math/tex; mode=display">c_{t}^{'} = \sum_{t-1}^{T}\beta _{t^{'}}^{t}h_{t}</script><ul><li><script type="math/tex">\beta _{t^{'}}^{t}</script>的设计类似于Bahanau的工作，基于前一个时刻解码器的hidden state $ d_{t’-1} $和cell state$s_{t’-1}^{‘}$计算得到：</li></ul><script type="math/tex; mode=display">l_{t}^{t}=v_{d}^{T}tanh(W_{d}[d_{t-1};s_{t-1}^{'}]+U_{d}h_{t})</script><script type="math/tex; mode=display">\beta _{t}^{i}=\frac{exp(l_{t}^{i})}{\sum_{j=1}^{T}exp(l_{t}^{j})}</script><script type="math/tex; mode=display">c_{t}=\sum_{i=1}^{T}\beta _{t}^{i}h_{i}</script><ul><li>解码器的输入是上一个时刻的目标序列$y_{t’-1}$和hidden state$d_{t’-1}$以及context vector $c_{t’-1}$，即<script type="math/tex; mode=display">d_{t'}=f_{2}(y_{t'-1},c_{t'-1},d_{t'-1})</script></li><li>这里设计了$\tilde{y}_{t’-1}$来combie$y_{t’-1}$与$c_{t’-1}$的信息，即<script type="math/tex; mode=display">\tilde{y}_{t'-1} = \tilde{\omega }^{T}[y_{t'-1};c_{t'-1}]+\tilde{b}</script></li><li>然后<script type="math/tex; mode=display">d_{t}=f_{2}(d_{t-1},\tilde{y}_{t-1})</script></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;对于处理输出序列为不定长情况的问题，例如机器翻译，例如英文到法语的句子翻译，输入和输出均为不定长。前人提出了seq2seq模型，basic idea是设计一个encoder与decoder，其中encoder将输入序列编码为一个包含输入序列所有信息的context vector $ c $，decoder通过对$ c $的解码获得输入序列的信息，从而得到输出序列。encoder及decoder都通常为RNN循环神经网络&lt;br&gt;
    
    </summary>
    
    
    
      <category term="seq2seq" scheme="http://arithmeticjia.github.io/tags/seq2seq/"/>
    
      <category term="attention" scheme="http://arithmeticjia.github.io/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>da-rnn-bug-fix</title>
    <link href="http://arithmeticjia.github.io/2019/12/21/da-rnn-bug-fix/"/>
    <id>http://arithmeticjia.github.io/2019/12/21/da-rnn-bug-fix/</id>
    <published>2019-12-21T14:19:27.000Z</published>
    <updated>2019-12-23T08:00:10.477Z</updated>
    
    <content type="html"><![CDATA[<p>Bugs fix for<br><a href="https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py" target="_blank" rel="noopener" title="https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py">https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py</a><br><a id="more"></a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> unicode_literals, print_function, division</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> open</span><br><span class="line"><span class="keyword">import</span> unicodedata</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> concatenate</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">filename = <span class="string">'nasdaq100_padding.csv'</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">dataset = pd.read_csv(filename)</span><br><span class="line"><span class="comment"># print(dataset.values)</span></span><br><span class="line"></span><br><span class="line">features = dataset.values.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 82</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EncoderAtt</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, T)</span>:</span></span><br><span class="line">        <span class="comment"># input size: number of underlying factors (81)</span></span><br><span class="line">        <span class="comment"># T: number of time steps (10)</span></span><br><span class="line">        <span class="comment"># hidden_size: dimension of the hidden state</span></span><br><span class="line">        super(EncoderAtt, self).__init__()</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.T = T</span><br><span class="line"></span><br><span class="line">        self.lstm_layer = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=<span class="number">1</span>)</span><br><span class="line">        self.attn_linear = nn.Linear(in_features=<span class="number">2</span> * hidden_size + T - <span class="number">1</span>, out_features=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_data)</span>:</span></span><br><span class="line">        <span class="comment"># input_data: batch_size * T - 1 * input_size</span></span><br><span class="line">        input_weighted = Variable(input_data.data.new(input_data.size(<span class="number">0</span>), self.T - <span class="number">1</span>, self.input_size).zero_())</span><br><span class="line">        input_encoded = Variable(input_data.data.new(input_data.size(<span class="number">0</span>), self.T - <span class="number">1</span>, self.hidden_size).zero_())</span><br><span class="line">        <span class="comment"># hidden, cell: initial states with dimention hidden_size</span></span><br><span class="line">        hidden = self.init_hidden(input_data) <span class="comment"># 1 * batch_size * hidden_size</span></span><br><span class="line">        cell = self.init_hidden(input_data)</span><br><span class="line">        <span class="comment"># hidden.requires_grad = False</span></span><br><span class="line">        <span class="comment"># cell.requires_grad = False</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(self.T - <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># Eqn. 8: concatenate the hidden states with each predictor</span></span><br><span class="line">            x = torch.cat((hidden.repeat(self.input_size, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>),</span><br><span class="line">                           cell.repeat(self.input_size, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>),</span><br><span class="line">                           input_data.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)), dim = <span class="number">2</span>) <span class="comment"># batch_size * input_size * (2*hidden_size + T - 1)</span></span><br><span class="line">            <span class="comment"># Eqn. 9: Get attention weights</span></span><br><span class="line">            x = self.attn_linear(x.view(<span class="number">-1</span>, self.hidden_size * <span class="number">2</span> + self.T - <span class="number">1</span>)) <span class="comment"># (batch_size * input_size) * 1</span></span><br><span class="line">            attn_weights = F.softmax(x.view(<span class="number">-1</span>, self.input_size)) <span class="comment"># batch_size * input_size, attn weights with values sum up to 1.</span></span><br><span class="line">            <span class="comment"># Eqn. 10: LSTM</span></span><br><span class="line">            weighted_input = torch.mul(attn_weights, input_data[:, t, :]) <span class="comment"># batch_size * input_size</span></span><br><span class="line">            <span class="comment"># Fix the warning about non-contiguous memory</span></span><br><span class="line">            <span class="comment"># see https://discuss.pytorch.org/t/dataparallel-issue-with-flatten-parameter/8282</span></span><br><span class="line">            self.lstm_layer.flatten_parameters()</span><br><span class="line">            _, lstm_states = self.lstm_layer(weighted_input.unsqueeze(<span class="number">0</span>), (hidden, cell))</span><br><span class="line">            hidden = lstm_states[<span class="number">0</span>]</span><br><span class="line">            cell = lstm_states[<span class="number">1</span>]</span><br><span class="line">            <span class="comment"># Save output</span></span><br><span class="line">            input_weighted[:, t, :] = weighted_input</span><br><span class="line">            input_encoded[:, t, :] = hidden</span><br><span class="line">        <span class="keyword">return</span> input_weighted, input_encoded</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># No matter whether CUDA is used, the returned variable will have the same type as x.</span></span><br><span class="line">        <span class="keyword">return</span> Variable(x.data.new(<span class="number">1</span>, x.size(<span class="number">0</span>), self.hidden_size).zero_()) <span class="comment"># dimension 0 is the batch dimension</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DecoderAtt</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder_hidden_size, decoder_hidden_size, T)</span>:</span></span><br><span class="line">        super(DecoderAtt, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.T = T</span><br><span class="line">        self.encoder_hidden_size = encoder_hidden_size</span><br><span class="line">        self.decoder_hidden_size = decoder_hidden_size</span><br><span class="line"></span><br><span class="line">        self.attn_layer = nn.Sequential(nn.Linear(<span class="number">2</span> * decoder_hidden_size + encoder_hidden_size, encoder_hidden_size),</span><br><span class="line">                                        nn.Tanh(), nn.Linear(encoder_hidden_size, <span class="number">1</span>))</span><br><span class="line">        self.lstm_layer = nn.LSTM(input_size=<span class="number">1</span>, hidden_size=decoder_hidden_size)</span><br><span class="line">        self.fc = nn.Linear(encoder_hidden_size + <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc_final = nn.Linear(decoder_hidden_size + encoder_hidden_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        self.fc.weight.data.normal_()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_encoded, y_history)</span>:</span></span><br><span class="line">        <span class="comment"># input_encoded: batch_size * T - 1 * encoder_hidden_size</span></span><br><span class="line">        <span class="comment"># y_history: batch_size * (T-1)</span></span><br><span class="line">        <span class="comment"># Initialize hidden and cell, 1 * batch_size * decoder_hidden_size</span></span><br><span class="line">        hidden = self.init_hidden(input_encoded)</span><br><span class="line">        cell = self.init_hidden(input_encoded)</span><br><span class="line">        <span class="comment"># hidden.requires_grad = False</span></span><br><span class="line">        <span class="comment"># cell.requires_grad = False</span></span><br><span class="line">        <span class="keyword">for</span> t <span class="keyword">in</span> range(self.T - <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># Eqn. 12-13: compute attention weights</span></span><br><span class="line">            <span class="comment">## batch_size * T * (2*decoder_hidden_size + encoder_hidden_size)</span></span><br><span class="line">            x = torch.cat((hidden.repeat(self.T - <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>),</span><br><span class="line">                           cell.repeat(self.T - <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>).permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>), input_encoded), dim=<span class="number">2</span>)</span><br><span class="line">            x = F.softmax(self.attn_layer(x.view(<span class="number">-1</span>, <span class="number">2</span> * self.decoder_hidden_size + self.encoder_hidden_size</span><br><span class="line">                                                 )).view(<span class="number">-1</span>, self.T - <span class="number">1</span>))  <span class="comment"># batch_size * T - 1, row sum up to 1</span></span><br><span class="line">            <span class="comment"># Eqn. 14: compute context vector</span></span><br><span class="line">            context = torch.bmm(x.unsqueeze(<span class="number">1</span>), input_encoded)[:, <span class="number">0</span>, :]  <span class="comment"># batch_size * encoder_hidden_size</span></span><br><span class="line">            <span class="keyword">if</span> t &lt; self.T - <span class="number">1</span>:</span><br><span class="line">                <span class="comment"># Eqn. 15</span></span><br><span class="line">                y_tilde = self.fc(torch.cat((context, y_history[:, t].unsqueeze(<span class="number">1</span>)), dim=<span class="number">1</span>))  <span class="comment"># batch_size * 1</span></span><br><span class="line">                <span class="comment"># Eqn. 16: LSTM</span></span><br><span class="line">                self.lstm_layer.flatten_parameters()</span><br><span class="line">                _, lstm_output = self.lstm_layer(y_tilde.unsqueeze(<span class="number">0</span>), (hidden, cell))</span><br><span class="line">                hidden = lstm_output[<span class="number">0</span>]  <span class="comment"># 1 * batch_size * decoder_hidden_size</span></span><br><span class="line">                cell = lstm_output[<span class="number">1</span>]  <span class="comment"># 1 * batch_size * decoder_hidden_size</span></span><br><span class="line">        <span class="comment"># Eqn. 22: final output</span></span><br><span class="line">        y_pred = self.fc_final(torch.cat((hidden[<span class="number">0</span>], context), dim=<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># self.logger.info("hidden %s context %s y_pred: %s", hidden[0][0][:10], context[0][:10], y_pred[:10])</span></span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> Variable(x.data.new(<span class="number">1</span>, x.size(<span class="number">0</span>), self.decoder_hidden_size).zero_())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_data</span><span class="params">(dat, col_names)</span>:</span></span><br><span class="line">    scale = StandardScaler().fit(dat)</span><br><span class="line">    proc_dat = scale.transform(dat)</span><br><span class="line"></span><br><span class="line">    mask = np.ones(proc_dat.shape[<span class="number">1</span>], dtype=bool)</span><br><span class="line">    dat_cols = list(dat.columns)</span><br><span class="line">    <span class="keyword">for</span> col_name <span class="keyword">in</span> col_names:</span><br><span class="line">        mask[dat_cols.index(col_name)] = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    feats = proc_dat[:, mask]</span><br><span class="line">    targs = proc_dat[:, ~mask]</span><br><span class="line">    <span class="keyword">return</span> feats, targs, scale</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">da_rnn</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, file_data, encoder_hidden_size=<span class="number">64</span>, decoder_hidden_size=<span class="number">64</span>, T=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 learning_rate=<span class="number">0.01</span>, batch_size=<span class="number">128</span>, parallel=True, debug=False)</span>:</span></span><br><span class="line">        self.T = T</span><br><span class="line">        dat = pd.read_csv(file_data, nrows=<span class="number">100</span> <span class="keyword">if</span> debug <span class="keyword">else</span> <span class="literal">None</span>)</span><br><span class="line">        <span class="comment"># read first 100 rows</span></span><br><span class="line">        <span class="comment"># self.logger.info("Shape of data: %s.\nMissing in data: %s.", dat.shape, dat.isnull().sum().sum())</span></span><br><span class="line">        <span class="comment"># scale = StandardScaler().fit(dat.values)</span></span><br><span class="line">        <span class="comment"># dat = pd.DataFrame(scale.transform(dat.values))</span></span><br><span class="line">        <span class="comment"># self.X = dat.loc[:, [x for x in dat.columns.tolist() if x != 'NDX']].as_matrix()</span></span><br><span class="line">        self.X, self.y, self.scaler = preprocess_data(dat, (<span class="string">"NDX"</span>,))</span><br><span class="line">        <span class="comment"># select matrix without NDX</span></span><br><span class="line">        <span class="comment"># (ndarray:(40560,81))</span></span><br><span class="line">        self.y = (self.y).reshape((self.y).shape[<span class="number">0</span>],)</span><br><span class="line">        <span class="comment"># self.y = np.array(dat.NDX)</span></span><br><span class="line">        <span class="comment"># (ndarray:(40560,))</span></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        <span class="comment"># 128</span></span><br><span class="line">        self.encoder = EncoderAtt(input_size=self.X.shape[<span class="number">1</span>], hidden_size=encoder_hidden_size, T=T).to(device)</span><br><span class="line">        self.decoder = DecoderAtt(encoder_hidden_size=encoder_hidden_size, decoder_hidden_size=decoder_hidden_size, T=T).to(device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> parallel:</span><br><span class="line">            self.encoder = nn.DataParallel(self.encoder)</span><br><span class="line">            self.decoder = nn.DataParallel(self.decoder)</span><br><span class="line">        <span class="comment">#  multiple GPU training</span></span><br><span class="line"></span><br><span class="line">        self.encoder_optimizer = optim.Adam(params=filter(<span class="keyword">lambda</span> p: p.requires_grad, self.encoder.parameters()),</span><br><span class="line">                                           lr=learning_rate)</span><br><span class="line">        self.decoder_optimizer = optim.Adam(params=filter(<span class="keyword">lambda</span> p: p.requires_grad, self.decoder.parameters()),</span><br><span class="line">                                           lr=learning_rate)</span><br><span class="line">        <span class="comment"># self.learning_rate = learning_rate</span></span><br><span class="line"></span><br><span class="line">        self.train_size = int(self.X.shape[<span class="number">0</span>] * <span class="number">0.7</span>)</span><br><span class="line">        <span class="comment"># &#123;int&#125; 28392</span></span><br><span class="line">        <span class="comment"># self.y = self.y - np.mean(self.y[:self.train_size])</span></span><br><span class="line">        <span class="comment"># self.y = (self.y - np.mean(self.y[:self.train_size])) / np.std(self.y[:self.train_size])</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Question: why Adam requires data to be normalized?</span></span><br><span class="line">        <span class="comment"># self.logger.info("Training size: %d.", self.train_size)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, n_epochs=<span class="number">10</span>)</span>:</span></span><br><span class="line">        iter_per_epoch = int(np.ceil(self.train_size * <span class="number">1.</span> / self.batch_size))</span><br><span class="line">        print(<span class="string">"Iterations per epoch: %3.3f ~ %d."</span>, self.train_size * <span class="number">1.</span> / self.batch_size, iter_per_epoch)</span><br><span class="line">        self.iter_losses = np.zeros(n_epochs * iter_per_epoch)</span><br><span class="line">        self.epoch_losses = np.zeros(n_epochs)</span><br><span class="line"></span><br><span class="line">        self.loss_func = nn.MSELoss()</span><br><span class="line"></span><br><span class="line">        n_iter = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        learning_rate = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n_epochs):</span><br><span class="line">            perm_idx = np.random.permutation(self.train_size - self.T)</span><br><span class="line">            j = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> j &lt; self.train_size:</span><br><span class="line">                batch_idx = perm_idx[j:(j + self.batch_size)]</span><br><span class="line">                X = np.zeros((len(batch_idx), self.T - <span class="number">1</span>, self.X.shape[<span class="number">1</span>]))</span><br><span class="line">                y_history = np.zeros((len(batch_idx), self.T - <span class="number">1</span>))</span><br><span class="line">                y_target = self.y[batch_idx + self.T]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">for</span> k <span class="keyword">in</span> range(len(batch_idx)):</span><br><span class="line">                    X[k, :, :] = self.X[batch_idx[k] : (batch_idx[k] + self.T - <span class="number">1</span>), :]</span><br><span class="line">                    y_history[k, :] = self.y[batch_idx[k]: (batch_idx[k] + self.T - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">                loss = self.train_iteration(X, y_history, y_target)</span><br><span class="line">                self.iter_losses[int(i * iter_per_epoch + j / self.batch_size)] = loss</span><br><span class="line">                <span class="comment">#if (j / self.batch_size) % 50 == 0:</span></span><br><span class="line">                <span class="comment">#    self.logger.info("Epoch %d, Batch %d: loss = %3.3f.", i, j / self.batch_size, loss)</span></span><br><span class="line">                j += self.batch_size</span><br><span class="line">                n_iter += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> n_iter % <span class="number">10000</span> == <span class="number">0</span> <span class="keyword">and</span> n_iter &gt; <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">for</span> param_group <span class="keyword">in</span> self.encoder_optimizer.param_groups:</span><br><span class="line">                        param_group[<span class="string">'lr'</span>] = param_group[<span class="string">'lr'</span>] * <span class="number">0.9</span></span><br><span class="line">                    <span class="keyword">for</span> param_group <span class="keyword">in</span> self.decoder_optimizer.param_groups:</span><br><span class="line">                        param_group[<span class="string">'lr'</span>] = param_group[<span class="string">'lr'</span>] * <span class="number">0.9</span></span><br><span class="line"></span><br><span class="line">            self.epoch_losses[i] = np.mean(self.iter_losses[range(i * iter_per_epoch, (i + <span class="number">1</span>) * iter_per_epoch)])</span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Epoch %d, loss: %3.3f."</span> % (i, self.epoch_losses[i]))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                y_train_pred = self.predict(on_train=<span class="literal">True</span>)  <span class="comment"># 28383</span></span><br><span class="line">                y_test_pred = self.predict(on_train=<span class="literal">False</span>)  <span class="comment"># 12168</span></span><br><span class="line">                y_pred = np.concatenate((y_train_pred, y_test_pred))    <span class="comment"># 40551</span></span><br><span class="line">                <span class="comment"># X (40560,)</span></span><br><span class="line">                <span class="comment"># y (40560,)</span></span><br><span class="line">                print(y_train_pred.shape, y_test_pred.shape, y_pred.shape)</span><br><span class="line">                print((self.y).shape,(self.X).shape)</span><br><span class="line">                <span class="comment"># (40560,) (40560, 81)</span></span><br><span class="line">                true = concatenate(((self.y).reshape(self.y.shape[<span class="number">0</span>], <span class="number">1</span>), self.X), axis=<span class="number">1</span>)</span><br><span class="line">                true = self.scaler.inverse_transform(true)</span><br><span class="line">                self.y = true[:, <span class="number">0</span>]</span><br><span class="line">                <span class="comment"># true [1,40560] len = 40560</span></span><br><span class="line">                print(self.T, len(y_train_pred) + self.T)</span><br><span class="line">                <span class="comment"># 10 28393</span></span><br><span class="line">                print(self.T + len(y_train_pred), len(self.y) + <span class="number">1</span>)</span><br><span class="line">                <span class="comment"># 28393 40561</span></span><br><span class="line">                <span class="comment"># y_train_pred = concatenate((y_train_pred.reshape(y_train_pred.shape[0], 1), self.X[self.T-1: len(y_train_pred) + self.T-1]), axis=1)</span></span><br><span class="line">                y_train_pred = concatenate((y_train_pred.reshape(y_train_pred.shape[<span class="number">0</span>], <span class="number">1</span>),</span><br><span class="line">                                            self.X[: len(y_train_pred)]), axis=<span class="number">1</span>)</span><br><span class="line">                y_train_pred = self.scaler.inverse_transform(y_train_pred)</span><br><span class="line">                y_train_pred = y_train_pred[:, <span class="number">0</span>]</span><br><span class="line">                <span class="comment"># y_train_pred [10,28392] len = 28383</span></span><br><span class="line">                <span class="comment"># y_test_pred = concatenate((y_test_pred.reshape(y_test_pred.shape[0], 1), self.X[self.T + len(y_train_pred)-1:]), axis=1)</span></span><br><span class="line">                y_test_pred = concatenate(</span><br><span class="line">                    (y_test_pred.reshape(y_test_pred.shape[<span class="number">0</span>], <span class="number">1</span>), self.X[len(y_train_pred):len(y_train_pred)+len(y_test_pred)]), axis=<span class="number">1</span>)</span><br><span class="line">                y_test_pred = self.scaler.inverse_transform(y_test_pred)</span><br><span class="line">                y_test_pred = y_test_pred[:, <span class="number">0</span>]</span><br><span class="line">                <span class="comment"># y_test_pred [28393,40560] len = 12168</span></span><br><span class="line">                plt.figure()</span><br><span class="line">                plt.plot(range(<span class="number">1</span>, <span class="number">1</span> + len(self.y)), self.y, label=<span class="string">"True"</span>)</span><br><span class="line">                plt.plot(range(self.T, len(y_train_pred) + self.T), y_train_pred, label = <span class="string">'Predicted - Train'</span>)</span><br><span class="line">                plt.plot(range(self.T + len(y_train_pred), len(self.y) + <span class="number">1</span>), y_test_pred, label = <span class="string">'Predicted - Test'</span>)</span><br><span class="line">                plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">                plt.savefig(<span class="string">'./resultpic/epoch_%d.jpg'</span> % i)</span><br><span class="line">                plt.show()</span><br><span class="line"></span><br><span class="line">        y_train_pred = self.predict(on_train=<span class="literal">True</span>)</span><br><span class="line">        y_test_pred = self.predict(on_train=<span class="literal">False</span>)</span><br><span class="line">        y_pred = np.concatenate((y_train_pred, y_test_pred))</span><br><span class="line">        plt.figure()</span><br><span class="line">        plt.plot(range(<span class="number">1</span>, <span class="number">1</span> + len(self.y)), self.y, label=<span class="string">"True"</span>)</span><br><span class="line">        plt.plot(range(self.T, len(y_train_pred) + self.T), y_train_pred, label=<span class="string">'Predicted - Train'</span>)</span><br><span class="line">        plt.plot(range(self.T + len(y_train_pred), len(self.y) + <span class="number">1</span>), y_test_pred, label=<span class="string">'Predicted - Test'</span>)</span><br><span class="line">        plt.legend(loc=<span class="string">'upper left'</span>)</span><br><span class="line">        plt.savefig(<span class="string">'./resultpic/final.jpg'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train_iteration</span><span class="params">(self, X, y_history, y_target)</span>:</span></span><br><span class="line">        self.encoder_optimizer.zero_grad()</span><br><span class="line">        self.decoder_optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        input_weighted, input_encoded = self.encoder(Variable(torch.from_numpy(X).type(torch.FloatTensor).to(device)))</span><br><span class="line">        y_pred = self.decoder(input_encoded, Variable(torch.from_numpy(y_history).type(torch.FloatTensor).to(device)))</span><br><span class="line">        y_pred = y_pred.view(<span class="number">-1</span>)</span><br><span class="line">        <span class="comment"># print('y_pred', y_pred.shape)</span></span><br><span class="line">        y_true = Variable(torch.from_numpy(y_target).type(torch.FloatTensor).to(device))</span><br><span class="line">        <span class="comment"># print('y_true', y_true.shape)</span></span><br><span class="line">        loss = self.loss_func(y_pred, y_true)</span><br><span class="line">        loss.backward()</span><br><span class="line"></span><br><span class="line">        self.encoder_optimizer.step()</span><br><span class="line">        self.decoder_optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, on_train = False)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> on_train:</span><br><span class="line">            y_pred = np.zeros(self.train_size - self.T + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_pred = np.zeros(self.X.shape[<span class="number">0</span>] - self.train_size)</span><br><span class="line"></span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; len(y_pred):</span><br><span class="line">            batch_idx = np.array(range(len(y_pred)))[i : (i + self.batch_size)]</span><br><span class="line">            X = np.zeros((len(batch_idx), self.T - <span class="number">1</span>, self.X.shape[<span class="number">1</span>]))</span><br><span class="line">            y_history = np.zeros((len(batch_idx), self.T - <span class="number">1</span>))</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(batch_idx)):</span><br><span class="line">                <span class="keyword">if</span> on_train:</span><br><span class="line">                    X[j, :, :] = self.X[range(batch_idx[j], batch_idx[j] + self.T - <span class="number">1</span>), :]</span><br><span class="line">                    y_history[j, :] = self.y[range(batch_idx[j],  batch_idx[j]+ self.T - <span class="number">1</span>)]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    X[j, :, :] = self.X[range(batch_idx[j] + self.train_size - self.T, batch_idx[j] + self.train_size - <span class="number">1</span>), :]</span><br><span class="line">                    y_history[j, :] = self.y[range(batch_idx[j] + self.train_size - self.T,  batch_idx[j]+ self.train_size - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">            y_history = Variable(torch.from_numpy(y_history).type(torch.FloatTensor).to(device))</span><br><span class="line">            _, input_encoded = self.encoder(Variable(torch.from_numpy(X).type(torch.FloatTensor).to(device)))</span><br><span class="line">            y_pred[i:(i + self.batch_size)] = self.decoder(input_encoded, y_history).cpu().data.numpy()[:, <span class="number">0</span>]</span><br><span class="line">            i += self.batch_size</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">io_dir = <span class="string">'nasdaq100_padding.csv'</span></span><br><span class="line"></span><br><span class="line">model = da_rnn(file_data=<span class="string">'&#123;&#125;'</span>.format(io_dir), parallel=<span class="literal">False</span>, learning_rate=<span class="number">.001</span>)</span><br><span class="line"></span><br><span class="line">model.train(n_epochs=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">y_pred = model.predict()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.semilogy(range(len(model.iter_losses)), model.iter_losses)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.semilogy(range(len(model.epoch_losses)), model.epoch_losses)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(y_pred, label = <span class="string">'Predicted'</span>)</span><br><span class="line">plt.plot(model.y[model.train_size:], label = <span class="string">"True"</span>)</span><br><span class="line">plt.legend(loc = <span class="string">'upper left'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Bugs fix for&lt;br&gt;&lt;a href=&quot;https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot; title=&quot;https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py&quot;&gt;https://github.com/chandlerzuo/chandlerzuo.github.io/blob/master/codes/da_rnn/DA_RNN.py&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Learn-Pytorch-Variable</title>
    <link href="http://arithmeticjia.github.io/2019/12/10/Learn-Pytorch-Variable/"/>
    <id>http://arithmeticjia.github.io/2019/12/10/Learn-Pytorch-Variable/</id>
    <published>2019-12-10T11:01:51.000Z</published>
    <updated>2019-12-10T11:14:08.493Z</updated>
    
    <content type="html"><![CDATA[<p>Tensor是Pytorch的一个完美组件(可以生成高维数组)，但是要构建神经网络还是远远不够的，我们需要能够计算图的Tensor，那就是Variable。Variable是对Tensor的一个封装，操作和Tensor是一样的，但是每个Variable都有三个属性，Varibale的Tensor本身的.data，对应Tensor的梯度.grad，以及这个Variable是通过什么方式得到的.grad_fn<br><a id="more"></a></p><h4 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h4><p>autograd.Variable 是包的核心类. 它包装了张量, 并且支持几乎所有的操作. 一旦你完成了你的计算, 你就可以调用 .backward() 方法, 然后所有的梯度计算会自动进行.你还可以通过 .data 属性来访问原始的张量, 而关于该 variable（变量）的梯度会被累计到 .grad上去.还有一个针对自动求导实现来说非常重要的类 - Function.Variable 和 Function 是相互联系的, 并且它们构建了一个非循环的图, 编码了一个完整的计算历史信息. 每一个 variable（变量）都有一个 .grad_fn 属性, 它引用了一个已经创建了 Variable 的 Function （除了用户创建的 Variable <code>之外 - 它们的</code>grad_fn is None ）.如果你想计算导数, 你可以在 Variable 上调用 .backward() 方法. 如果 Variable 是标量的形式（例如, 它包含一个元素数据）, 你不必指定任何参数给 backward(), 但是, 如果它有更多的元素. 你需要去指定一个 grad_output 参数, 该参数是一个匹配 shape（形状）的张量.</p><h4 id="创建一个2×2的变量"><a href="#创建一个2×2的变量" class="headerlink" title="创建一个2×2的变量"></a>创建一个2×2的变量</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">View more, visit my tutorial page: https://arithmeticjia.github.io</span></span><br><span class="line"><span class="string">My Blog: https://www.guanacossj.com</span></span><br><span class="line"><span class="string">Dependencies:</span></span><br><span class="line"><span class="string">torch: 1.3.0</span></span><br><span class="line"><span class="string">matplotlib</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># Variable in torch is to build a computational graph,</span></span><br><span class="line"><span class="comment"># but this graph is dynamic compared with a static graph in Tensorflow or Theano.</span></span><br><span class="line"><span class="comment"># So torch does not have placeholder, torch can just pass variable to the computational graph.</span></span><br><span class="line"></span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])            <span class="comment"># build a tensor</span></span><br><span class="line">variable = Variable(tensor, requires_grad=<span class="literal">True</span>)      <span class="comment"># build a variable, usually for compute gradients</span></span><br><span class="line"></span><br><span class="line">print(tensor)       <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br><span class="line">print(variable)     <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]])</span><br><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]], requires_grad=True)</span><br></pre></td></tr></table></figure><h4 id="计算变量的点乘积、梯度"><a href="#计算变量的点乘积、梯度" class="headerlink" title="计算变量的点乘积、梯度"></a>计算变量的点乘积、梯度</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">View more, visit my tutorial page: https://arithmeticjia.github.io</span></span><br><span class="line"><span class="string">My Blog: https://www.guanacossj.com</span></span><br><span class="line"><span class="string">Dependencies:</span></span><br><span class="line"><span class="string">torch: 1.3.0</span></span><br><span class="line"><span class="string">matplotlib</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="comment"># Variable in torch is to build a computational graph,</span></span><br><span class="line"><span class="comment"># but this graph is dynamic compared with a static graph in Tensorflow or Theano.</span></span><br><span class="line"><span class="comment"># So torch does not have placeholder, torch can just pass variable to the computational graph.</span></span><br><span class="line"></span><br><span class="line">tensor = torch.FloatTensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])            <span class="comment"># build a tensor</span></span><br><span class="line">variable = Variable(tensor, requires_grad=<span class="literal">True</span>)      <span class="comment"># build a variable, usually for compute gradients</span></span><br><span class="line"></span><br><span class="line">print(tensor)       <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br><span class="line">print(variable)     <span class="comment"># [torch.FloatTensor of size 2x2]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># till now the tensor and variable seem the same.</span></span><br><span class="line"><span class="comment"># However, the variable is a part of the graph, it's a part of the auto-gradient.</span></span><br><span class="line"></span><br><span class="line">t_out = torch.mean(tensor*tensor)       <span class="comment"># x^2</span></span><br><span class="line">v_out = torch.mean(variable*variable)   <span class="comment"># x^2</span></span><br><span class="line">print(t_out)</span><br><span class="line">print(v_out)                            <span class="comment"># 7.5</span></span><br><span class="line"></span><br><span class="line">print(variable*variable)</span><br><span class="line"><span class="comment"># 点乘操作</span></span><br><span class="line">print(torch.mm(variable,variable))</span><br><span class="line"><span class="comment"># 矩阵相乘</span></span><br><span class="line">v_out.backward()    <span class="comment"># backpropagation from v_out</span></span><br><span class="line"><span class="comment"># v_out = 1/4 * sum(variable*variable)</span></span><br><span class="line"><span class="comment"># the gradients w.r.t the variable, d(v_out)/d(variable) = 1/4*2*variable = variable/2</span></span><br><span class="line">print(variable.grad)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string"> 0.5000  1.0000</span></span><br><span class="line"><span class="string"> 1.5000  2.0000</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]])</span><br><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]], requires_grad=True)</span><br><span class="line">tensor(7.5000)</span><br><span class="line">tensor(7.5000, grad_fn=&lt;MeanBackward0&gt;)</span><br><span class="line">tensor([[ 1.,  4.],</span><br><span class="line">        [ 9., 16.]], grad_fn=&lt;MulBackward0&gt;)</span><br><span class="line">tensor([[ 7., 10.],</span><br><span class="line">        [15., 22.]], grad_fn=&lt;MmBackward&gt;)</span><br><span class="line">tensor([[0.5000, 1.0000],</span><br><span class="line">        [1.5000, 2.0000]])</span><br></pre></td></tr></table></figure><p>注意这里的变量的点乘和相乘的区别</p><h4 id="查看变量的数据"><a href="#查看变量的数据" class="headerlink" title="查看变量的数据"></a>查看变量的数据</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">print(variable)     <span class="comment"># this is data in variable format</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">print(variable.data)    <span class="comment"># this is data in tensor format</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string"> 1  2</span></span><br><span class="line"><span class="string"> 3  4</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]], requires_grad=True)</span><br><span class="line">tensor([[1., 2.],</span><br><span class="line">        [3., 4.]])</span><br></pre></td></tr></table></figure><h4 id="Variable转numpy"><a href="#Variable转numpy" class="headerlink" title="Variable转numpy"></a>Variable转numpy</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">print(variable.data.numpy())    <span class="comment"># numpy format</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">[[ 1.  2.]</span></span><br><span class="line"><span class="string"> [ 3.  4.]]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[[1. 2.]</span><br><span class="line"> [3. 4.]]</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Tensor是Pytorch的一个完美组件(可以生成高维数组)，但是要构建神经网络还是远远不够的，我们需要能够计算图的Tensor，那就是Variable。Variable是对Tensor的一个封装，操作和Tensor是一样的，但是每个Variable都有三个属性，Varibale的Tensor本身的.data，对应Tensor的梯度.grad，以及这个Variable是通过什么方式得到的.grad_fn&lt;br&gt;
    
    </summary>
    
    
      <category term="Pytorch" scheme="http://arithmeticjia.github.io/categories/Pytorch/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://arithmeticjia.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>2019-12-13周报</title>
    <link href="http://arithmeticjia.github.io/2019/12/10/2019-12-13%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2019/12/10/2019-12-13%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2019-12-10T09:46:58.000Z</published>
    <updated>2019-12-27T10:13:01.478Z</updated>
    
    <content type="html"><![CDATA[<p>Work between 2019/12/07-2019/12/13<br><a id="more"></a></p><h4 id="A-Dual-Stage-Attention-Based-Recurrent-Neural-Network-for-Time-Series-Prediction"><a href="#A-Dual-Stage-Attention-Based-Recurrent-Neural-Network-for-Time-Series-Prediction" class="headerlink" title="A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction"></a>A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction</h4><h5 id="Theory"><a href="#Theory" class="headerlink" title="Theory"></a>Theory</h5><p><img src="https://www.guanacossj.com/media/articlebodypics/lstm-attention.jpg" alt=""></p><ul><li>Bi-LSTM + Attention 就是在Bi-LSTM的模型上加入Attention层，在Bi-LSTM中我们会用最后一个时序的输出向量 作为特征向量，然后进行softmax分类。Attention是先计算每个时序的权重，然后将所有时序 的向量进行加权和作为特征向量，然后进行softmax分类</li><li>sigmoid把一个real value映射到(0,1)的区间(当然也可以是(-1,1)),这样可以用来做二分类</li><li>softmax把一个k维的real value向量(a1,a2,a3,a4…)映射成一个(b1,b2,b3,b4…)其中bi是一个0-1的常数，然后可以根据bi的大小来进行多分类的任务，如取权重最大的一维</li><li>传统的注意力机制只用在解码器的输入阶段，即对不同时刻产生不同的context vector不同，该文还在编码器的输入阶段引入了注意力机制，从而同时实现了选取特征因子(feature selection)和把握长期时序依赖关系(long-term temporal dependencies)</li><li>第一阶段，使用注意力机制自适应地提取每个时刻的相关feature<script type="math/tex; mode=display">e_{t}^{k}=v_{e}^{T}tanh(W_{e}[h_{t-1};s_{t-1}]+U_{e}x^{k})</script></li><li>用softmax函数将其归一化<script type="math/tex; mode=display">\alpha _{t}^{k}=\frac{exp(e_{t}^{k})}{\sum_{i-1}^{n}exp(e_{t}^{i})}</script></li><li>得到更新后的x<script type="math/tex; mode=display">\tilde{x} = (\alpha _{t}^{1}x_{t}^{1}, \alpha _{t}^{2}x_{t}^{2},...,\alpha _{t}^{n}x_{t}^{n})</script></li><li>选取LSTM作为编码器<script type="math/tex">f_{1}</script><script type="math/tex; mode=display">h_{t} = f_{1}(h_{t-1},  \tilde{x})</script></li><li><p>第二阶段，使用另一个注意力机制选取与之相关的encoder hidden states</p><script type="math/tex; mode=display">c_{t}^{'} = \sum_{t-1}^{T}\beta _{t^{'}}^{t}h_{t}</script></li><li><p><script type="math/tex">\beta _{t^{'}}^{t}</script>的设计类似于Bahanau的工作，基于前一个时刻解码器的hidden state[公式]和cell state[公式]计算得到：</p></li></ul><h5 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 10</span><br><span class="line"><span class="keyword">Test </span>RMSE: 0.259</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.056</span><br><span class="line"><span class="keyword">Test </span>Data: all</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/pred_0.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/final_predicted_reloaded.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/final_predicted_reloaded_standard.png" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 10</span><br><span class="line"><span class="keyword">Test </span>RMSE: 0.327</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.275</span><br><span class="line"><span class="keyword">Test </span>Data: all * 0.3</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/final_predicted.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/final_predicted_standard.png" alt=""></p><h4 id="Vm1-Power-Matrix-From-Paper"><a href="#Vm1-Power-Matrix-From-Paper" class="headerlink" title="Vm1-Power-Matrix-From-Paper"></a>Vm1-Power-Matrix-From-Paper</h4><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 10</span><br><span class="line"><span class="keyword">Test </span>RMSE: 2.602</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.207</span><br><span class="line"><span class="keyword">Test </span>Data: all<span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/all_server_final_predicted_reloaded-larger.png" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 10</span><br><span class="line"><span class="keyword">Test </span>RMSE: 3.057</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.361</span><br><span class="line"><span class="keyword">Test </span>Data: 49000<span class="string">-50000</span><span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/49000_50000_server_final_predicted_reloaded.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/49000_50000_server_final_predicted_reloaded-larger.png" alt=""><br><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 10</span><br><span class="line"><span class="keyword">Test </span>RMSE: 2.949</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.259</span><br><span class="line"><span class="keyword">Test </span>Data: 32000<span class="string">-33000</span><span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><br><img src="https://www.guanacossj.com/media/articlebodypics/32000_33000_server_final_predicted_reloaded-larger.png" alt=""></p><h4 id="Vm1-Power-Matrix-From-Jia"><a href="#Vm1-Power-Matrix-From-Jia" class="headerlink" title="Vm1-Power-Matrix-From-Jia"></a>Vm1-Power-Matrix-From-Jia</h4><h5 id="Input"><a href="#Input" class="headerlink" title="Input"></a>Input</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Training Data: 1111<span class="string">-1115</span></span><br><span class="line"><span class="keyword">Testing </span>Data: 1116<span class="string">-1120</span></span><br></pre></td></tr></table></figure><figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="params">(n,m)</span> -&gt;</span> <span class="function"><span class="params">(n,m * timestamp)</span> -&gt;</span> (n,timestamp,m)</span><br></pre></td></tr></table></figure><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">    var1(t<span class="number">-60</span>)  var2(t<span class="number">-60</span>)  var3(t<span class="number">-60</span>)  ...   var5(t)   var6(t)   var7(t)</span><br><span class="line"><span class="number">60</span>    <span class="number">0.655771</span>   <span class="number">-1.060919</span>   <span class="number">-0.735941</span>  ... <span class="number">-0.447461</span> <span class="number">-0.217168</span> <span class="number">-0.030555</span></span><br><span class="line"><span class="number">61</span>    <span class="number">0.560612</span>   <span class="number">-1.060919</span>   <span class="number">-0.735941</span>  ... <span class="number">-0.023654</span> <span class="number">-0.222058</span> <span class="number">-0.030555</span></span><br><span class="line"><span class="number">62</span>    <span class="number">0.655771</span>   <span class="number">-1.060919</span>   <span class="number">-0.735941</span>  ... <span class="number">-0.222916</span> <span class="number">-0.207537</span> <span class="number">-0.030555</span></span><br><span class="line"><span class="number">63</span>    <span class="number">0.465453</span>   <span class="number">-1.060919</span>   <span class="number">-0.735941</span>  ... <span class="number">-0.638782</span> <span class="number">-0.222058</span> <span class="number">-0.030555</span></span><br><span class="line"><span class="number">64</span>   <span class="number">-0.010342</span>   <span class="number">-1.047501</span>   <span class="number">-0.735941</span>  ...  <span class="number">1.133103</span> <span class="number">-0.222058</span> <span class="number">-0.030555</span></span><br></pre></td></tr></table></figure><h5 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h5><ul><li>Mean Normaliztion(均值归一化)</li></ul><script type="math/tex; mode=display">x^{*} = \frac{x-\mu}{\sigma }</script><ul><li>Min-Max Normalization</li></ul><script type="math/tex; mode=display">x^{*} = \frac{x-min}{max-min }</script><h5 id="Bi-GRU-Attention"><a href="#Bi-GRU-Attention" class="headerlink" title="Bi-GRU + Attention"></a>Bi-GRU + Attention</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 20</span><br><span class="line"><span class="keyword">Test </span>RMSE: 3.054</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.243</span><br><span class="line"><span class="keyword">Test </span>Data: all<span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/gru-bi-20-all-att-1620.jpg" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 20</span><br><span class="line"><span class="keyword">Test </span>RMSE: 3.458</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.418</span><br><span class="line"><span class="keyword">Test </span>Data: 49000<span class="string">-50000</span><span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/49000_50000_n_final_gru_pro_attention_bi_20.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/mark_49000_50000_n_final_gru_pro_attention_bi_20.png" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 20</span><br><span class="line"><span class="keyword">Test </span>RMSE: 3.196</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.287</span><br><span class="line"><span class="keyword">Test </span>Data: 32000<span class="string">-33000</span><span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/32000_33000_n_final_gru_pro_attention_bi_20.png" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/mark_32000_33000_n_final_gru_pro_attention_bi_20.png" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 20</span><br><span class="line"><span class="keyword">Test </span>RMSE: 4.573</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.45</span><br><span class="line"><span class="keyword">Test </span>Data: 70000<span class="string">-70500</span><span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/70000-70500-gru-bi-att-20-1620.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/mark_70000-70500-gru-bi-att-20-1620.jpg" alt=""></p><h5 id="Bi-LSTM-Attention"><a href="#Bi-LSTM-Attention" class="headerlink" title="Bi-LSTM + Attention"></a>Bi-LSTM + Attention</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 20</span><br><span class="line"><span class="keyword">Test </span>RMSE: 2.776</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.221</span><br><span class="line"><span class="keyword">Test </span>Data: all<span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/lstm-bi-20-att-all-1620.jpg" alt=""></p><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 20</span><br><span class="line"><span class="keyword">Test </span>RMSE: 4.596</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.460</span><br><span class="line"><span class="keyword">Test </span>Data: 70000<span class="string">-70500</span><span class="string">-1116</span><span class="string">-1120</span></span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/70000-70500-lstm-bi-att-20-01-1620.jpg" alt=""></p><p><img src="https://www.guanacossj.com/media/articlebodypics/mark_70000-70500-lstm-bi-att-20-01-1620.jpg" alt=""></p><h5 id="Bi-LSTM-Only"><a href="#Bi-LSTM-Only" class="headerlink" title="Bi-LSTM Only"></a>Bi-LSTM Only</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Epochs: 500</span><br><span class="line">Train-Data: 1111<span class="string">-1115</span></span><br><span class="line">Test-Data: 1116<span class="string">-1120</span></span><br><span class="line"><span class="keyword">Test </span>RMSE: 3.880</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.308</span><br></pre></td></tr></table></figure><p>Test Data: all-1116-1120<br><img src="https://www.guanacossj.com/media/articlebodypics/lstm-bi-500-all-1620.jpg" alt=""></p><p>Test Data: 50000-50500-1116-1120<br><img src="https://www.guanacossj.com/media/articlebodypics/lstm-bi-500-01-1620.jpg" alt=""></p><p>Test Data: 60000-60500-1116-1120<br><img src="https://www.guanacossj.com/media/articlebodypics/lstm-bi-500-02-1620.jpg" alt=""></p><p>Test Data: 30000-30500-1116-1120<br><img src="https://www.guanacossj.com/media/articlebodypics/lstm-bi-500-03-1620.jpg" alt=""></p><h4 id="Bi-LSTM-Attenion-amp-GRU-LSTM-Attenion-In-Stock-With-Pytorch"><a href="#Bi-LSTM-Attenion-amp-GRU-LSTM-Attenion-In-Stock-With-Pytorch" class="headerlink" title="Bi-LSTM-Attenion &amp; GRU-LSTM-Attenion In Stock With Pytorch"></a>Bi-LSTM-Attenion &amp; GRU-LSTM-Attenion In Stock With Pytorch</h4><h5 id="Bi-GRU"><a href="#Bi-GRU" class="headerlink" title="Bi-GRU"></a>Bi-GRU</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 66.403</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.077</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/pytorch-bi-gru-stock.png" alt=""></p><h5 id="Bi-LSTM"><a href="#Bi-LSTM" class="headerlink" title="Bi-LSTM"></a>Bi-LSTM</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 88.421</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.103</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/pytorch-bi-lstm-stock.png" alt=""></p><h5 id="Bi-GRU-Attention-1"><a href="#Bi-GRU-Attention-1" class="headerlink" title="Bi-GRU-Attention"></a>Bi-GRU-Attention</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 86.775</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.101</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/pytorch-bi-gru-att-stock.png" alt=""></p><h5 id="Bi-LSTM-Attention-1"><a href="#Bi-LSTM-Attention-1" class="headerlink" title="Bi-LSTM-Attention"></a>Bi-LSTM-Attention</h5><figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Test </span>RMSE: 86.588</span><br><span class="line"><span class="keyword">Test </span>nRMSE: 0.101</span><br></pre></td></tr></table></figure><p><img src="https://www.guanacossj.com/media/articlebodypics/pytorch-bi-lstm-att-stock.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Work between 2019/12/07-2019/12/13&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>Django个人博客搭建教程-Django-Rest-Framework外键与多对多序列化</title>
    <link href="http://arithmeticjia.github.io/2019/12/09/Django%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B-Django-Rest-Framework%E5%A4%96%E9%94%AE%E4%B8%8E%E5%A4%9A%E5%AF%B9%E5%A4%9A%E5%BA%8F%E5%88%97%E5%8C%96/"/>
    <id>http://arithmeticjia.github.io/2019/12/09/Django%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%95%99%E7%A8%8B-Django-Rest-Framework%E5%A4%96%E9%94%AE%E4%B8%8E%E5%A4%9A%E5%AF%B9%E5%A4%9A%E5%BA%8F%E5%88%97%E5%8C%96/</id>
    <published>2019-12-09T09:45:05.000Z</published>
    <updated>2019-12-09T09:56:36.727Z</updated>
    
    <content type="html"><![CDATA[<p>如果对一个含有多对多、外键的模型进行序列化，这时候这些关联的字段会只展示id，因此需要对外键或者多对多的字段进行序列化处理<br><a id="more"></a></p><h4 id="外键序列化（ForeignKey）-amp-多对多序列化（manytomany"><a href="#外键序列化（ForeignKey）-amp-多对多序列化（manytomany" class="headerlink" title="外键序列化（ForeignKey）&amp;多对多序列化（manytomany)"></a>外键序列化（ForeignKey）&amp;多对多序列化（manytomany)</h4><p>这里要序列化的模型是Articles,其中的authorname、tags、category用了外键或者多对多关联<br>关联的两个模型是Tag和Category<br>models.py<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Category</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        Django 要求模型必须继承 models.Model 类。</span></span><br><span class="line"><span class="string">        Category 只需要一个简单的分类名 name 就可以了。</span></span><br><span class="line"><span class="string">        CharField 指定了分类名 name 的数据类型，CharField 是字符型，</span></span><br><span class="line"><span class="string">        CharField 的 max_length 参数指定其最大长度，超过这个长度的分类名就不能被存入数据库。</span></span><br><span class="line"><span class="string">        当然 Django 还为我们提供了多种其它的数据类型，如日期时间类型 DateTimeField、整数类型 IntegerField 等等。</span></span><br><span class="line"><span class="string">        Django 内置的全部类型可查看文档：</span></span><br><span class="line"><span class="string">        https://docs.djangoproject.com/en/1.10/ref/models/fields/#field-types</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">    name = models.CharField(max_length=<span class="number">100</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">catcount</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> Articles.objects.filter(category__name__exact=self.name).filter(status=<span class="string">'有效'</span>).count()</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.name</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Tag</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">        标签 Tag 也比较简单，和 Category 一样。</span></span><br><span class="line"><span class="string">        再次强调一定要继承 models.Model 类！</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">    name = models.CharField(max_length=<span class="number">100</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.name</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Articles</span><span class="params">(models.Model)</span>:</span></span><br><span class="line">    id = models.AutoField(primary_key=<span class="literal">True</span>)  <span class="comment"># id</span></span><br><span class="line">    title = models.CharField(max_length=<span class="number">150</span>)  <span class="comment"># 博客标题</span></span><br><span class="line">    body = models.TextField()  <span class="comment"># 博客正文</span></span><br><span class="line">    timestamp = models.DateTimeField()  <span class="comment"># 创建时间</span></span><br><span class="line">    authorname = models.ForeignKey(<span class="string">'JiaBlog.BlogUser'</span>, on_delete=models.CASCADE)  <span class="comment"># 作者姓名</span></span><br><span class="line">    views = models.PositiveIntegerField(default=<span class="number">0</span>)</span><br><span class="line">    category = models.ForeignKey(Category, on_delete=models.CASCADE, primary_key=<span class="literal">False</span>)</span><br><span class="line">    tags = models.ManyToManyField(Tag, blank=<span class="literal">True</span>, null=<span class="literal">True</span>)</span><br><span class="line">    greats = models.PositiveIntegerField(default=<span class="number">0</span>)</span><br><span class="line">    comments = models.IntegerField(default=<span class="number">0</span>)</span><br><span class="line">    status = models.CharField(max_length=<span class="number">20</span>, default=<span class="string">"DEL"</span>)</span><br><span class="line">    brief = models.CharField(max_length=<span class="number">200</span>, blank=<span class="literal">True</span>, null=<span class="literal">True</span>)</span><br><span class="line">    pic = models.ImageField(upload_to=<span class="string">'jiablogimages'</span>)</span><br><span class="line">    <span class="comment"># bodypic = models.ImageField(upload_to='jiablogimages', blank=True, null=True)</span></span><br><span class="line">    istop = models.CharField(max_length=<span class="number">5</span>, default=<span class="string">''</span>, null=<span class="literal">True</span>, blank=<span class="literal">True</span>)</span><br><span class="line">    articlebodybrief = models.TextField(blank=<span class="literal">True</span>, null=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><br>views.py<br>这里的source对应的是字段名<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ArticlesSerializers</span><span class="params">(serializers.ModelSerializer)</span>:</span></span><br><span class="line">    authorname = serializers.CharField(source=<span class="string">'authorname.name'</span>)</span><br><span class="line">    category = serializers.CharField(source=<span class="string">'category.name'</span>)</span><br><span class="line">    tags = serializers.StringRelatedField(many=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Meta</span>:</span></span><br><span class="line">        model = Articles  <span class="comment"># 指定的模型类</span></span><br><span class="line">        fields = (<span class="string">'id'</span>, <span class="string">'title'</span>, <span class="string">'body'</span>, <span class="string">'timestamp'</span>, <span class="string">'authorname'</span>, <span class="string">'views'</span>, <span class="string">'tags'</span>, <span class="string">'category'</span>)  <span class="comment"># 需要序列化的属性</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;如果对一个含有多对多、外键的模型进行序列化，这时候这些关联的字段会只展示id，因此需要对外键或者多对多的字段进行序列化处理&lt;br&gt;
    
    </summary>
    
    
      <category term="Django" scheme="http://arithmeticjia.github.io/categories/Django/"/>
    
    
      <category term="django" scheme="http://arithmeticjia.github.io/tags/django/"/>
    
      <category term="restframework" scheme="http://arithmeticjia.github.io/tags/restframework/"/>
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>Learn-Pytorch-使用LSTM预测航班客流量</title>
    <link href="http://arithmeticjia.github.io/2019/12/08/Learn-Pytorch-%E4%BD%BF%E7%94%A8LSTM%E9%A2%84%E6%B5%8B%E8%88%AA%E7%8F%AD%E5%AE%A2%E6%B5%81%E9%87%8F/"/>
    <id>http://arithmeticjia.github.io/2019/12/08/Learn-Pytorch-%E4%BD%BF%E7%94%A8LSTM%E9%A2%84%E6%B5%8B%E8%88%AA%E7%8F%AD%E5%AE%A2%E6%B5%81%E9%87%8F/</id>
    <published>2019-12-08T14:22:52.000Z</published>
    <updated>2019-12-08T14:37:58.046Z</updated>
    
    <content type="html"><![CDATA[<p>本文会详细讲解如何使用Pytorch预测航班客流量，包括数据的处理、网络的结构<br><a id="more"></a></p><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">data_csv = pd.read_csv(<span class="string">'airline-passengers.csv'</span>,usecols=[<span class="number">1</span>])</span><br><span class="line">plt.plot(data_csv)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>效果如下<br><img src="https://www.guanacossj.com/media/articlebodypics/1575815313343.jpg" alt=""><br>这里是真实的数据，接下来我们对数据进行预处理<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据预处理</span></span><br><span class="line">data_csv = data_csv.dropna()    <span class="comment"># 滤除缺失数据</span></span><br><span class="line">dataset = data_csv.values       <span class="comment"># 获得csv的值</span></span><br><span class="line">print((dataset,type(dataset),dataset.shape))</span><br><span class="line">dataset = dataset.astype(<span class="string">'float32'</span>)</span><br><span class="line">max_value = np.max(dataset)     <span class="comment"># 获得最大值</span></span><br><span class="line">min_value = np.min(dataset)     <span class="comment"># 获得最小值</span></span><br><span class="line">scalar = max_value - min_value  <span class="comment"># 获得间隔数量</span></span><br><span class="line">dataset = list(map(<span class="keyword">lambda</span> x: x / scalar, dataset)) <span class="comment"># 归一化</span></span><br></pre></td></tr></table></figure><br>可以看一下最后处理完成的dataset，是一个list<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0.21621622] [0.22779922] [0.25482625]...</span><br></pre></td></tr></table></figure><br>创建输入输出，这里使用当前时间的前两个时刻<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span><span class="params">(dataset, look_back=<span class="number">2</span>)</span>:</span></span><br><span class="line">    dataX, dataY = [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(dataset) - look_back):</span><br><span class="line">        a = dataset[i:(i + look_back)]</span><br><span class="line">        dataX.append(a)</span><br><span class="line">        dataY.append(dataset[i + look_back])</span><br><span class="line">    <span class="keyword">return</span> np.array(dataX), np.array(dataY)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">data_X, data_Y = create_dataset(dataset)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文会详细讲解如何使用Pytorch预测航班客流量，包括数据的处理、网络的结构&lt;br&gt;
    
    </summary>
    
    
      <category term="Pytorch" scheme="http://arithmeticjia.github.io/categories/Pytorch/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://arithmeticjia.github.io/tags/pytorch/"/>
    
      <category term="lstm" scheme="http://arithmeticjia.github.io/tags/lstm/"/>
    
  </entry>
  
  <entry>
    <title>Learn-Pytorch-用Pytorch写一个神经网络</title>
    <link href="http://arithmeticjia.github.io/2019/12/07/Learn-Pytorch-%E7%94%A8Pytorch%E5%86%99%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://arithmeticjia.github.io/2019/12/07/Learn-Pytorch-%E7%94%A8Pytorch%E5%86%99%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2019-12-07T10:00:11.000Z</published>
    <updated>2019-12-07T11:05:50.476Z</updated>
    
    <content type="html"><![CDATA[<p>本文将用Pytorch构建一个最简单的线性神经网络，练习Pytorch中神经网络模型的保存和重载<br><a id="more"></a></p><h4 id="定义一个线性神经网络"><a href="#定义一个线性神经网络" class="headerlink" title="定义一个线性神经网络"></a>定义一个线性神经网络</h4><script type="math/tex; mode=display">y = wx + b</script><p>这是一个基本的网络Net，它只包含一个全连接层<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net,self).__init__()</span><br><span class="line">        self.layer = nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">        self.layer.weight = nn.Parameter(torch.FloatTensor([[<span class="number">10</span>]]))</span><br><span class="line">        self.layer.bias = nn.Parameter(torch.FloatTensor([<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        y = self.layer(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>]])</span><br><span class="line">net = Net()</span><br><span class="line">linearout = net(x)</span><br><span class="line">print(linearout)</span><br></pre></td></tr></table></figure><br>这里假设输入x=1<br>y的值应为11<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[11.]], grad_fn=&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></figure></p><h4 id="保存Net的参数值"><a href="#保存Net的参数值" class="headerlink" title="保存Net的参数值"></a>保存Net的参数值</h4><p>查看网络的状态字典<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(net.state_dict())</span><br></pre></td></tr></table></figure><br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OrderedDict([('layer.weight', tensor([[10.]])), ('layer.bias', tensor([1.]))])</span><br></pre></td></tr></table></figure><br>保存状态字典<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.save(obj=net.state_dict(), f=<span class="string">"models/net.pth"</span>)</span><br></pre></td></tr></table></figure></p><h4 id="加载Net参数值并用于新的模型"><a href="#加载Net参数值并用于新的模型" class="headerlink" title="加载Net参数值并用于新的模型"></a>加载Net参数值并用于新的模型</h4><p>重新定义一个相同结构的模型NewNet<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net,self).__init__()</span><br><span class="line">        self.layer = nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">        self.layer.weight = nn.Parameter(torch.FloatTensor([[<span class="number">10</span>]]))</span><br><span class="line">        self.layer.bias = nn.Parameter(torch.FloatTensor([<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        y = self.layer(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NewNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(NewNet, self).__init__()</span><br><span class="line">        self.layer = nn.Linear(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        self.layer.weight = nn.Parameter(torch.FloatTensor([[<span class="number">0</span>]]))</span><br><span class="line">        self.layer.bias = nn.Parameter(torch.FloatTensor([<span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.layer(x)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">x = torch.FloatTensor([[<span class="number">1</span>]])</span><br><span class="line">net = NewNet()</span><br><span class="line">print(net.state_dict())                             <span class="comment"># 初始的NewNet的状态字典</span></span><br><span class="line">net.load_state_dict(torch.load(<span class="string">"models/net.pth"</span>))</span><br><span class="line">print(net.state_dict())                             <span class="comment"># 加载参数值的NewNet的状态字典</span></span><br></pre></td></tr></table></figure><br>net的w和b值就不再是0了，而是之前保存的模型中w和b对应的10和1<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">OrderedDict([('layer.weight', tensor([[0.]])), ('layer.bias', tensor([0.]))])</span><br><span class="line">OrderedDict([('layer.weight', tensor([[10.]])), ('layer.bias', tensor([1.]))])</span><br></pre></td></tr></table></figure></p><h4 id="优化器与epoch的保存"><a href="#优化器与epoch的保存" class="headerlink" title="优化器与epoch的保存"></a>优化器与epoch的保存</h4><p>保存优化器参数值和epoch值的主要目的是用于继续训练，保存的流程依旧是先“torch.save()”再“torch.load_state_dict()”<br>我们首先定义一个Adam优化器、一个任意的epoch值与net如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">mport torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn,optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net,self).__init__()</span><br><span class="line">        self.layer = nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">        self.layer.weight = nn.Parameter(torch.FloatTensor([[<span class="number">10</span>]]))</span><br><span class="line">        self.layer.bias = nn.Parameter(torch.FloatTensor([<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self,x)</span>:</span></span><br><span class="line">        y = self.layer(x)</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line">Adam = optim.Adam(params=net.parameters(), lr=<span class="number">0.001</span>, betas=(<span class="number">0.5</span>, <span class="number">0.999</span>))</span><br><span class="line">epoch = <span class="number">50</span></span><br><span class="line">all_states = &#123;<span class="string">"net"</span>: net.state_dict(), <span class="string">"Adam"</span>: Adam.state_dict(), <span class="string">"epoch"</span>: epoch&#125;</span><br><span class="line">torch.save(obj=all_states, f=<span class="string">"models/all_states.pth"</span>)</span><br></pre></td></tr></table></figure><br>查看模型所有的参数<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">all_states = torch.load(<span class="string">"models/all_states.pth"</span>)</span><br><span class="line">print(all_states)</span><br></pre></td></tr></table></figure><br><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">'net': OrderedDict([('layer.weight', tensor([[10.]])), ('layer.bias', tensor([1.]))]),</span><br><span class="line">'Adam': &#123;</span><br><span class="line">'state': &#123;&#125;,</span><br><span class="line">'param_groups': [&#123;</span><br><span class="line">'lr': 0.001,</span><br><span class="line">'betas': (0.5, 0.999),</span><br><span class="line">'eps': 1e-08,</span><br><span class="line">'weight_decay': 0,</span><br><span class="line">'amsgrad': False,</span><br><span class="line">'params': [4660776392, 4559888248]</span><br><span class="line">&#125;]</span><br><span class="line">&#125;,</span><br><span class="line">'epoch': 50</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将用Pytorch构建一个最简单的线性神经网络，练习Pytorch中神经网络模型的保存和重载&lt;br&gt;
    
    </summary>
    
    
      <category term="Pytorch" scheme="http://arithmeticjia.github.io/categories/Pytorch/"/>
    
    
      <category term="python" scheme="http://arithmeticjia.github.io/tags/python/"/>
    
      <category term="pytorch" scheme="http://arithmeticjia.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>2019/12/06周报-单沙嘉</title>
    <link href="http://arithmeticjia.github.io/2019/12/06/2019-12-06%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/"/>
    <id>http://arithmeticjia.github.io/2019/12/06/2019-12-06%E5%91%A8%E6%8A%A5-%E5%8D%95%E6%B2%99%E5%98%89/</id>
    <published>2019-12-06T08:38:04.000Z</published>
    <updated>2019-12-06T10:22:35.734Z</updated>
    
    <content type="html"><![CDATA[<p>Nothing<br><a id="more"></a></p><h3 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h3><p>11.11-11.15<br>40万</p><h3 id="特征"><a href="#特征" class="headerlink" title="特征"></a>特征</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cpu.all.usage.percent</span><br><span class="line">memory.used.percent</span><br><span class="line">interface.eth0.if_octets.rx</span><br><span class="line">interface.eth0.if_octets.tx</span><br><span class="line">disk.vda.disk_octets.write</span><br><span class="line">disk.vda.disk_octets.read</span><br></pre></td></tr></table></figure><h3 id="训练参数"><a href="#训练参数" class="headerlink" title="训练参数"></a>训练参数</h3><p>epoch = 200<br>bach_size = 72<br>BiLSTM + Attention</p><h3 id="效果图"><a href="#效果图" class="headerlink" title="效果图"></a>效果图</h3><p><img src="https://www.guanacossj.com/media/articlebodypics/1575621413951.jpg" alt=""><br><img src="https://www.guanacossj.com/media/articlebodypics/1575621002903.jpg" alt=""><br><img src="https://www.guanacossj.com/media/articlebodypics/1575627714970.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>epoch 不够多</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Nothing&lt;br&gt;
    
    </summary>
    
    
      <category term="周报" scheme="http://arithmeticjia.github.io/categories/%E5%91%A8%E6%8A%A5/"/>
    
    
  </entry>
  
  <entry>
    <title>Keras以及Tensorflow强制使用GPU</title>
    <link href="http://arithmeticjia.github.io/2019/12/06/Keras%E4%BB%A5%E5%8F%8ATensorflow%E5%BC%BA%E5%88%B6%E4%BD%BF%E7%94%A8GPU/"/>
    <id>http://arithmeticjia.github.io/2019/12/06/Keras%E4%BB%A5%E5%8F%8ATensorflow%E5%BC%BA%E5%88%B6%E4%BD%BF%E7%94%A8GPU/</id>
    <published>2019-12-06T07:39:41.000Z</published>
    <updated>2019-12-06T07:44:53.704Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要介绍在tensorflow2.0下如何强制使用GPU，方法一在tensorflow1.x版本中失效<br><a id="more"></a></p><h3 id="环境：python3-6-tensorflow-2-0-keras2-3-1"><a href="#环境：python3-6-tensorflow-2-0-keras2-3-1" class="headerlink" title="环境：python3.6+tensorflow==2.0+keras2.3.1"></a>环境：python3.6+tensorflow==2.0+keras2.3.1</h3><h3 id="方法一："><a href="#方法一：" class="headerlink" title="方法一："></a>方法一：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras.backend.tensorflow_backend <span class="keyword">as</span> KTF</span><br><span class="line"> </span><br><span class="line">KTF.set_session(tf.Session(config=tf.ConfigProto(device_count=&#123;<span class="string">'gpu'</span>:<span class="number">0</span>&#125;)))</span><br></pre></td></tr></table></figure><p>这里在tensorflow2.0中肯定报错<br>修改如下：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> keras.backend.tensorflow_backend <span class="keyword">as</span> KTF</span><br><span class="line"> </span><br><span class="line">KTF.set_session(tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(device_count=&#123;<span class="string">'gpu'</span>:<span class="number">0</span>&#125;)))</span><br></pre></td></tr></table></figure><br>然而这样还是不行，就是告诉你tensorflow2.0中不能这样用<br>遂放弃</p><h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=0 python3 xxx.py</span><br></pre></td></tr></table></figure><p>貌似可行<br><img src="https://www.guanacossj.com/media/articlebodypics/1575618190733.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文主要介绍在tensorflow2.0下如何强制使用GPU，方法一在tensorflow1.x版本中失效&lt;br&gt;
    
    </summary>
    
    
    
      <category term="keras" scheme="http://arithmeticjia.github.io/tags/keras/"/>
    
      <category term="tensorflow" scheme="http://arithmeticjia.github.io/tags/tensorflow/"/>
    
      <category term="gpu" scheme="http://arithmeticjia.github.io/tags/gpu/"/>
    
  </entry>
  
</feed>
